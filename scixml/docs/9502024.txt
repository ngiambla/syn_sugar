 An extragrammatical sentence is what a normal parser fails to analyze . 
 It is important to recover it using only syntactic information although results of recovery are better if semantic factors are considered . 
 A general algorithm for least-errors recognition , which is based only on syntactic information , was proposed by G. Lyon to deal with the extragrammaticality . 
 We extended this algorithm to recover extragrammatical sentence into grammatical one in running text . 
 Our robust parser with recovery mechanism - extended general algorithm for least-errors recognition - can be easily scaled up and modified because it utilize only syntactic information . 
 To upgrade this robust parser we proposed heuristics through the analysis on the Penn treebank corpus . 
 The experimental result shows 68 % - 77 % accuracy in error recovery . 
 Extragrammatical sentences include patently ungrammatical constructions as well as utterances that may be grammatically acceptable but are beyond the syntactic coverage of a parser , and any other difficult ones that are encountered in parsing Carbonell and Hayes 1983 . 
 Above examples show that people are used to write same meaningful sentences differently . 
 In addition , people are prone to mistakes in writing sentences . 
 So , the bulk of written sentences are open to the extragrammaticality . 
 In the Penn treebank tree-tagged corpus Marcus 1991 , for instance , about 80 percents of the rules are concerned with peculiar sentences which include inversive , elliptic , parenthetic , or emphatic phrases . 
 For example , we can drive a rule VP  vb NP comma rb comma PP from the following sentence . 
 A robust parser is one that can analyze these extragrammatical sentences without failure . 
 However , if we try to preserve robustness by adding such rules whenever we encounter an extragrammatical sentence , the rulebase will grow up rapidly , and thus processing and maintaining the excessive number of rules will become inefficient and impractical . 
 Therefore , extragrammatical sentences should be handled by some recovery mechanism ( s ) rather than by a set of additional rules . 
 Many researchers have attempted several techniques to deal with extragrammatical sentences such as Augmented Transition Network ( ATN ) Kwasny and Sondheimer 1981 , network-based semantic grammar Hendrix 1977 , partial pattern matching Hayes and Mouradian 1981 , conceptual case frame Schank et al. 1980 , and multiple cooperating methods Hayes and Carbonell 1981 . 
 Above mentioned techniques take into account various semantic factors depending on specific domains on question in recovering extragrammatical sentences . 
 Whereas they can provide even better solutions intrinsically , they are usually ad-hoc and are lack of extensibility . 
 Therefore , it is important to recover extragrammatical sentences using syntactic factors only , which are independent of any particular system and any particular domain . 
 Mellish 1989 introduced some chart-based techniques using only syntactic information for extragrammatical sentences . 
 This technique has an advantage that there is no repeating work for the chart to prevent the parser from generating the same edge as the previously existed edge . 
 Also , because the recovery process runs when a normal parser terminates unsuccessfully , the performance of the normal parser does not decrease in case of handling grammatical sentences . 
 However , his experiment was not based on the errors in running texts but on artificial ones which were randomly generated by human . 
 Moreover , only one word error was considered though several word errors can occur simultaneously in the running text . 
 A general algorithm for least-errors recognition Lyon 1974 , proposed by G. Lyon , is to find out the least number of errors necessary to successful parsing and recover them . 
 Because this algorithm is also syntactically oriented and based on a chart , it has the same advantage as that of Mellish 's parser . 
 When the original parsing algorithm terminates unsuccessfully , the algorithm begins to assume errors of insertion , deletion and mutation of a word . 
 For any input , including grammatical and extragrammatical sentences , this algorithm can generate the resultant parse tree . 
 At the cost of the complete robustness , however , this algorithm degrades the efficiency of parsing , and generates many intermediate edges . 
 In this paper , we present a robust parser with a recovery mechanism . 
 We extend the general algorithm for least-errors recognition to adopt it as the recovery mechanism in our robust parser . 
 Because our robust parser handle extragrammatical sentences with this syntactic information oriented recovery mechanism , it can be independent of a particular system or particular domain . 
 Also , we present the heuristics to reduce the number of edges so that we can upgrade the performance of our parser . 
 This paper is organized as follows : We first review a general algorithm for least-errors recognition . 
 Then we present the extension of this algorithm , and the heuristics adopted by the robust parser . 
 Next , we describe the implementation of the system and the result of the experiment of parsing real sentences . 
 Finally , we make conclusion with future direction . 
 The general algorithm for least-errors recognition Lyon 1974 , which is based on Earley 's algorithm , assumes that sentences may have insertion , deletion , and mutation errors of terminal symbols . 
 The objective of this algorithm is to parse input string with the least number of errors . 
 A state used in this algorithm is quadruple  , where p is a production number in grammar , j marks a position in  , f is a start position of the state in input string , and e is an error value . 
 A final state  denotes recognition of a phrase  with e errors where  is a number of components in rule p . 
 A stateset  , where i is the position of the input , is an ordered set of states . 
 States within a stateset are ordered by ascending value of j , within a p within a f ; f takes descending value . 
 When adding to statesets , if state  is a candidate for admission to a stateset which already has a similar member  and e '  e , then  is rejected . 
 However , if  , then  is replaced by . 
 The algorithm works as follows : A procedure SCAN is carried out for each state in  . 
 SCAN checks various correspondences of input token  against terminal symbols in RHS of rules . 
 Once SCAN is done , COMPLETER substitutes all final states of  into all other analyses which can use them as components . 
 SCAN 
 SCAN handles states of  , checking each input terminal against requirements of states in  and various error hypotheses . 
 Figure  shows how SCAN processes . 
 Let  be j-th component of  and  be i-th word of input string .
 perfect match : 
 If  then add  to  if possible . 
 insertion-error hypothesis : 
 Add  to  if possible . 
  is the cost of an insertion-error for a terminal symbol . 
 deletion-error hypothesis : 
 If  is terminal , then add  to  if possible . 
  is the cost of a deletion-error for a terminal symbol . 
 mutation-error hypothesis : 
 If  is terminal but not equal to  , then add  to  if possible . 
  is the cost of a mutation-error for a terminal symbol . 
 COMPLETER 
 COMPLETER handles substitution of final states in  like that of original Earley 's algorithm . 
 Each final state means the recognition of a nonterminal . 
 The algorithm in section  can analyze any input string with the least number of errors . 
 But this algorithm can handle only the errors of terminal symbols because it doesn't consider the errors of nonterminal nodes . 
 In the real text , however , the insertion , deletion , or inversion of a phrase - namely , nonterminal node - occurs more frequently . 
 So , we extend the original algorithm in order to handle the errors of nonterminal symbols as well . 
 In our extended algorithm , the same SCAN as that of the original algorithm is used , while COMPLETER is modified and extended . 
 Figure  shows the processing of extended-COMPLETER . 
 In figure  , [ NP ] denotes the final state whose rule has NP as its LHS . 
 In other words , it means the recognition of a noun phrase . 
 extended-COMPLETER 
 If there is a final state  in  , 
 phrase perfect match 
 If there exists a state  in  and  then add  into  . 
 phrase insertion-error hypothesis 
 If there exists a state  in  then add  into  if possible . 
  is the cost of a insertion-error for a nonterminal symbol . 
 phrase deletion-error hypothesis 
 If there exists a state  in  and  is a nonterminal then add  into  if possible . 
  is the cost of a deletion-error for a nonterminal symbol . 
 phrase mutation-error hypothesis 
 If there exists a state  in  and  is a nonterminal but not equal to  then add  into  if possible . 
  is the cost of a mutation-error for a nonterminal symbol . 
 The extended least-errors recognition algorithm can handle not only terminal errors but also nonterminal errors . 
 The robust parser using the extended least-errors recognition algorithm overgenerates many error-hypothesis edges during parsing process . 
 To cope with this problem , we adjust error values according to the following heuristics . 
 Edges with more error values are regarded as less important ones , so that those edges are processed later than those of less error values . 
 Heuristics 1 : error types 
 The analysis on 3,538 sentences of the Penn treebank corpus WSJ shows that there are 498 sentences with phrase deletions and 224 sentences with phrase insertions . 
 So , we assign less error value to the deletion-error hypothesis edge than to the insertion - and mutation-errors . 
 where  is the error cost of a terminal symbol ,  is the error cost of a nonterminal symbol . 
 Heuristics 2 : fiducial nonterminal 
 People often make mistakes in writing English . 
 These mistakes usually take place rather between small constituents such as a verbal phrase , an adverbial phrase and noun phrase than within small constituents themselves . 
 The possibility of error occurrence within noun phrases are lower than between a noun phrase and a verbal phrase , a preposition phrase , an adverbial phrase . 
 So , we assume some phrases , for example noun phrases , as fiducial nonterminals , which means error-free nonterminals . 
 When handling sentences , the robust parser assings more error values (  ) to the error hypothesis edge occurring within a fiducial nonterminal . 
 Heuristics 3 : kinds of terminal symbols 
 Some terminal symbols like punctuation symbols , conjunctions and particles are often misused . 
 So , the robust parser assigns less error values (  ) to the error hypothesis edges with these symbols than to the other terminal symbols . 
 Heuristics 4 : inserted phrases between commas or parentheses 
 Most of inserted phrases are surrounded by commas or parentheses . 
 For example ,  
 We will assign less error values (  ) to the insertion-error hypothesis edges of nonterminals which are embraced by comma or parenthesis . 
  and  are weights for the error of terminal nodes , and  is a weight for the error of nonterminal nodes . 
 The error value e of an edge is calculated as follows . 
 All error values are additive . 
 The error value e for a rule  , where a is a terminal node and A is a nonterminal node , is  
 where  ,  and  is an error value of a child edge . 
 By these heuristics , our robust parser can process only plausible edges first , instead of processing all generated edges at the same time , so that we can enhance the performance of the robust parser and result in the great reduction in the number of resultant trees . 
 Our robust parsing system is composed of two modules . 
 One module is a normal parser which is the bottom-up chart parser . 
 The other is a robust parser with the error recovery mechanism proposed herein . 
 At first , an input sentence is processed by the normal parser . 
 If the sentence is within the grammatical coverage of the system , the normal parser succeed to analyze it . 
 Otherwise , the normal parser fails , and then the robust parser starts to execute with edges generated by the normal parser . 
 The result of the robust parser is the parse trees which are within the grammatical coverage of the system . 
 The overview of the system is shown in figure  . 
 To show usefulness of the robust parser proposed in this paper , we made some experiments . 
 Rule 
 We can derive 4,958 rules and their frequencies out of 14,137 sentences in the Penn treebank tree-tagged corpus , the Wall Street Journal . 
 The average frequency of each rule is 48 times in the corpus . 
 Of these rules , we remove rules which occurs fewer times than the average frequency in the corpus , and then only 192 rules are left . 
 These removed rules are almost for peculiar sentences and the left rules are very general rules . 
 We can show that our robust parser can compensate for lack of rules using only 192 rules with the recovery mechanism and heuristics . 
 Test set 
 First , 1,000 sentences are selected randomly from the WSJ corpus , which we have referred to in proposing the robust parser . 
 Of these sentences , 410 are failed in normal parsing , and are processed again by the robust parser . 
 To show the validity of these heuristics , we compare the result of the robust parser using heuristics with one not using heuristics . 
 Second , to show the adaptability of our robust parser ,  
 same experiments are carried out on 1,000 sentences from the ATIS corpus in Penn treebank , which we haven't referred to when we propose the robust parser . 
 Among 1,000 sentences from the ATIS , 465 sentences are processed by the robust parser after the failure of the normal parsing . 
 Parameter adjustment 
 We chose the best parameters of heuristics by executing several experiments . 
 Accuracy is measured as the percentage of constituents in the test sentences which do not cross any Penn treebank constituents Black 1991 . 
 Table  shows the results of the robust parser on WSJ . 
 In table  , 5th , 6th and 7th raw mean that the percentage of sentences which have no crossing constituents , less than one crossing and less than two crossing respectively . 
 With heuristics , our robust parser can enhance the processing time and reduce the number of edges . 
 Also , the accuracy is improved from 72.8 % to 77.1 % even if the heuristics differentiate edges and prefer some edges . 
 It shows that the proposed heuristics is valid in parsing the real sentences . 
 The experiment says that our robust parser with heuristics can recover perfectly about 23 sentences out of 100 sentences which are just failed in normal parsing , as the percentage of no-crossing sentences is about 23.28 . 
 Table  is the results of the robust parser on ATIS which we did not refer to before . 
 The accuracy of the result on ATIS is lower than WSJ because the parameters of the heuristics are adjusted not by ATIS itself but by WSJ . 
 However , the percentage of sentences with constituents crossing less than 2 is higher than the WSJ , as sentences of ATIS are more or less simple . 
 The experimental results of our robust parser show high accuracy in recovery even though 96 % of total rules are removed . 
 It is impossible to construct complete grammar rules in the real parsing system to succeed in analyzing every real sentence . 
 So , parsing systems are likely to have extragrammatical sentences which cannot be analyzed by the systems . 
 Our robust parser can recover these extragrammatical sentences with 68 - 77 % accuracy . 
 It is very interesting that parameters of heuristics reflect the characteristics of the test corpus . 
 For example , if people tend to write sentences with inserted phrases , then the parameter  must increase . 
 Therefore we can get better results if the parameter are fitted to the characteristics of the corpus . 
 In this paper , we have presented the robust parser with the extended least-errors recognition algorithm as the recovery mechanism . 
 This robust parser can easily be scaled up and applied to various domains because this parser depends only on syntactic factors . 
 To enhance the performance of the robust parser for extragrammatical sentences , we proposed several heuristics . 
 The heuristics assign the error values to each error-hypothesis edge , and edges which has less error values are processed first . 
 So , not all the generated edges are processed by the robust parser , but the most plausible parse trees can be generated first . 
 The accuracy of the recovery in our robust parser is about 68 % - 77 % . 
 Hence , this parser is suitable for systems in real application areas . 
 Our short term goal is to propose an automatic method that can learn parameter values of heuristics by analyzing the corpus . 
 We expect that automatically learned values of parameters can upgrade the performance of the parser . 
 This work was supported ( in part ) by Korea Science and Engineering Foundation ( KOSEF ) through Center for Artificial Intelligence Research ( CAIR ) , the Engineering Research Center ( ERC ) of Excellence Program . 
