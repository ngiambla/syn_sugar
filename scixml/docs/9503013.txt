 Why should computers interpret language incrementally ? 
 In recent years psycholinguistic evidence for incremental interpretation has become more and more compelling , suggesting that humans perform semantic interpretation before constituent boundaries , possibly word by word . 
 However , possible computational applications have received less attention . 
 In this paper we consider various potential applications , in particular graphical interaction and dialogue . 
 We then review the theoretical and computational tools available for mapping from fragments of sentences to fully scoped semantic representations . 
 Finally , we tease apart the relationship between dynamic semantics and incremental interpretation . 
 Following the work of , for example , Marslen-Wilson 1973 , Just and Carpenter 1980 and Altmann and Steedman 1988 , it has become widely accepted that semantic interpretation in human sentence processing can occur before sentence boundaries and even before clausal boundaries . 
 It is less widely accepted that there is a need for incremental interpretation in computational applications . 
 In the 1970 s and early 1980 s several computational implementations motivated the use of incremental interpretation as a way of dealing with structural and lexical ambiguity ( a survey is given in Haddock 1989 ) . 
 A sentence such as the following has 4862 different syntactic parses due solely to attachment ambiguity Stabler 1991 . 
 Although some of the parses can be ruled out using structural preferences during parsing ( such as Late Closure or Minimal Attachment Frazier 1979 ) , extraction of the correct set of plausible readings requires use of real world knowledge . 
 Incremental interpretation allows on-line semantic filtering , i.e. parses of initial fragments which have an implausible or anomalous interpretation are rejected , thereby preventing ambiguities from multiplying as the parse proceeds . 
 However , on-line semantic filtering for sentence processing does have drawbacks . 
 Firstly , for sentence processing using a serial architecture ( rather than one in which syntactic and semantic processing is performed in parallel ) , the savings in computation obtained from on-line filtering have to be balanced against the additional costs of performing semantic computations for parses of fragments which would eventually be ruled out anyway from purely syntactic considerations . 
 Moreover , there are now relatively sophisticated ways of packing ambiguities during parsing ( e.g. by the use of graph-structured stacks and packed parse forests Tomita 1985 ) . 
 Secondly , the task of judging plausibility or anomaly according to context and real world knowledge is a difficult problem , except in some very limited domains . 
 In contrast , statistical techniques using lexeme co-occurrence provide a relatively simple mechanism which can imitate semantic filtering in many cases . 
 For example , instead of judging bank as a financial institution as more plausible than bank as a riverbank in the noun phrase the rich bank , we can compare the number of co-occurrences of the lexemes rich and bank  ( = riverbank ) versus rich and bank  ( = financial institution ) in a semantically analysed corpus . 
 Cases where statistical techniques seem less appropriate are where plausibility is affected by local context . 
 For example , consider the ambiguous sentence , 
 in the two contexts 
 Such cases involve reasoning with an interpretation in its immediate context , as opposed to purely judging the likelihood of a particular linguistic expression in a given application domain ( see e.g. Cooper 1993 for discussion ) . 
 Although the usefulness of on-line semantic filtering during the processing of complete sentences is debatable , filtering has a more plausible role to play in interactive , real-time environments , such as interactive spell checkers ( see e.g. Wirn 1990 for arguments for incremental parsing in such environments ) . 
 Here the choice is between whether or not to have semantic filtering at all , rather than whether to do it on-line , or at the end of the sentence . 
 The concentration in early literature on using incremental interpretation for semantic filtering has perhaps distracted from some other applications which provide less controversial applications . 
 We will consider two in detail here : graphical interfaces , and dialogue . 
 The Foundations for Intelligent Graphics Project ( FIG ) considered various ways in which natural language input could be used within computer aided design systems ( the particular application studied was computer aided kitchen design , where users would not necessarily be professional designers ) . 
 Incremental interpretation was considered to be useful in enabling immediate visual feedback . 
 Visual feedback could be used to provide confirmation ( for example , by highlighting an object referred to by a successful definite description ) , or it could be used to give the user an improved chance of achieving successful reference . 
 For example , if sets of possible referents for a definite noun phrase are highlighted during word by word processing then the user knows how much or how little information is required for successful reference . 
 Human dialogue , in particular , task oriented dialogue is characterised by a large numbers of self-repairs  Levelt 1983 , Carletta et al. 1993  , such as hesitations , insertions , and replacements . 
 It is also common to find interruptions requesting extra clarification , or disagreements before the end of a sentence . 
 It is even possible for sentences started by one dialogue participant to be finished by another . 
 Applications involving the understanding of dialogues include information extraction from conversational databases , or computer monitoring of conversations . 
 It also may be useful to include some features of human dialogue in man-machine dialogue . 
 For example , interruptions can be used for early signalling of errors and ambiguities . 
 Let us first consider some examples of self-repair . 
 Insertions add extra information , usually modifiers e.g. 
 Replacements correct pieces of information e.g. 
 In some cases information from the corrected material is incorporated into the final message . 
 For example , consider : 
 In  , the corrected material the three main sources of data come provides the antecedent for the pronoun they . 
 In  the corrected material tells us that the man is both old and has a wife . 
 In  , the pronoun he is bound by the quantifier every boy . 
 For a system to understand dialogues involving self-repairs such as those in  would seem to require either an ability to interpret incrementally , or the use of a grammar which includes self repair as a syntactic construction akin to non-constituent coordination ( the relationship between coordination and self-correction is noted by Levelt 1983 ) . 
 For a system to generate self repairs might also require incremental interpretation , assuming a process where the system performs on-line monitoring of its output ( akin to Levelt 's model of the human self-repair mechanism ) . 
 It has been suggested that generation of self repairs is useful in cases where there are severe time constraints , or where there is rapidly changing background information Carletta p.c. . 
 A more compelling argument for incremental interpretation is provided by considering dialogues involving interruptions . 
 Consider the following dialogue from the TRAINS corpus Gross et al. 1993 . 
 This requires interpretation by speaker B before the end of A 's sentence to allow objection to the apposition , the engine at Avon , engine E . 
 An example of the potential use of interruptions in human computer interaction is the following : 
 In this example , interpretation must not only be before the end of the sentence , but before a constituent boundary ( the verb phrase in the user 's command has not yet been completed ) . 
 In this section we shall briefly review work on providing semantic representations ( e.g. lambda expressions ) word by word . 
 Traditional layered models of sentence processing first build a full syntax tree for a sentence , and then extract a semantic representation from this . 
 To adapt this to an incremental perspective , we need to be able to provide syntactic structures ( of some sort ) for fragments of sentences , and be able to extract semantic representations from these . 
 One possibility , which has been explored mainly within the Categorial Grammar tradition Steedman 1988 is to provide a grammar which can treat most if not all initial fragments as constituents . 
 They then have full syntax trees from which the semantics can be calculated . 
 However , an alternative possibility is to directly link the partial syntax trees which can be formed for non-constituents with functional semantic representations . 
 For example , a fragment missing a noun phrase such as John likes can be associated with a semantics which is a function from entities to truth values . 
 Hence , the partial syntax tree given in Fig.  , 
  can be associated with a semantic representation ,  x. likes ( john , x ) . 
 Both Categorial approaches to incremental interpretation and approaches which use partial syntax trees get into difficulty in cases of left recursion . 
 Consider the sentence fragment , Mary thinks John . 
 A possible partial syntax tree is provided by Fig.  . 
 However , this is not the only possible partial tree . 
 In fact there are infinitely many different trees possible . 
 The completed sentence may have an arbitrarily large number of intermediate nodes between the lower s node and the lower np . 
 For example , John could be embedded within a gerund e.g. Mary thinks John leaving here was a mistake , and this in turn could be embedded e.g. Mary thinks John leaving here being a mistake is surprising . 
 John could also be embedded within a sentence which has a sentence modifier requiring its own s node e.g. Mary thinks John will go home probably , and this can be further embedded e.g. Mary thinks John will go home probably because he is tired . 
 The problem of there being an arbitrary number of different partial trees for a particular fragment is reflected in most current approaches to incremental interpretation being either incomplete , or not fully word by word . 
 For example , incomplete parsers have been proposed by Stabler 1991 and Moortgat 1988 . 
 Stabler 's system is a simple top-down parser which does not deal with left recursive grammars . 
 Moortgat 's M-System is based on the Lambek Calculus : the problem of an infinite number of possible tree fragments is replaced by a corresponding problem of initial fragments having an infinite number of possible types . 
 A complete incremental parser , which is not fully word by word , was proposed by Pulman 1986 . 
 This is based on arc-eager left-corner parsing Resnik 1992 . 
 To enable complete , fully word by word parsing requires a way of encoding an infinite number of partial trees . 
 There are several possibilities . 
 The first is to use a language describing trees where we can express the fact that John is dominated by the s node , but do not have to specify what it is immediately dominated by ( e.g. D-Theory ) Marcus et al. 1983 . 
 Semantic representations could be formed word by word by extracting ` default ' syntax trees ( by strengthening dominance links into immediated dominance links wherever possible ) . 
 A second possibility is to factor out recursive structures from a grammar . 
 Thompson et al. 1991 show how this can be done for a phrase structure grammar ( creating an equivalent Tree Adjoining Grammar Joshi 1987 ) . 
 The parser for the resulting grammar allows linear parsing for an ( infinitely ) parallel system , with the absorption of each word performed in constant time . 
 At each choice point , there are only a finite number of possible new partial TAG trees ( the TAG trees represents the possibly infinite number of trees which can be formed using adjunction ) . 
 It should again be possible to extract ` default ' semantic values , by taking the semantics from the TAG tree ( i.e. by assuming that there are to be no adjunctions ) . 
 A somewhat similar system has recently been proposed by Shieber and Johnson 1993 . 
 The third possibility is suggested by considering the semantic representations which are appropriate during a word by word parse . 
 Although there are any number of different partial trees for the fragment Mary thinks John , the semantics of the fragment can be represented using just two lambda expressions : 
 Consider the first . 
 The lambda abstraction ( over a functional item of type e  t ) can be thought of as a way of encoding an infinite set of partial semantic ( tree ) structures . 
 For example , the eventual semantic structure may embed john at any depth e.g. 
 The second expression ( a functional item over type e  t and t  t ) , allows for eventual structures where the main sentence is embedded e.g. 
 This third possibility is therefore to provide a syntactic correlate of lambda expressions . 
 In practice , however , provided we are only interested in mapping from a string of words to a semantic representation , and don't need explicit syntax trees to be constructed , we can merely use the types of the ` syntactic lambda expressions ' , rather than the expressions themselves . 
 This is essentially the approach taken in Milward 1992 in order to provide complete , word by word , incremental interpretation using simple lexicalised grammars , such as a lexicalised version of formal dependency grammar and simple categorial grammar . 
 In processing the sentence Mary introduced John to Susan , a word-by-word approach such as Milward 1992 provides the following logical forms after the corresponding sentence fragments are absorbed : 
 Each input level representation is appropriate for the meaning of an incomplete sentence , being either a proposition or a function into a proposition . 
 In Chater et al. 1994 it is argued that the incrementally derived meanings are not judged for plausibility directly , but instead are first turned into existentially quantified propositions . 
 For example , instead of judging the plausibility of  , we judge the plausibility of  . 
 This is just the proposition Mary introduced something to something using a generalized quantifier notation of the form Quantifier ( Variable , Restrictor , Body ) . 
 Although the lambda expressions are built up monotonically , word by word , the propositions formed from them may need to be retracted , along with all the resulting inferences . 
 For example , Mary introduced something to something is inappropriate if the final sentence is Mary introduced noone to anybody . 
 A rough algorithm is as follows : 
 Parse a new word , Word  
 Form a new lambda expression by combining the lambda expression formed after parsing Word  with the lexical semantics for Word  
 Form a proposition , P  , by existentially quantifying over the lambda abstracted variables . 
 Assert P  . 
 If P  does not entail P  retract P  and all conclusions made from it . 
 Judge the plausibility of P  . 
 If implausible block this derivation . 
 It is worth noting that the need for retraction is not due to a failure to extract the correct ` least commitment ' proposition from the semantic content of the fragment Mary introduced . 
 This is due to the fact that it is possible to find pairs of possible continuations which are the negation of each other ( e.g. Mary introduced noone to anybody and Mary introduced someone to somebody ) . 
 The only proposition compatible with both a proposition , p , and its negation ,  p is the trivial proposition , T ( see Chater et al. for further discussion ) . 
 So far we have only considered semantic representations which do not involve quantifiers ( except for the existential quantifier introduced by the mechanism above ) . 
 In sentences with two or more quantifiers , there is generally an ambiguity concerning which quantifier has wider scope . 
 For example , in sentence  below the preferred reading is for the same kid to have climbed every tree ( i.e. the universal quantifier is within the scope of the existential ) whereas in sentence  the preferred reading is where the universal quantifier has scope over the existential . 
 Scope preferences sometimes seem to be established before the end of a sentence . 
 For example , in sentence  below , there seems a preference for an outer scope reading for the first quantifier as soon as we interpret child . 
 In  the preference , by the time we get to e.g. grammar , is for an inner scope reading for the first quantifier . 
 This intuitive evidence can be backed up by considering garden path effects with quantifier scope ambiguities ( called jungle paths by Barwise 1987 ) . 
 The original examples , such as the following ,
 showed that preferences for a particular scope are established and are overturned . 
 To show that preferences are sometimes established before the end of a sentence , and before a potential sentence end , we need to show garden path effects in examples such as the following : 
 Most psycholinguistic experimentation has been concerned with which scope preferences are made , rather than the point at which the preferences are established Kurtzman and MacDonald 1993 . 
 Given the intuitive evidence , our hypothesis is that scope preferences can sometimes be established early , before the end of a sentence . 
 This leaves open the possibility that in other cases , where the scoping information is not particularly of interest to the hearer , preferences are determined late , if at all . 
 Dealing with quantifiers incrementally is a rather similar problem to dealing with fragments of trees incrementally . 
 Just as it is impossible to predict the level of embedding of a noun phrase such as John from the fragment Mary thinks John , it is also impossible to predict the scope of a quantifier in a fragment with respect to the arbitrarily large number of quantifiers which might appear later in the sentence . 
 Again the problem can be avoided by a form of packing . 
 A particularly simple way of doing this is to use unscoped logical forms where quantifiers are left in situ ( similar to the representations used by Hobbs and Shieber 1987 , or to Quasi Logical Form Alshawi 1990 ) . 
 For example , the fragment Every man gives a book can be given the following representation : 
 Each quantified term consists of a quantifier , a variable and a restrictor , but no body . 
 To convert lambda expressions to unscoped propositions , we replace an occurrence of each argument with an empty existential quantifier term . 
 In this case we obtain : 
 Scoped propositions can then be obtained by using an outside-in quantifier scoping algorithm Lewin 1990 , or an inside-out algorithm with a free variable constraint Hobbs and Shieber 1987 . 
 The propositions formed can then be judged for plausibility . 
 To imitate jungle path phenomena , these plausibility judgements need to feed back into the scoping procedure for the next fragment . 
 For example , if every man is taken to be scoped outside a book after processing the fragment Every man gave a book , then this preference should be preserved when determining the scope for the full sentence Every man gave a book to a child . 
 Thus instead of doing all quantifier scoping at the end of the sentence , each new quantifier is scoped relative to the existing quantifiers ( and operators such as negation , intensional verbs etc ) . 
 A preliminary implementation achieves this by annotating the semantic representations with node names , and recording which quantifiers are ` discharged ' at which nodes , and in which order . 
 Dynamic semantics adopts the view that `` the meaning of a sentence does not lie in its truth conditions , but rather in the way in which it changes ( the representation of ) the information of the interpreter '' Groenendijk and Stokhof 1991 . 
 At first glance such a view seems ideally suited to incremental interpretation . 
 Indeed , Groenendijk and Stokhof claim that the compositional nature of Dynamic Predicate Logic enables one to `` interpret a text in an on-line manner , i.e. , incrementally , processing and interpreting each basic unit as it comes along , in the context created by the interpretation of the text so far '' . 
 Putting these two quotes together is , however , misleading , since it suggests a more direct mapping between incremental semantics and dynamic semantics than is actually possible . 
 In an incremental semantics , we would expect the information state of an interpreter to be updated word by word . 
 In contrast , in dynamic semantics , the order in which states are updated is determined by semantic structure , not by left-to-right order ( see e.g. Lewin 1992 for discussion ) . 
 For example , in Dynamic Predicate Logic Groenendijk and Stokhof 1991 , states are threaded from the antecedent of a conditional into the consequent , and from a restrictor of a quantifier into the body . 
 Thus , in interpreting , 
 the input state for evaluation of John will buy it right away is the output state from the antecedent a car impresses him . 
 In this case the threading through semantic structure is in the opposite order to the order in which the two clauses appear in the sentence . 
 Some intuitive justification for the direction of threading in dynamic semantics is provided by considering appropriate orders for evaluation of propositions against a database : the natural order in which to evaluate a conditional is first to add the antecedent , and then see if the consequent can be proven . 
 It is only at the sentence level in simple narrative texts that the presentation order and the natural order of evaluation necessarily coincide . 
 The ordering of anaphors and their antecedents is often used informally to justify left-to-right threading or threading through semantic structure . 
 However , threading from left-to-right disallows examples of optional cataphora , as in example  , and examples of compulsory cataphora as in : 
 Similarly , threading from the antecedents of conditionals into the consequent fails for examples such as : 
 It is also possible to get sentences with ` donkey ' readings , but where the indefinite is in the consequent : 
 This sentence seems to get a reading where we are not talking about a particular student ( an outer existential ) , or about a typical student ( a generic reading ) . 
 Moreover , as noted by Zeevat 1990 , the use of any kind of ordered threading will tend to fail for Bach-Peters sentences , such as : 
 For this kind of example , it is still possible to use a standard dynamic semantics , but only if there is some prior level of reference resolution which reorders the antecedents and anaphors appropriately . 
 For example , if  is converted into the ` donkey ' sentence : 
 When we consider threading of possible worlds , as in Update Semantics Veltman 1990 , the need to distinguish between the order of evaluation and the order of presentation becomes more clear cut . 
 Consider trying to perform threading in left-to-right order during interpretation of the sentence , John left if Mary left . 
 After processing the proposition John left the set of worlds is refined down to those worlds in which John left . 
 Now consider processing if Mary left . 
 Here we want to reintroduce some worlds , those in which neither Mary or John left . 
 However , this is not allowed by Update Semantics which is eliminative : each new piece of information can only further refine the set of worlds . 
 It is worth noting that the difficulties in trying to combine eliminative semantics with left-to-right threading apply to constraint-based semantics as well as to Update Semantics . 
 Haddock 1987 uses incremental refinement of sets of possible referents . 
 For example , the effect of processing the rabbit in the noun phrase the rabbit in the hat is to provide a set of all rabbits . 
 The processing of in refines this set to rabbits which are in something . 
 Finally , processing of the hat refines the set to rabbits which are in a hat . 
 However , now consider processing the rabbit in none of the boxes . 
 By the time the rabbit in has been processed , the only rabbits remaining in consideration are rabbits which are in something . 
 This incorrectly rules out the possibility of the noun phrase referring to a rabbit which is in nothing at all . 
 The case is actually a parallel to the earlier example of Mary introduced someone to something being inappropriate if the final sentence is Mary introduced noone to anybody . 
 Although this discussion has argued that it is not possible to thread the states which are used by a dynamic or eliminative semantics from left to right , word by word , this should not be taken as an argument against the use of such a semantics in incremental interpretation . 
 What is required is a slightly more indirect approach . 
 In the present implementation , semantic structures ( akin to logical forms ) are built word by word , and each structure is then evaluated independently using a dynamic semantics ( with threading performed according to the structure of the logical form ) . 
 At present there is a limited implementation , which performs a mapping from sentence fragments to fully scoped logical representations . 
 To illustrate its operation , consider the following discourse : 
 We assume that the first sentence has been processed , and concentrate on processing the fragment . 
 The implementation consists of five modules : 
 A word-by-word incremental parser for a lexicalised version of dependency grammar Milward 1992 . 
 This takes fragments of sentences and maps them to unscoped logical forms . 
 A module which replaces lambda abstracted variables with existential quantifiers in situ . 
 A pronoun coindexing procedure which replaces pronoun variables with a variable from the same sentence , or from the preceding context . 
 An outside-in quantifier scoping algorithm based on Lewin 1990 . 
 An ` evaluation ' procedure based on Lewin 1992 , which takes a logical form containing free variables ( such as the w in the LF above ) , and evaluates it using a dynamic semantics in the context given by the preceding sentences . 
 The output is a new logical form representing the context as a whole , with all variables correctly bound . 
 At present , the coverage of module  is limited , and module  is a naive coindexing procedure which allows a pronoun to be coindexed with any quantified variable or proper noun in the context or the current sentence . 
 The paper described some potential applications of incremental interpretation . 
 It then described the series of steps required in mapping from initial fragments of sentences to propositions which can be judged for plausibility . 
 Finally , it argued that the apparently close relationship between the states used in incremental semantics and dynamic semantics fails to hold below the sentence level , and briefly presented a more indirect way of using dynamic semantics in incremental interpretation . 
