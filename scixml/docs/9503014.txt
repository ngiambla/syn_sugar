 Despite the large amount of theoretical work done on non-constituent coordination during the last two decades , many computational systems still treat coordination using adapted parsing strategies , in a similar fashion to the SYSCONJ system developed for ATNs . 
 This paper reviews the theoretical literature , and shows why many of the theoretical accounts actually have worse coverage than accounts based on processing . 
 Finally , it shows how processing accounts can be described formally and declaratively in terms of Dynamic Grammars . 
 This paper is concerned with symmetrical coordination , where the order of the conjuncts ( the items being coordinated by a conjunction such as and or or ) can be altered without affecting acceptability . 
 Coordination of this kind is traditionally split into constituent coordination , where each conjunct forms a constituent according to ` standard ' phrase structure grammars , and non-constituent coordination . 
 Constituent and non-constituent coordination have been treated as entirely separate phenomena ( see van Oirsouw 1987 for discussion ) , and different mechanisms have been proposed for each . 
 However , by considering grammaticality judgements alone , there seems little justification for such a division . 
 To illustrate this , consider the sentence : 
 Each of the final proper substrings of the sentence ( i.e. some books , Mary some books etc. ) can be used as a conjunct e.g. 
 Similarly , each of the initial substrings can be used as a conjunct e.g . 
 and so can each of the middle substrings e.g . 
 Only examples  are constituent coordinations . 
 Example  seems slightly unnatural , but it is much improved if we replace books by a heavier string such as books about gardening . 
 Thus , for this example , any substring of the sentence can form a viable conjunct . 
 In the last twenty to thirty years there have been a series of accounts of coordination involving various deletion mechanisms ( from e.g. Gleitman 1965 to van Oirsouw 1987 ) . 
 For example , from the following ` antecedent ' sentence , 
 van Oirsouw allows deletion of words to the left and to the right of the conjunction , 
 resulting in the sentence : 
 Most deletion accounts assume that deletion is performed under identity of words , but don't analyse what it means for two words to be identical ( an exception is van Oirsouw who discusses phonological , morphological and referential identity ) . 
 Consider the following example of deletion . 
 Here the two cases of drive are phonologically identical , but have different syntactic categories . 
 Now consider : 
 These are cases of ` zeugma ' and are unacceptable except as jokes . 
 It therefore seems that the deleted words must have the same major syntactic category , and the same lexical meaning . 
 However , even if we fix both syntactic category and lexical meaning , we still get some weird coordinations . 
 For example , consider : 
 In example  the two prepositions are attached differently , one to the verb saw , the other to the noun , man . 
 In example  , attributed to Paul Dekker , the two conjuncts require Mary 's handbag to have a different syntactic structure : the bracketing appropriate for the first conjunct is [ [ a friend of Mary ] 's handbag ] . 
 The unacceptability of these examples suggests that word by word identity is insufficient , and that deleted material must have identical syntactic structure , as well as identical lexical meanings . 
 Some of the most compelling arguments against deletion have been semantic . 
 For example , Lakoff and Peters 1969 argued that deletion accounts are inappropriate for certain constituent coordinations such as : 
 since the ` antecedent ' sentence John are alike and Mary are alike is nonsensical ( it is also ungrammatical if we consider number agreement ) . 
 However , semantically inappropriate or nonsensical ` antecedents ' are also possible when we consider non-constituent coordination . 
 For example , consider ` antecedents ' for the following : 
  is non-constituent coordination under the primary reading where the scope of former does not contain living in England i.e. where the semantic bracketing is : 
 Examples  and  could be expanded out at the NP level , but not at the S level . 
 However  cannot be expanded out at any constituent level , whilst retaining an appropriate semantics . 
 For example , expansion at the VP level gives : 
 Thus , although Lakoff and Peters 's arguments count against standard deletion analyses , they do not count as general arguments against a unified treatment of constituent and non-constituent coordination . 
 Consider the sentence : 
 Instead of thinking of John gave and by Chomsky as deleted , we can also think of them as shared by the two conjuncts . 
 This structure can be represented as follows : 
 From the result of the previous section , each conjunct must share not just the phonological material , but also the syntactic structure and the lexical meanings . 
 There are three main methods by which this sharing of structure can be achieved : phrasal coordination , 3-D coordination , and processing strategies . 
 At first sight , analysing non-constituent coordination using phrasal ( i.e. constituent ) coordination seems nonsensical . 
 This is not the case . 
 Coordinations are classified as non-constituent coordination if the conjuncts fail to be constituents in a ` standard ' phrase structure grammar . 
 However , they may well be constituents in other grammars . 
 For example , it has been argued that the weaker notion of constituency provided by Categorial Grammars is exactly what is required to allow all conjuncts to be treated as constituents Steedman 1985 . 
 Phrasal coordination is exemplified by the schema : X  X Conj X The shared material is necessarily treated identically for each conjunct since there is only a single copy : the conjunction is embedded in a single syntax tree . 
 The phrasal coordination schema requires each conjunct to be given a single type , and for the conjuncts and the conjunction as a whole to be of the same type . 
 Problems with the latter requirement were pointed out by Sag et al. 1985 , who gave the following counterexamples : 
 Sag et al. deal with these examples by treating categories as feature bundles , and allowing coordination in cases where there are features in common . 
 For example , the two conjuncts in  share the feature +MANNER . 
 As it stands , the account does not deal with examples such as the following , 
 Here the adverbial phrase would presumably be +MANNER , and the prepositional phrase , +TEMP . 
 Further examples which are problematic for Sag et al. are given by Jorgensen and Abeille 1992 . 
 An alternative , suggested by Morrill 1990 and similar to Jorgensen and Abeille 1992 , is to use the following coordination schema : 
 X  Y  X Conj Y 
 This does not impose any condition that the two categories X and Y share anything in common . 
 However , the new category X  Y is used to ensure that both categories are appropriate in the context . 
 For example ,  is acceptable since the coordination type is NP  AP , and is subcategorises for both NPs and APs . 
 A rather more difficult problem is that of providing types for all possible conjuncts . 
 Consider the following : 
  is a conjunction of two pairs of noun phrases . 
  is a case of ` unbounded Right-Node Raising ' where the noun phrase Peter is embedded at different depths in the two conjuncts . 
 There have been two main approaches to dealing with examples such as  using phrasal coordination . 
 The first is to introduce an explicit product operator Wood 1988 , allowing types of the form NP * NP . 
 The second is to use a calculus in which types can undergo ` type-raising ' Dowty 1988 , or can be formed by abstraction ( as in the Lambek Calculus , Lambek 1958 ) . 
 The effect is to treat Fred a book as a verb phrase missing its verb . 
 The advantage of adopting a general abstraction mechanism , as in the Lambek Calculus , is that this also provides a treatment of examples such as  . 
 Unfortunately , the ability to perform abstraction of categories with functional types ( which is required for  ) also allows shared material to get different syntactic analyses , resulting in acceptance of all the sentences predicted by deletion accounts where identity of lexical categories and lexical semantics is respected , but not identity of syntactic structure . 
 Reconsider : 
 We can obtain identical syntactic types for a friend of and the manufacturer of by subtracting the lexical types of I , saw , Mary , 's , and handbag from the sentence type S . 
 Since the types are identical , coordination can then take place . 
 Thus the ability to ` subtract ' one type from another allows the Lambek Calculus to replicate a deletion account , and it thereby suffers from the same problems . 
 There have been some proposals to restrict the Lambek Calculus in order to prevent such overgeneration . 
 Barry and Pickering 1993 propose a calculus in which  is dealt with using a product operation , and abstraction is limited to categories which do not act as a function in the derivation . 
 This account makes reasonably good empirical predictions , though it does fail for the following examples : 
 In  , each conjunct contains different numbers of modifiers of different types ( an adverbial phrase with two prepositional phrases ) . 
 In  the subcategorisation order is swapped in the two conjuncts . 
 Successful treatment of non-constituent coordination using phrasal coordination seems to require elaborate encoding in the conjunct type of a simple generalisation : conjuncts can coordinate provided they are acceptable within the same syntactic context . 
 The 3-D approaches and processing strategies use syntactic context more directly , and it is to these methods which we now turn . 
 Let us briefly reconsider our explanation of deletion . 
 Example  was explained by saying that the two strings by Chomsky and Sue gave are deleted under some notion of identity . 
 However , we could equally well have described this as a process whereby the first instance of by Chomsky is merged with the second ( under some notion of identity ) , and the second instance of Sue gave is merged with the first . 
 Merging word strings instead of deleting them does not help with the problems of deletion accounts which we outlined earlier . 
 In particular , it does not help to exclude examples  and  which suggest shared material must have identical syntactic structure . 
 However , once we have started to think in terms of merging , there is an obvious next step , which is to move from merging of word strings to merging of syntax trees . 
 This is the move made by Goodall 1987 , who advocates treating coordination as a union of phrase markers : `` a ` pasting together ' one on top of the other of two trees , with any identical nodes merging together '' Goodall 1987  . 
 We can visualise the result in terms of a three-dimensional tree structure , where the merged material is on one plane , and the syntax trees for each conjunct are on two other planes . 
 For example , consider the 3-D tree for example  given in Fig.  . 
 The merged part of the tree includes all the nodes which dominate the shared material Sue gave . 
 The conjuncts retain separate planes ( denoted here by using stars or crosses for branches ) . 
 Goodall 's account does not deal with examples such as  , which he argues to be examples of a different phenomenon . 
 However these can be incorporated into a 3-D account Moltmann 1992 . 
 There are various technical difficulties with Goodall 's account van Oirsouw 1987 , Moltmann 1992 . 
 There is also a fundamental problem concerning semantic interpretation of coordinated structures ( see Moltmann 1992 which provides a revised and more complex 3-D account based on Muadz 1991 ) . 
 For coordination of unlike categories , as in the examples in  , Goodall proposes a treatment somewhat similar to Sag et al. 1985 . 
 However there is still a problem in dealing with examples where there are different numbers of modifiers , such as  or the following : 
 The syntactic structure appropriate for TNT deliver efficiently has one S node and two VP nodes . 
 However , the structure for TNT deliver after 5 pm in Edinburgh requires one S node and three VP nodes ( or three S nodes and one VP node ) . 
 The two structures therefore fail to merge since the structure dominating the shared material TNT deliver must be identical . 
 The use of ordered phrase structure trees also excludes examples such as  . 
 In summary , the 3-D approaches correctly enforce identity of syntactic structure for shared material . 
 However , the way of characterising syntactic structure using ( parts of ) standard phrase structure trees results in an overly strict requirement of parallelism between the conjuncts . 
 We will now consider processing strategies , where syntactic structure of shared material is characterised more indirectly by the state of the parser . 
 There have been several attempts to treat coordination by adapting pre-existing parsing strategies . 
 For example , ATNs were adapted by Woods 1973 , DCGs by Dahl and McCord 1983 , and chart parsers by Haugeneder 1992 . 
 Woods and Dahl and McCord 's system are similar . 
 Haugeneder 's system has very limited coverage . 
 In Wood 's SYSCONJ system , the parser can back up to various points in the history of the parse , and parse the second conjunct according to the configuration found . 
 For example , in parsing , 
 at the point after encountering and , the parser can reaccess the configuration after parsing John gave i.e. a stack consisting of a sentence and a verb-phrase , and an arc traversal by the verb . 
 The second conjunct is then parsed according to this configuration . 
 SYSCONJ does not immediately merge the two stack configurations after completing the second conjunct , but , instead , separately parses both conjuncts in parallel until a constituent is completed . 
 For example , on parsing the sentence ,  
 John gave Mary a book and Peter a paper about subjacency 
 the SYSCONJ system separately parses Peter a paper about subjacency and Mary a book about subjacency before conjoining at the level of some enclosing constituent ( for example the verb phase ) . 
 The result is therefore similar to starting with the sentence : 
 As noted by Dahl and McCord , this mechanism means that SYSCONJ inherits the problems of nonsensical semantics which plague the deletion accounts , since John and Mary are alike is treated the same as John are alike and Mary are alike . 
 The mechanism also causes problems for dealing with nested coordination . 
 Consider the sentence : 
 The smallest constituent containing to study medicine when he was eleven is the verb phase wanted to study medicine when he was eleven . 
 However , if coordination of the first two conjuncts occurs at this level , it is difficult to see how to deal with the final conjunct . 
 Both Woods and Dahl and McCord use stack based configurations rather than a full parsing history . 
 Thus once something is popped off the stack its internal structure cannot be accessed by the coordination routine . 
 This rules out examples such as the following , 
 where the NP , some books is completed prior to the conjunction being reached . 
 Although processing accounts can provide reasonable coverage of the coordination data , the exact predictions often require detailed examination of the code . 
 This suggests a need for the more abstract level of description which dynamic grammars provide . 
 Dynamics is just the study of states and transitions between states . 
 It can be used to specify the states of a left to right parser and the possible mappings between states . 
 For example , Milward 1994a provides a dynamic description of a shift reduce parser , and a dynamic description of a fully incremental parser based on dependency grammar . 
 Suitable languages for dynamics are both formal and declarative , and are therefore also appropriate to express linguistic generalisations . 
 In a Dynamic Grammar Milward 1994a , each word is regarded as an action which performs some change in the syntactic and semantic context . 
 For example , a parse of the sentence John likes Mary becomes a mapping between an initial state , c  , through some intermediate states , c  , c  to a final state c  i.e. c   c   c   c  If we use a dynamic grammar to describe a shift reduce parser , states encode the current stack configuration , and are related by rules which correspond to shifting and reducing . 
 Since there are arbitrarily large numbers of different stack configurations ( the stack can be of arbitrary size ) , the dynamics for shift reduce parsing involves the use of an infinite number of states . 
 It thus differs from , say ATNs Woods 1973 , which have a finite number of states , augmented by an explicit recursion mechanism . 
 Dynamic grammars can be presented as rewrite grammars by using transition types instead of the more usual S or NP . 
 For example , to get the parse above we need the lexical entries : 
 and a single combination rule schema which states that , 
 A string of words is a sentence if it has the type , c  c  where c  and c  are appropriate initial and final states for a parse . 
 In a dynamic grammar , any substring of a sentence can be assigned a type . 
 For example , likes and Mary can be combined to get the type c   c  . 
 Thus we have an appropriate level to perform substring coordination . 
 Dynamic grammars may be extended using the following combination rule ( and and or are both given the special transition type CONJ ) : F
 Similar to SYSCONJ , this allows coordination when two conjuncts map between the same pairs of states . 
 Processing is also similar , with the encountering of a conjunction causing back-up to an earlier stage in the parsing history . 
 However , since there is no popping of a stack , the full parsing history is available . 
 For example , Ben gave some books to Sue has the transitions : 
 we can then parse papers to Joe using the transitions : 
 Since the final state c  matches the state immediately before the conjunction , the two strings can combine . 
 The resulting transition diagram is as follows : 
 Iterated coordination ( e.g. for examples such as Mary , Peter and Sue ) can be treated in the same way as iterated constituent coordination is treated in phrase structure grammars . 
 For example , each transition type can be augmented with a feature ( +/ - ) denoting whether or not that transition has been iterated . 
 The coordination rule becomes : 
 Iterated types are formed as follows : 
 The precise grammaticality predictions made by the dynamic approach depend upon the characterisation of the states , and hence depend on the particular parsing strategy which is specified by the dynamics . 
 However there are some general predictions which can be made . 
 Firstly , consider conjuncts which correspond one to one in the categories of the corresponding words . 
 Here the conjuncts must provide the same transitions , and hence must be able to coordinate ( this is a reflection of the fact that processing can back up to any point in the parsing history ) . 
 This predicts that any substring of a sentence can coordinate with itself , and hence that any substring of a sentence can act as a conjunct . 
 For convenience we will call this the substring hypothesis . 
 This hypothesis has been broadly adopted in the work of van Oirsouw 1987 , Barry and Pickering 1993 , and by work on the Lambek Calculus Moortgat 1988 . 
 Apparent counterexamples are as follows : 
 However it is difficult to exclude these using syntactic constraints , without also excluding the more acceptable : 
 More natural examples where conjuncts are formed by fragments from different constituents are the following : 
 The relative unacceptability of the examples in  is perhaps best explained as due to violations of intonational requirements , rather than syntactic requirements Steedman 1989 . 
 One case where the dynamic grammars correctly violate the substring hypothesis is when a string already involves a coordination . 
 Here , the internal states are not accessible , so we can't get interleaving of two coordinations , as in : 
 There may be an argument for similarly blocking coordination in cases which would involve the breaking apart of idioms or other structures which are not standard cases of lexical subcategorisation . 
 An example ( due to Mark Steedman ) , which may be such a case , is the following , 
 As noted above , the precise grammaticality predictions depend on the kind of parsing model which is encoded in the states . 
 In Milward 1992 , the dynamics specifies a word-by-word incremental parser for a lexicalised version of dependency grammar . 
 Each state is a recursively defined category , similar to a category in Categorial Grammar . 
 For example , after parsing You can call me one possible state is a sentence missing a sentence modifier . 
 This state is appropriate as the initial state for a parse of both directly , or of after 3 pm through my secretary , resulting in a final state of category sentence . 
 Thus examples such as  are dealt with , since the syntactic context after You can call me does not distinguish between one or more than one subsequent modifier . 
 This lack of distinction as to whether one or more modifier is expected is actually a necessary prerequisite for performing decidable fully word-by-word incremental interpretation Milward and Cooper 1994 . 
 Some of the problems with categorial grammar accounts of coordination do reoccur with a dynamic account based on the parser used in Milward 1992 . 
 For example , 
 is predicted to be acceptable , as are the following , 
 This second batch of examples is particularly difficult to exclude without making changes to the characterisation of the states . 
 A feature plus or minus tensed verb on each conjunct does block them , but is difficult to motivate . 
 Dynamic grammars can be regarded purely as formal systems , as direct representations of processing , or as something inbetween ( for example , in the packed parallel parser described in Milward 1994a , the actual parsing states are packed versions of the states in the grammar ) . 
 If we consider the dynamics to be a direct representation of processing , then a dependence of linguistic data upon parsing states would only seem plausible if the parsing process corresponds , at least to some extent , with actual human language processing . 
 This brings up the intriguing possibility that we can predict coordination facts from known processing data , and vice versa . 
 For example , consider the well known example of garden pathing : 
 The choice between the use of raced as the main verb , or as part of the reduced relative is usually assumed to be within the fragment the horse raced , suggesting that there are two distinguished parsing states after raced . 
 Thus this correctly predicts the unacceptability of the following : 
 This paper has sketched various problems with some of the linguistic accounts of coordination . 
 It suggested that this was primarily due to difficulty in encoding a proper notion of syntactic context . 
 The paper then considered various processing accounts , where the syntactic context is encoded within the state of the parser . 
 Finally it showed how dynamics can be used as a formal description of processing accounts which use a full parsing history , and how the characterisations of parsing states can be chosen to enforce the requisite degree of parallelism between conjuncts . 
