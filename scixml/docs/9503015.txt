 The paper describes a parser for Categorial Grammar which provides fully word by word incremental interpretation . 
 The parser does not require fragments of sentences to form constituents , and thereby avoids problems of spurious ambiguity . 
 The paper includes a brief discussion of the relationship between basic Categorial Grammar and other formalisms such as HPSG , Dependency Grammar and the Lambek Calculus . 
 It also includes a discussion of some of the issues which arise when parsing lexicalised grammars , and the possibilities for using statistical techniques for tuning to particular languages . 
 There is a large body of psycholinguistic evidence which suggests that meaning can be extracted before the end of a sentence , and before the end of phrasal constituents Marslen-Wilson 1973 , Tanenhaus et al. 1990 . 
 There is also recent evidence suggesting that , during speech processing , partial interpretations can be built extremely rapidly , even before words are completed Spivey-Knowlton et al. 1994 . 
 There are also potential computational applications for incremental interpretation , including early parse filtering using statistics based on logical form plausibility , and interpretation of fragments of dialogues ( a survey is provided by Milward and Cooper 1994 , henceforth referred to as M and C ) . 
 In the current computational and psycholinguistic literature there are two main approaches to the incremental construction of logical forms . 
 One approach is to use a grammar with ` non-standard ' constituency , so that an initial fragment of a sentence , such as John likes , can be treated as a constituent , and hence be assigned a type and a semantics . 
 This approach is exemplified by Combinatory Categorial Grammar , CCG Steedman 1991 , which takes a basic CG with just application , and adds various new ways of combining elements together . 
 Incremental interpretation can then be achieved using a standard bottom-up shift reduce parser , working from left to right along the sentence . 
 The alternative approach , exemplified by the work of Stabler on top-down parsing Stabler 1991 , and Pulman on left-corner parsing Pulman 1986 is to associate a semantics directly with the partial structures formed during a top-down or left-corner parse . 
 For example , a syntax tree missing a noun phrase , such as the following  
 can be given a semantics as a function from entities to truth values i.e.  x. likes ( john , x ) , without having to say that John likes is a constituent . 
 Neither approach is without problems . 
 If a grammar is augmented with operations which are powerful enough to make most initial fragments constituents , then there may be unwanted interactions with the rest of the grammar ( examples of this in the case of CCG and the Lambek Calculus are given in Section  ) . 
 The addition of extra operations also means that , for any given reading of a sentence there will generally be many different possible derivations ( so-called ` spurious ' ambiguity ) , making simple parsing strategies such as shift-reduce highly inefficient . 
 The limitations of the parsing approaches become evident when we consider grammars with left recursion . 
 In such cases a simple top-down parser will be incomplete , and a left corner parser will resort to buffering the input ( so won't be fully word-by-word ) . 
 M and C illustrate the problem by considering the fragment Mary thinks John . 
 This has a small number of possible semantic representations ( the exact number depending upon the grammar ) e.g. 
 The second representation is appropriate if the sentence finishes with a sentential modifier . 
 The third allows there to be a verb phrase modifier . 
 If the semantic representation is to be read off syntactic structure , then the parser must provide a single syntax tree ( possibly with empty nodes ) . 
 However , there are actually any number of such syntax trees corresponding to , for example , the first semantic representation , since the np and the s can be arbitrarily far apart . 
 The following tree is suitable for the sentence Mary thinks John shaves but not for e.g. Mary thinks John coming here was a mistake . 
 M and C suggest various possibilities for packing the partial syntax trees , including using Tree Adjoining Grammar Joshi 1987 or Description Theory Marcus et al. 1983 . 
 One further possibility is to choose a single syntax tree , and to use destructive tree operations later in the parse . 
 The approach which we will adopt here is based on Milward 1992, Milward 1994a . 
 Partial syntax trees can be regarded as performing two main roles . 
 The first is to provide syntactic information which guides how the rest of the sentence can be integrated into the tree . 
 The second is to provide a basis for a semantic representation . 
 The first role can be captured using syntactic types , where each type corresponds to a potentially infinite number of partial syntax trees . 
 The second role can be captured by the parser constructing semantic representations directly . 
 The general processing model therefore consists of transitions of the form : 
 This provides a state-transition or dynamic model of processing , with each state being a pair of a syntactic type and a semantic value . 
 The main difference between our approach and that of Milward 1992 , Milward 1994a is that it is based on a more expressive grammar formalism , Applicative Categorial Grammar , as opposed to Lexicalised Dependency Grammar . 
 Applicative Categorial Grammars allow categories to have arguments which are themselves functions ( e.g. very can be treated as a function of a function , and given the type (n/n)/(n/n) when used as an adjectival modifier ) . 
 The ability to deal with functions of functions has advantages in enabling more elegant linguistic descriptions , and in providing one kind of robust parsing : the parser never fails until the last word , since there could always be a final word which is a function over all the constituents formed so far . 
 However , there is a corresponding problem of far greater non-determinism , with even unambiguous words allowing many possible transitions . 
 It therefore becomes crucial to either perform some kind of ambiguity packing , or language tuning . 
 This will be discussed in the final section of the paper . 
 Applicative Categorial Grammar is the most basic form of Categorial Grammar , with just a single combination rule corresponding to function application . 
 It was first applied to linguistic description by Adjukiewicz and Bar-Hillel in the 1950s . 
 Although it is still used for linguistic description Bouma and van Noord 1994 , it has been somewhat overshadowed in recent years by HPSG Pollard and Sag 1994 , and by Lambek Categorial Grammars Lambek 1958 . 
 It is therefore worth giving some brief indications of how it fits in with these developments . 
 The first directed Applicative CG was proposed by Bar-Hillel 1953 . 
 Functional types included a list of arguments to the left , and a list of arguments to the right . 
 Translating Bar-Hillel 's notation into a feature based notation similar to that in HPSG Pollard and Sag 1994 , we obtain the following category for a ditransitive verb such as put : 
 The list of arguments to the left are gathered under the feature , l , and those to the right , an np and a pp in that order , under the feature r . 
 Bar-Hillel employed a single application rule , which corresponds to the following : 
 The result was a system which comes very close to the formalised dependency grammars of Gaifman 1965 and Hays 1964 . 
 The only real difference is that Bar-Hillel allowed arguments to themselves be functions . 
 For example , an adverb such as slowly could be given the type 
 An unfortunate aspect of Bar-Hillel 's first system was that the application rule only ever resulted in a primitive type . 
 Hence , arguments with functional types had to correspond to single lexical items : there was no way to form the type np  s for a non-lexical verb phrase such as likes Mary . 
 Rather than adapting the Application Rule to allow functions to be applied to one argument at a time , Bar-Hillel 's second system ( often called AB Categorial Grammar , or Adjukiewicz  / Bar-Hillel CG , Bar-Hillel 1964 ) adopted a ` Curried ' notation , and this has been adopted by most CGs since . 
 To represent a function which requires an np on the left , and an np and a pp to the right , there is a choice of the following three types using Curried notation : 
 Most CGs either choose the third of these ( to give a vp structure ) , or include a rule of Associativity which means that the types are interchangeable ( in the Lambek Calculus , Associativity is a consequence of the calculus , rather than being specified separately ) . 
 The main impetus to change Applicative CG came from the work of Ades and Steedman 1982 . 
 Ades and Steedman noted that the use of function composition allows CGs to deal with unbounded dependency constructions . 
 Function composition enables a function to be applied to its argument , even if that argument is incomplete e.g. 
 This allows peripheral extraction , where the ` gap ' is at the start or the end of e.g. a relative clause . 
 Variants of the composition rule were proposed in order to deal with non-peripheral extraction , but this led to unwanted effects elsewhere in the grammar Bouma 1987 . 
 Subsequent treatments of non-peripheral extraction based on the Lambek Calculus ( where standard composition is built in : it is a rule which can be proven from the calculus ) have either introduced an alternative to the forward and backward slashes i.e. / and  for normal args ,  for wh-args Moortgat 1988 , or have introduced so called modal operators on the wh-argument Morrill et al. 1990 . 
 Both techniques can be thought of as marking the wh-arguments as requiring special treatment , and therefore do not lead to unwanted effects elsewhere in the grammar . 
 However , there are problems with having just composition , the most basic of the non-applicative operations . 
 In CGs which contain functions of functions ( such as very , or slowly ) , the addition of composition adds both new analyses of sentences , and new strings to the language . 
 This is due to the fact that composition can be used to form a function , which can then be used as an argument to a function of a function . 
 For example , if the two types , n / n and n / n are composed to give the type n / n , then this can be modified by an adjectival modifier of type ( n / n ) / ( n / n ) . 
 Thus , the noun very old dilapidated car can get the unacceptable bracketing , [ [ very [ old dilapidated ] ] car ] . 
 Associative CGs with Composition , or the Lambek Calculus also allow strings such as boy with the to be given the type n / n predicting very boy with the car to be an acceptable noun . 
 Although individual examples might be possible to rule out using appropriate features , it is difficult to see how to do this in general whilst retaining a calculus suitable for incremental interpretation . 
 If wh-arguments need to be treated specially anyway ( to deal with non-peripheral extraction ) , and if composition as a general rule is problematic , this suggests we should perhaps return to grammars which use just Application as a general operation , but have a special treatment for wh-arguments . 
 Using the non-Curried notation of Bar-Hillel , it is more natural to use a separate wh-list than to mark wh-arguments individually . 
 For example , the category appropriate for relative clauses with a noun phrase gap would be : 
 It is then possible to specify operations which act as purely applicative operations with respect to the left and right arguments lists , but more like composition with respect to the wh-list . 
 This is very similar to the way in which wh-movement is dealt with in GPSG Gazdar et al. 1985 and HPSG , where wh-arguments are treated using slash mechanisms or feature inheritance principles which correspond closely to function composition . 
 Given that our arguments have produced a categorial grammar which looks very similar to HPSG , why not use HPSG rather than Applicative CG ? 
 The main reason is that Applicative CG is a much simpler formalism , which can be given a very simple syntax semantics interface , with function application in syntax mapping to function application in semantics  . 
 This in turn makes it relatively easy to provide proofs of soundness and completeness for an incremental parsing algorithm . 
 Ultimately , some of the techniques developed here should be able to be extended to more complex formalisms such as HPSG . 
 In this section we define a grammar similar to Bar-Hillel 's first grammar . 
 However , unlike Bar-Hillel , we allow one argument to be absorbed at a time . 
 The resulting grammar is equivalent to AB Categorial Grammar plus associativity . 
 The categories of the grammar are defined as follows : 
 If X is a syntactic type ( e.g. s , np ) , then  is a category . 
 If X is a syntactic type , and L and R are lists of categories , then  is a category . 
 Application to the right is defined by the rule : 
 Application to the left is defined by the rule : 
 The basic grammar provides some spurious derivations , since sentences such as John likes Mary can be bracketed as either ( ( John likes ) Mary ) or ( John ( likes Mary ) ) . 
 However , we will see that these spurious derivations do not translate into spurious ambiguity in the parser , which maps from strings of words directly to semantic representations . 
 Most parsers which work left to right along an input string can be described in terms of state transitions i.e. by rules which say how the current parsing state ( e.g. a stack of categories , or a chart ) can be transformed by the next word into a new state . 
 Here this will be made particularly explicit , with the parser described in terms of just two rules which take a state , a new word and create a new state . 
 There are two unusual features . 
 Firstly , there is nothing equivalent to a stack mechanism : at all times the state is characterised by a single syntactic type , and a single semantic value , not by some stack of semantic values or syntax trees which are waiting to be connected together . 
 Secondly , all transitions between states occur on the input of a new word : there are no ` empty ' transitions ( such as the reduce step of a shift-reduce parser ) . 
 The two rules , which are given in Figure  , are difficult to understand in their most general form . 
 Here we will work upto the rules gradually , by considering which kinds of rules we might need in particular instances . 
 Consider the following pairing of sentence fragments with their simplest possible CG type : 
 Now consider taking each type as a description of the state that the parser is in after absorbing the fragment . 
 We obtain a sequence of transitions as follows : 
 If an embedded sentence such as John likes Sue is a mapping from an s / s to an s , this suggests that it might be possible to treat all sentences as mapping from some category expecting an s to that category i.e. from X / s to X . 
  Similarly , all noun phrases might be treated as mappings from an X / np to an X . 
 Now consider individual transitions . 
 The simplest of these is where the type of argument expected by the state is matched by the next word i.e. 
  This can be generalised to the following rule , which is similar to Function Application in standard CG 
 A similar transition occurs for likes . 
 Here an np  s was expected , but likes only provides part of this : it requires an np to the right to form an np  s . 
 Thus after likes is absorbed the state category will need to expect an np . 
 The rule required is similar to Function Composition in CG i.e. 
 Considering this informally in terms of tree structures , what is happening is the replacement of an empty node in a partial tree by a second partial tree i.e. 
 The two rules specified so far need to be further generalised to allow for the case where a lexical item has more than one argument ( e.g. if we replace likes by a di-transitive such as gives or a tri-transitive such as bets ) . 
 This is relatively trivial using a non-curried notation similar to that used for AACG . 
 What we obtain is the single rule of State-Application , which corresponds to application when the list of arguments , R  , is empty , to function composition when R  is of length one , and to n-ary composition when R  is of length n . 
 The only change needed from AACG notation is the inclusion of an extra feature list , the h list , which stores information about which arguments are waiting for a head ( the reasons for this will be explained later ) . 
 The lexicon is identical to that for a standard AACG , except for having h-lists which are always set to empty . 
 Now consider the first transition . 
 Here a sentence was expected , but what was encountered was a noun phrase , John . 
 The appropriate rule in CG notation would be : 
 This rule states that if looking for a Y and get a Z then look for a Y which is missing a Z . 
 In tree structure terms we have : 
 The rule of State-Prediction is obtained by further generalising to allow the lexical item to have missing arguments , and for the expected argument to have missing arguments . 
 State-Application and State-Prediction together provide the basis of a sound and complete parser . 
 Parsing of sentences is achieved by starting in a state expecting a sentence , and applying the rules non-deterministically as each word is input . 
 A successful parse is achieved if the final state expects no more arguments . 
 As an example , reconsider the string John likes Sue . 
 The sequence of transitions corresponding to John likes Sue being a sentence , is given in Figure  . 
 The transition on encountering John is deterministic : State-Application cannot apply , and State-Prediction can only be instantiated one way . 
 The result is a new state expecting an argument which , given an np could give an s i.e. an np  s . 
 The transition on input of likes is non-deterministic . 
 State-Application can apply , as in Figure  . 
 However , State-Prediction can also apply , and can be instantiated in four ways ( these correspond to different ways of cutting up the left and right subcategorisation lists of the lexical entry , likes , i.e. as  np  or  np  ) . 
 One possibility corresponds to the prediction of an s  s modifier , a second to the prediction of an ( np  s )  ( np  s ) modifier ( i.e. a verb phrase modifier ) , a third to there being a function which takes the subject and the verb as separate arguments , and the fourth corresponds to there being a function which requires an s / np argument . 
 The second of these is perhaps the most interesting , and is given in Figure  . 
 It is the choice of this particular transition at this point which allows verb phrase modification , and hence , assuming the next word is Sue , an implicit bracketing of the string fragment as ( John ( likes Sue ) ) . 
 Note that if State-Application is chosen , or the first of the State-Prediction possibilities , the fragment John likes Sue retains a flat structure . 
 If there is to be no modification of the verb phrase , no verb phrase structure is introduced . 
 This relates to there being no spurious ambiguity : each choice of transition has semantic consequences ; each choice affects whether a particular part of the semantics is to be modified or not . 
 Finally , it is worth noting why it is necessary to use h-lists . 
 These are needed to distinguish between cases of real functional arguments ( of functions of functions ) , and functions formed by State-Prediction . 
 Consider the following trees , where the np  s node is empty . 
 Both trees have the same syntactic type , however in the first case we want to allow for there to be an s  s modifier of the lower s , but not in the second . 
 The headed list distinguishes between the two cases , with only the first having an np on its headed list , allowing prediction of an s modifier . 
 When we consider full sentence processing , as opposed to incremental processing , the use of lexicalised grammars has a major advantage over the use of more standard rule based grammars . 
 In processing a sentence using a lexicalised formalism we do not have to look at the grammar as a whole , but only at the grammatical information indexed by each of the words . 
 Thus increases in the size of a grammar don't necessarily effect efficiency of processing , provided the increase in size is due to the addition of new words , rather than increased lexical ambiguity . 
 Once the full set of possible lexical entries for a sentence is collected , they can , if required , then be converted back into a set of phrase structure rules ( which should correspond to a small subset of the rule based formalism equivalent to the whole lexicalised grammar ) , before being parsing with a standard algorithm such as Earley 1970 's . 
 In incremental parsing we cannot predict which words will appear in the sentence , so cannot use the same technique . 
 However , if we are to base a parser on the rules given above , it would seem that we gain further . 
 Instead of grammatical information being localised to the sentence as a whole , it is localised to a particular word in its particular context : there is no need to consider a pp as a start of a sentence if it occurs at the end , even if there is a verb with an entry which allows for a subject pp . 
 However there is a major problem . 
 As we noted in the last paragraph , it is the nature of parsing incrementally that we don't know what words are to come next . 
 But here the parser doesn't even use the information that the words are to come from a lexicon for a particular language . 
 For example , given an input of 3 nps , the parser will happily create a state expecting 3 nps to the left . 
 This might be a likely state for say a head final language , but an unlikely state for a language such as English . 
 Note that incremental interpretation will be of no use here , since the semantic representation should be no more or less plausible in the different languages . 
 In practical terms , a naive interactive parallel Prolog implementation on a current workstation fails to be interactive in a real sense after about 8 words . 
 What seems to be needed is some kind of language tuning . 
 This could be in the nature of fixed restrictions to the rules e.g. for English we might rule out uses of prediction when a noun phrase is encountered , and two already exist on the left list . 
 A more appealing alternative is to base the tuning on statistical methods . 
 This could be achieved by running the parser over corpora to provide probabilities of particular transitions given particular words . 
 These transitions would capture the likelihood of a word having a particular part of speech , and the probability of a particular transition being performed with that part of speech . 
 There has already been some early work done on providing statistically based parsing using transitions between recursively structured syntactic categories Tugwell 1995 . 
 Unlike a simple Markov process , there are a potentially infinite number of states , so there is inevitably a problem of sparse data . 
 It is therefore necessary to make various generalisations over the states , for example by ignoring the R  lists . 
 The full processing model can then be either serial , exploring the most highly ranked transitions first ( but allowing backtracking if the semantic plausibility of the current interpretation drops too low ) , or ranked parallel , exploring just the n paths ranked highest according to the transition probabilities and semantic plausibility . 
 The paper has presented a method for providing interpretations word by word for basic Categorial Grammar . 
 The final section contrasted parsing with lexicalised and rule based grammars , and argued that statistical language tuning is particularly suitable for incremental , lexicalised parsing strategies . 
