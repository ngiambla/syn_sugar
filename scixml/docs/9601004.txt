 This paper proposes a method for measuring semantic similarity between words as a new tool for text analysis . 
 The similarity is measured on a semantic network constructed systematically from a subset of the English dictionary , LDOCE ( Longman Dictionary of Contemporary English ) . 
 Spreading activation on the network can directly compute the similarity between any two words in the Longman Defining Vocabulary , and indirectly the similarity of all the other words in LDOCE . 
 The similarity represents the strength of lexical cohesion or semantic relation , and also provides valuable information about similarity and coherence of texts . 
 A text is not just a sequence of words , but it also has coherent structure . 
 The meaning of each word in a text depends on the structure of the text . 
 Recognizing the structure of text is an essential task in text understanding Grosz and Sidner 1986 . 
 One of the valuable indicators of the structure of text is lexical cohesion Halliday and Hasan 1976 . 
 Lexical cohesion is the relationship between words , classified as follows : 
 Reiteration : 
 Semantic relation : 
 Reiteration of words is easy to capture by morphological analysis . 
 Semantic relation between words , which is the focus of this paper , is hard to recognize by computers . 
 We consider lexical cohesion as semantic similarity between words . 
 Similarity is computed by spreading activation ( or association ) Waltz and Pollack 1985 on a semantic network constructed systematically from an English dictionary . 
 Whereas it is edited by some lexicographers , a dictionary is a set of associative relation shared by the people in a linguistic community . 
 The similarity between words is a mapping  :  , where L is a set of words ( or lexicon ) . 
 The following examples suggest the feature of the similarity : 
 The value of  increases with strength of semantic relation between w and w ' . 
 The following section examines related work in order to clarify the nature of the semantic similarity . 
 Section  describes how the semantic network is systematically constructed from the English dictionary . 
 Section  explains how to measure the similarity by spreading activation on the semantic network . 
 Section  shows applications of the similarity measure -- computing similarity between texts , and measuring coherence of a text . 
 Section  discusses the theoretical aspects of the similarity . 
 Words in a language are organized by two kinds of relationship . 
 One is a syntagmatic relation : how the words are arranged in sequential texts . 
 The other is a paradigmatic relation : how the words are associated with each other . 
 Similarity between words can be defined by either a syntagmatic or a paradigmatic relation . 
 Syntagmatic similarity is based on co-occurrence data extracted from corpora Church and Hanks 1990 , definitions in dictionaries Wilks et al. 1989 , and so on . 
 Paradigmatic similarity is based on association data extracted from thesauri Morris and Hirst 1991 , psychological experiments Osgood 1952 , and so on . 
 This paper concentrates on paradigmatic similarity , because a paradigmatic relation can be established both inside a sentence and across sentence boundaries , while syntagmatic relations can be seen mainly inside a sentence -- like syntax deals with sentence structure . 
 The rest of this section focuses on two related works on measuring paradigmatic similarity -- a psycholinguistic approach and a thesaurus-based approach . 
 Psycholinguists have been proposed methods for measuring similarity . 
 One of the pioneering works is ` semantic differential ' Osgood 1952 which analyses meaning of words into a range of different dimensions with the opposed adjectives at both ends ( see Figure  ) , and locates the words in the semantic space . 
 Recent works on knowledge representation are somewhat related to Osgood 's semantic differential . 
 Most of them describe meaning of words using special symbols like microfeatures Waltz and Pollack 1985 , Hendler 1989 that correspond to the semantic dimensions . 
 However , the following problems arise from the semantic differential procedure as measurement of meaning . 
 The procedure is not based on the denotative meaning of a word , but only on the connotative emotions attached to the word ; it is difficult to choose the relevant dimensions , i.e. the dimensions required for the sufficient semantic space . 
 Morris and Hirst 1991 used Roget 's thesaurus as knowledge base for determining whether or not two words are semantically related . 
 For example , the semantic relation of truck / car and drive / car are captured in the following way : 
 This method can capture almost all types of semantic relations ( except emotional and situational relation ) , such as paraphrasing by superordinate ( ex . cat / pet ) , systematic relation ( ex .  north / east ) , and non-systematic relation ( ex .  theatre / film ) . 
 However , thesauri provide neither information about semantic difference between words juxtaposed in a category , nor about strength of the semantic relation between words -- both are to be dealt in this paper . 
 The reason is that thesauri are designed to help writers find relevant words , not to provide the meaning of words . 
 We analyse word meaning in terms of the semantic space defined by a semantic network , called Paradigme . 
 Paradigme is systematically constructed from Glossme , a subset of an English dictionary . 
 A dictionary is a closed paraphrasing system of natural language . 
 Each of its headwords is defined by a phrase which is composed of the headwords and their derivations . 
 A dictionary , viewed as a whole , looks like a tangled network of words . 
 We adopted Longman Dictionary of Contemporary English  LDOCE 1987 as such a closed system of English . 
 LDOCE has a unique feature that each of its 56,000 headwords is defined by using the words in Longman Defining Vocabulary ( hereafter , LDV ) and their derivations . 
 LDV consists of 2,851 words ( as the headwords in LDOCE ) based on the survey of restricted vocabulary West 1953 . 
 We made a reduced version of LDOCE , called Glossme . 
 Glossme has every entry of LDOCE whose headword is included in LDV . 
 Thus , LDV is defined by Glossme , and Glossme is composed of LDV . 
 Glossme is a closed subsystem of English . 
 Glossme has 2,851 entries that consist of 101,861 words ( 35.73 words / entry on the average ) . 
 An item of Glossme has a headword , a word-class , and one or more units corresponding to numbered definitions in the entry of LDOCE . 
 Each unit has one head-part and several det-parts . 
 The head-part is the first phrase in the definition , which describes the broader meaning of the headword . 
 The det-parts restrict the meaning of the head-part . 
 ( See Figure  . ) 
 We then translated Glossme into a semantic network Paradigme . 
 Each entry in Glossme is mapped onto a node in Paradigme . 
 Paradigme has 2,851 nodes and 295,914 unnamed links between the nodes ( 103.79 links / node on the average ) . 
 Figure  shows a sample node red_1 . 
 Each node consists of a headword , a word-class , an activity-value , and two sets of links : a rfrant and a rfr . 
 A rfrant of a node consists of several subrfrants correspond to the units of Glossme . 
 As shown in Figure  and  , a morphological analysis maps the word brownish in the second unit onto a link to the node brown_1 , and the word colour onto two links to colour_1 ( adjective ) and colour_2 ( noun ) . 
 A rfr of a node p records the nodes referring to p . 
 For example , the rfr of red_1 is a set of links to nodes ( ex. apple_1 ) that have a link to red_1 in their rfrants . 
 The rfr provides information about the extension of red_1 , not the intension shown in the rfrant . 
 Each link has thickness  , which is computed from the frequency of the word  in Glossme and other information , and normalized as  in each subrfrant or rfr . 
 Each subrfrant also has thickness ( for example , 0.333333 in the first subrfrant of red_1 ) , which is computed by the order of the units which represents significance of the definitions . 
 Appendix A describes the structure of Paradigme in detail . 
 Similarity between words is computed by spreading activation on Paradigme . 
 Each of its nodes can hold activity , and it moves through the links . 
 Each node computes its activity value  at time  as follows : 
 where  and  are the sum of weighted activity ( at time T ) of the nodes referred in the rfrant and rfr respectively . 
 And ,  is activity given from outside ( at time T ) ; to ` activate a node ' is to let  . 
 The output function  sums up three activity values in appropriate proportion and limits the output value to [ 0,1 ] . 
 Appendix B gives the details of the spreading activation . 
 Activating a node for a certain period of time causes the activity to spread over Paradigme and produce an activated pattern on it . 
 The activated pattern approximately gets equilibrium after 10 steps , whereas it will never reach the actual equilibrium . 
 The pattern thus produced represents the meaning of the node or of the words related to the node by morphological analysis . 
 The activated pattern , produced from a word w , suggests similarity between w and any headword in LDV . 
 The similarity  is computed in the following way . 
 ( See also Figure  ) 
 Reset activity of all nodes in Paradigme . 
 Activate w with strength s(w) for 10 steps , where s(w) is significance of the word w. 
 Then , an activated pattern P(w) is produced on Paradigme . 
 Observe  -- an activity value of the node w ' in P(w) . 
 Then ,  is  . 
 The word significance  is defined as the normalized information of the word w in the corpus West 1953 . 
 For example , the word red appears 2,308 times in the 5,487,056 - word corpus , and the word and appears 106,064 times . 
 So ,  and  are computed as follows : 
 We estimated the significance of the words excluded from the word list West 1953 at the average significance of their word classes . 
 This interpolation virtually enlarged West 's 5,000,000 - word corpus . 
 For example , let us consider the similarity between red and orange . 
 First , we produce an activated pattern  on Paradigme . 
 ( See Figure  . ) In this case , both of the nodes red_1 ( adjective ) and red_2 ( noun ) are activated with strength  . 
 Next , we compute  , and observe  . 
 Then , the similarity between red and orange is obtained as follows : 
 The procedure described above can compute the similarity  between any two words w , w ' in LDV and their derivations . 
 Computer programs of this procedure -- spreading activation ( in C ) , morphological analysis and others ( in Common Lisp ) -- can compute  within 2.5 seconds on a workstation ( SPARCstation 2 ) . 
 The similarity  between words works as an indicator of the lexical cohesion . 
 The following examples illustrate that  increases with the strength of semantic relation : 
 The similarity  also increases with the co-occurrence tendency of words , for example : 
 Note that  has direction ( from w to w ' ) , so that  may not be equal to  : 
 Meaningful words should have higher similarity ; meaningless words ( especially , function words ) should have lower similarity . 
 The similarity  increases with the significance s(w) and s ( w ' ) that represent meaningfulness of w and w ' : 
 Note that the reflective similarity  also depends on the significance s(w) , so that  : 
 The similarity of words in LDV and their derivations is measured directly on Paradigme ; the similarity of extra words is measured indirectly on Paradigme by treating an extra word as a word list  of its definition in LDOCE . 
 ( Note that each  is included in LDV or their derivations . ) 
 The similarity between the word lists W , W ' is defined as follows . 
 ( See also Figure  . ) 
 where P(w) is the activated pattern produced from W by activating each  with strength  for 10 steps . 
 And ,  is an output function which limits the value to [ 0,1 ] . 
 As shown in Figure  , bottle_1 and wine_1 have high activity in the pattern produced from the phrase `` red alcoholic drink '' . 
 So , we may say that the overlapped pattern implies `` a bottle of wine '' . 
 For example , the similarity between linguistics and stylistics , both are the extra words , is computed as follows : 
 Obviously , both  and  , where W is an extra word and w is not , are also computable . 
 Therefore , we can compute the similarity between any two headwords in LDOCE and their derivations . 
 This section shows the application of the similarity between words to text analysis -- measuring similarity between texts , and measuring text coherence . 
 Suppose a text is a word list without syntactic structure . 
 Then , the similarity  between two texts X , X ' can be computed as the similarity of extra words described above . 
 The following examples suggest that the similarity between texts indicates the strength of coherence relation between them : 
 It is worth noting that meaningless iteration of words ( especially , of function words ) has less influence on the text similarity : 
 The text similarity provides a semantic space for text retrieval -- to recall the most similar text in  to the given text X . 
 Once the activated pattern P ( X ) of the text X is produced on Paradigme , we can compute and compare the similarity  immediately . 
 ( See Figure  . ) . 
 Let us consider the reflective similarity  of a text X , and use the notation c ( X ) for  . 
 Then , c ( X ) can be computed as follows : 
 The activated pattern P ( X ) , as shown in Figure  , represents the average meaning of  . 
 So , c ( X ) represents cohesiveness of X -- or semantic closeness of  , or semantic compactness of X . 
 ( It is also closely related to distortion in clustering . ) 
 The following examples suggest that c ( X ) indicates the strength of coherence of X : 
 = 0.502510 ( coherent ) , 
 = 0.250840 ( incoherent ) . 
 However , a cohesive text can be incoherent ; the following example shows cohesiveness of the incoherent text -- three sentences randomly selected from LDOCE : 
 = 0.560172 ( incoherent , but cohesive ) . 
 Thus , c ( X ) can not capture all the aspects of text coherence . 
 This is because c ( X ) is based only on the lexical cohesion of the words in X . 
 The structure of Paradigme represents the knowledge system of English , and an activated state produced on it represents word meaning . 
 This section discusses the nature of the structure and states of Paradigme , and also the nature of the similarity computed on it . 
 The set of all the possible activated patterns produced on Paradigme can be considered as a semantic space where each state is represented as a point . 
 The semantic space is a 2,851 - dimensional hypercube ; each of its edges corresponds to a word in LDV . 
 LDV is selected according to the following information : the word frequency in written English , and the range of contexts in which each word appears . 
 So , LDV has a potential for covering all the concepts commonly found in the world . 
 This implies the completeness of LDV as dimensions of the semantic space . 
 Osgood 's semantic differential procedure used 50 adjective dimensions ; our semantic measurement uses 2,851 dimensions with completeness and objectivity . 
 Our method can be applied to construct a semantic network from an ordinary dictionary whose defining vocabulary is not restricted . 
 Such a network , however , is too large to spread activity over it . 
 Paradigme is the small and complete network for measuring the similarity . 
 The proposed similarity is based only on the denotational and intensional definitions in the dictionary LDOCE . 
 Lack of the connotational and extensional knowledge causes some unexpected results of measuring the similarity . 
 For example , consider the following similarity : 
 This is due to the nature of the dictionary definitions -- they only indicate sufficient conditions of the headword . 
 For example , the definition of tree in LDOCE tells nothing about leaves : tree n 1 a tall plant with a wooden trunk and branches , that lives for many years 2 a bush or other plant with a treelike form 3 a drawing with a branching form , esp . 
 as used for showing family relationships However , the definition is followed by pictures of leafy trees providing readers with connotational and extensional stereotypes of trees . 
 In the proposed method , the definitions in LDOCE are treated as word lists , though they are phrases with syntactic structures . 
 Let us consider the following definition of lift : 
 Anyone can imagine that something is moving upward . 
 But , such a movement can not be represented in the activated pattern produced from the phrase . 
 The meaning of a phrase , sentence , or text should be represented as pattern changing in time , though what we need is static and paradigmatic relation . 
 This paradox also arises in measuring the similarity between texts and the text coherence . 
 As we have seen in Section  , there is a difference between the similarity of texts and the similarity of word lists , and also between the coherence of a text and cohesiveness of a word list . 
 However , so far as the similarity between words is concerned , we assume that activated patterns on Paradigme will approximate the meaning of words , like a still picture can express a story . 
 We described measurement of semantic similarity between words . 
 The similarity between words is computed by spreading activation on the semantic network Paradigme which is systematically constructed from a subset of the English dictionary LDOCE . 
 Paradigme can directly compute the similarity between any two words in LDV , and indirectly the similarity of all the other words in LDOCE . 
 The similarity between words provides a new method for analysing the structure of text . 
 It can be applied to computing the similarity between texts , and measuring the cohesiveness of a text which suggests coherence of the text , as we have seen in Section  . 
 And , we are now applying it to text segmentation Grosz and Sidner 1986 , Youmans 1991 , i.e. to capture the shifts of coherent scenes in a story . 
 In future research , we intend to deal with syntagmatic relations between words . 
 Meaning of a text lies in the texture of paradigmatic and syntagmatic relations between words Hjelmslev 1943 . 
 Paradigme provides the former dimension -- an associative system of words -- as a screen onto which the meaning of a word is projected like a still picture . 
 The latter dimension -- syntactic process -- will be treated as a film projected dynamically onto Paradigme . 
 This enables us to measure the similarity between texts as a syntactic process , not as word lists . 
 We regard Paradigme as a field for the interaction between text and episodes in memory -- the interaction between what one is hearing or reading and what one knows Schank 1990 . 
 The meaning of words , sentences , or even texts can be projected in a uniform way on Paradigme , as we have seen in Section  and  . 
 Similarly , we can project text and episodes , and recall the most relevant episode for interpretation of the text . 
 The semantic network Paradigme is systematically constructed from the small and closed English dictionary Glossme . 
 Each entry of Glossme is mapped onto a node of Paradigme in the following way . 
 ( See also Figure  and  . ) 
 For each entry  in Glossme , map each unit  in  onto a subrfrant  of the corresponding node  in Paradigme . 
 Each word  is mapped onto a link or links in  , in the following way : 
 Let  be the reciprocal of the number of appearance of  ( as its root form ) in Glossme . 
 If  is in a head-part , let  be doubled . 
 Find nodes  corresponds to  ( ex. red  { red_1 , red_2 } ) . 
 Then , divide  into  in proportion to their frequency . 
 Add links  to  , where  is a link to the node  with thickness  . 
 Thus ,  becomes a set of links :  , where  is a link with thickness  . 
 Then , normalize thickness of the links as  , in each  . 
 Step 2 . 
 For each node  , compute thickness  of each subrfrant  in the following way : 
 Let  be the number of subrfrants of  . 
 Let  be  . 
 ( Note that  = 2:1 . ) 
 Normalize thickness  as  , in each  . 
 Step 3 . 
 Generate rfr of each node in Paradigme , in the following way : 
 For each node  in Paradigme , let its rfr  be an empty set . 
 For each  , for each subrfrant  of  , for each link  in  : 
 Let  be the node referred by  , and let  be thickness of  . 
 Add a new link l ' to rfr of  , where l ' is a link to  with thickness  . 
 Thus , each  becomes a set of links :  , where  is a link with thickness  . 
 Then , normalize thickness of the links as  , in each  . 
 Each node  of the semantic network Paradigme computes its activity value  at time  as follows : 
 where  and  are activity ( at time T ) collected from the nodes referred in the rfrant and rfr respectively ;  is activity given from outside ( at time T ) ; the output function  limits the value to [ 0,1 ] . 
  is activity of the most plausible subrfrant in  , defined as follows : 
 where  is thickness of the j-th subrfrant of  . 
  is the sum of weighted activity of the nodes referred in the j-th subrfrant of  , defined as follows : 
 where  is thickness of the k-th link of  , and  is activity ( at time T ) of the node referred by the k-th link of  . 
  is weighted activity of the nodes referred in the rfr  of  : 
 where  is thickness of the k-th link of  , and  is activity ( at time T ) of the node referred by the k-th link of  . 
