 We address the problem of automatically constructing a thesaurus by clustering words based on corpus data . 
 We view this problem as that of estimating a joint distribution over the Cartesian product of a partition of a set of nouns and a partition of a set of verbs , and propose a learning algorithm based on the Minimum Description Length ( MDL ) Principle for such estimation . 
 We empirically compared the performance of our method based on the MDL Principle against the Maximum Likelihood Estimator in word clustering , and found that the former outperforms the latter . 
 We also evaluated the method by conducting pp-attachment disambiguation experiments using an automatically constructed thesaurus . 
 Our experimental results indicate that such a thesaurus can be used to improve accuracy in disambiguation . 
 Recently various methods for automatically constructing a thesaurus ( hierarchically clustering words ) based on corpus data have been proposed Hindle 1990 , Brown et al. 1992 , Pereira et al. 1993, Tokunaga et al. 1995 . 
 The realization of such an automatic construction method would make it possible to 
 save the cost of constructing a thesaurus by hand , 
 do away with subjectivity inherent in a hand made thesaurus , and 
 make it easier to adapt a natural language processing system to a new domain . 
 In this paper , we propose a new method for automatic construction of thesauri . 
 Specifically , we view the problem of automatically clustering words as that of estimating a joint distribution over the Cartesian product of a partition of a set of nouns ( in general , any set of words ) and a partition of a set of verbs ( in general , any set of words ) , and propose an estimation algorithm using simulated annealing with an energy function based on the Minimum Description Length ( MDL ) Principle . 
 The MDL Principle is a well-motivated and theoretically sound principle for data compression and estimation in information theory and statistics . 
 As a strategy of statistical estimation MDL is guaranteed to be near optimal . 
 We empirically evaluated the effectiveness of our method . 
 In particular , we compared the performance of an MDL-based simulated annealing algorithm in hierarchical word clustering against that of one based on the Maximum Likelihood Estimator ( MLE , for short ) . 
 We found that the MDL-based method performs better than the MLE-based method . 
 We also evaluated our method by conducting pp-attachment disambiguation experiments using a thesaurus automatically constructed by it and found that disambiguation results can be improved . 
 Since some words never occur in a corpus , and thus cannot be reliably classified by a method solely based on corpus data , we propose to combine the use of an automatically constructed thesaurus and a hand made thesaurus in disambiguation . 
 We conducted some experiments in order to test the effectiveness of this strategy . 
 Our experimental results indicate that combining an automatically constructed thesaurus and a hand made thesaurus widens the coverage of our disambiguation method , while maintaining high accuracy . 
 A method of constructing a thesaurus based on corpus data usually consists of the following three steps : 
 Extract co-occurrence data ( e.g. case frame data , adjacency data ) from a corpus , 
 Starting from a single class ( or each word composing its own class ) , divide ( or merge ) word classes based on the co-occurrence data using some similarity ( distance ) measure . 
 ( The former approach is called ` divisive , ' the latter ` agglomerative ' ) 
 Repeat step  until some stopping condition is met , to construct a thesaurus ( tree ) . 
 The method we propose here consists of the same three steps . 
 Suppose available to us are data like those in Figure  , which are frequency data ( co-occurrence data ) between verbs and their objects extracted from a corpus ( step  ) . 
 We then view the problem of clustering words as that of estimating a probabilistic model ( representing probability distribution ) that generates such data . 
 We assume that the target model can be defined in the following way . 
 First , we define a noun partition  over a given set of nouns  and a verb partion  over a given set of verbs  . 
 A noun partition is any set  satisfying  ,  and  . 
 A verb partition  is defined analogously . 
 In this paper , we call a member of a noun partition a ` noun cluster , ' and a member of a verb partition a ` verb cluster ' . 
 We refer to a member of the Cartesian product of a noun partition and a verb partition (  ) simply as a ` cluster ' . 
 We then define a probabilistic model ( a joint distribution ) , written  , where random variable  assumes a value from a fixed noun partition  , and  a value from a fixed verb partition  . 
 Within a given cluster , we assume that each element is generated with equal probability , i.e. ,  
 Figure  shows two example models which might have given rise to the data in Figure  . 
 In this paper , we assume that the observed data are generated by a model belonging to the class of models just described , and select a model which best explains the data . 
 As a result of this , we obtain both noun clusters and verb clusters . 
 This problem setting is based on the intuitive assumption that similar words occur in the same context with roughly equal likelihood , as is made explicit in equation  . 
 Thus selecting a model which best explains the given data is equivalent to finding the most appropriate classification of words based on their co-occurrence . 
 We now turn to the question of what strategy ( or criterion ) we should employ for estimating the best model . 
 Our choice is the MDL ( Minimum Description Length ) principle Rissanen 1978 , Rissanen 1983 , Rissanen 1984 , Rissanen 1986 , Rissanen 1989 , a well-known principle of data compression and statistical estimation from information theory . 
 MDL stipulates that the best probability model for given data is that model which requires the least code length for encoding of the model itself , as well as the given data relative to it . 
 We refer to the code length for the model as the ` model description length ' and that for the data the ` data description length ' . 
 We apply MDL to the problem of estimating a model consisting of a pair of partitions as described above . 
 In this context , a model with less clusters , such as Model 2 in Figure  , tends to be simpler ( in terms of the number of parameters ) , but also tends to have a poorer fit to the data . 
 In contrast , a model with more clusters , such as Model 1 in Figure  , is more complex , but tends to have a better fit to the data . 
 Thus , there is a trade-off relationship between the simplicity of clustering ( a model ) and the goodness of fit to the data . 
 The model description length quantifies the simplicity ( complexity ) of a model , and the data description length quantifies the fit to the data . 
 According to MDL , the model which minimizes the sum total of the two types of description lengths should be selected . 
 In what follows , we will describe in detail how the description length is to be calculated in our current context , as well as our simulated annealing algorithm based on MDL . 
 We will now describe how the description length for a model is calculated . 
 Recall that each model is specified by the Cartesian product of a noun partition and a verb partition , and a number of parameters for them . 
 Here we let  denote the size of the noun partition , and  the size of the verb partition . 
 Then , there are  free parameters in a model . 
 Given a model M and data S , its total description length L ( M ) is computed as the sum of the model description length  , the description length of its parameters  , and the data description length  . 
 ( We often refer to  as the model description length ) . 
 Namely ,  
 We employ the ` binary noun clustering method , ' in which  is fixed at  and we are to decide whether  or  , which is then to be applied recursively to the clusters thus obtained . 
 This is as if we view the nouns as entities and the verbs as features and cluster the entities based on their features . 
 Since there are  subsets of the set of nouns  , and for each ` binary ' noun partition we have two different subsets ( a special case of which is when one subset is  and the other the empty set  ) , the number of possible binary noun partitions is  . 
 Thus for each binary noun partition we need  bits . 
 Hence  is calculated as  
  is calculated by  
 where | S | denotes the input data size , and  is the number of ( free ) parameters in the model . 
 It is known that using   bits to describe each of the parameters will ( approximately ) minimize the description length Rissanen 1984 . 
 Finally ,  is calculated by  
 where  denotes the observed frequency of the noun verb pair  , and  the estimated probability of  , which is calculated as follows 
 where  denotes the observed frequency of the noun verb pairs belonging to cluster  . 
 With the description length of a model defined in the above manner , we wish to select a model having the minimum description length and output it as the result of clustering . 
 Since the model description length  is the same for each model , in practice we only need to calculate and compare  . 
 The description lengths for the data in Figure  using the two models in Figure  are shown in Table  . 
 ( Table  shows some values needed for the calculation of the description length for Model 1 . ) These calculations indicate that according to MDL , Model 1 should be selected over Model 2 . 
 We could in principle calculate the description length for each model and select a model with the minimum description length , if computation time were of no concern . 
 However , since the number of probabilistic models under consideration is exponential , this is not feasible in practice . 
 We employ the ` simulated annealing technique ' to deal with this problem . 
 Figure  shows our ( divisive ) clustering algorithm . 
 Although there have been many methods of word clustering proposed to date , their objectives seem to vary . 
 In Table  we exhibit a simple comparison between our work and related work . 
 Perhaps the method proposed by Pereira et al. 1993 is the most relevant in our context . 
 In Pereira et al. 1993 , they proposed a method of ` soft clustering , ' namely , each word can belong to a number of distinct classes with certain probabilities . 
 Soft clustering has several desirable properties . 
 For example , word sense ambiguities in input data can be treated in a unified manner . 
 Here , we restrict our attention on ` hard clustering ' ( i.e. , each word must belong to exactly one class ) , in part because we are interested in comparing the thesauri constructed by our method with existing hand-made thesauri . 
 ( Note that a hand made thesaurus is based on hard clustering . )
 In this section , we elaborate on the merits of our method . 
 In statistical natural language processing , usually the number of parameters in a probabilistic model to be estimated is very large , and therefore such a model is difficult to estimate with a reasonable data size that is available in practice . 
 ( This problem is usually referred to as the ` data sparseness problem ' . )
 We could smooth the estimated probabilities using an existing smoothing technique Dagan et al. 1992 , Gale and Church 1990 , then calculate some similarity measure using the smoothed probabilities , and then cluster words according to it . 
 There is no guarantee , however , that the employed smoothing method is in any way consistent with the clustering method used subsequently . 
 Our method based on MDL resolves this issue in a unified fashion . 
 By employing models that embody the assumption that words belonging to a same cluster occur in the same context with equal likelihood , our method achieves the smoothing effect as a side effect of the clustering process , where the domains of smoothing coincide with the clusters obtained by clustering . 
 Thus , the coarseness or fineness of clustering also determines the degree of smoothing . 
 All of these effects fall out naturally as a corollary of the imperative of ` best possible estimation , ' the original motivation behind the MDL principle . 
 In our simulated annealing algorithm , we could alternatively employ the Maximum Likelihood Estimator ( MLE ) as criterion for the best probabilistic model , instead of MDL . 
 MLE , as its name suggests , selects a model which maximizes the likelihood of the data , that is ,  . 
 This is equivalent to minimizing the ` data description length ' as defined in Section 3 , i.e.  . 
 We can see easily that MDL generalizes MLE , in that it also takes into account the complexity of the model itself . 
 In the presence of models with varying complexity , MLE tends to overfit the data , and output a model that is too complex and tailored to fit the specifics of the input data . 
 If we employ MLE as criterion in our simulated annealing algorithm , it will result in selecting a very fine model with many small clusters , most of which will have probabilities estimated as zero . 
 Thus , in contrast to employing MDL , it will not have the effect of smoothing at all . 
 Purely as a method of estimation as well , the superiority of MDL over MLE is supported by convincing theoretical findings Barron and Cover 1991 , Yamanishi 1992 . 
 For instance , the speed of convergence of the models selected by MDL to the true model is known to be near optimal . 
 ( The models selected by MDL converge to the true model approximately at the rate of 1 / s where s is the number of parameters in the true model , whereas for MLE the rate is 1 / t , where t is the size of the domain , or in our context , the total number of elements of  . ) 
 ` Consistency ' is another desirable property of MDL , which is not shared by MLE . 
 That is , the number of parameters in the models selected by MDL converge to that of the true model Rissanen 1984 . 
 Both of these properties of MDL are empirically verified in our present context , as will be shown in the next section . 
 In particular , we have compared the performance of employing an MDL-based simulated annealing against that of one based on MLE in word clustering . 
 We describe our experimental results in this section . 
 We compared the performance of employing MDL as a criterion in our simulated annealing algorithm , against that of employing MLE by simulation experiments . 
 We artificially constructed a true model of word co-occurrence , and then generated data according to its distribution . 
 We then used the data to estimate a model ( clustering words ) , and measured the KL distance between the true model and the estimated model . 
 ( The algorithm used for MLE was the same as that shown in Figure  , except the ` data description length ' replaces the ( total ) description length ' in Step 2 . ) 
 Figure  plots the relation between the number of obtained noun clusters ( leaf nodes in the obtained thesaurus tree ) versus the input data size , averaged over 10 trials . 
 ( The number of noun clusters in the true model is 4 . ) 
 Figure   plots the KL distance versus the data size , also averaged over the same 10 trials . 
 The results indicate that MDL converges to the true model faster than MLE . 
 Also , MLE tends to select a model overfitting the data , while MDL tends to select a model which is simple and yet fits the data reasonably well . 
 We conducted the same simulation experiment for some other models and found the same tendencies . 
 ( Figure   and Figure   show the analogous results when the number of noun clusters in the true model is 2 ) . 
 We conclude that it is better to employ MDL than MLE in word clustering . 
 We extracted roughly 180,000 case frames from the bracketed WSJ ( Wall Street Journal ) corpus of the Penn Tree Bank Marcus et al. 1993 as co-occurrence data . 
 We then constructed a number of thesauri based on these data , using our method . 
 Figure  shows an example thesaurus for the 20 most frequently occurred nouns in the data , constructed based on their appearances as subject and object of roughly 2000 verbs . 
 The obtained thesaurus seems to agree with human intuition to some degree . 
 For example , ` million ' and ` billion ' are classified in one noun cluster , and ` stock ' and ` share ' are classified together . 
 Not all of the noun clusters , however , seem to be meaningful in the useful sense . 
 This is probably because the data size we had was not large enough . 
 This general tendency is also observed in another example thesaurus obtained by our method , shown in Figure  . 
 Pragmatically speaking , however , whether the obtained thesaurus agrees with our intuition in itself is only of secondary concern , since the main purpose is to use the constructed thesaurus to help improve on a disambiguation task . 
 We also evaluated our method by using a constructed thesaurus in a pp-attachment disambiguation experiment . 
 We used as training data the same 180,000 case frames in Experiment 1 . 
 We also extracted as our test data 172  patterns from the data in the same corpus , which is not used in the training data . 
 For the 150 words that appear in the position of  , we constructed a thesaurus based on the co-occurrences between heads and slot values of the frames in the training data . 
 This is because in our disambiguation test we only need a thesaurus consisting of these 150 words . 
 We then applied the learning method proposed in Li and Abe 1995 to learn case frame patterns with the constructed thesaurus as input using the same training data . 
 That is , we used it to learn the conditional distributions  ,  , where  and  vary over the internal nodes in a certain ` cut ' in the thesaurus tree . 
 Table  shows some example case frame patterns obtained by this method , and Figure  shows the leaf nodes dominated by the internal nodes appearing in the case frame patterns of Table  . 
 We then compare  and  , which are estimated based on the case frame patterns , to determine the attachment site of  . 
 More specifically , if the former is larger than the latter , we attach it to verb , and if the latter is larger than the former , we attach it to  , and otherwise ( including when both are 0 ) , we conclude that we cannot make a decision . 
 Table  shows the results of our pp-attachment disambiguation experiment in terms of ` coverage ' and ` accuracy ' . 
 Here ` coverage ' refers to the proportion ( in percentage ) of the test patterns on which the disambiguation method could make a decision . 
 ` Base Line ' refers to the method of always attaching  to  . 
 ` Word-Based , ' ` MLE-Thesaurus , ' and ` MDL-Thesaurus ' respectively stand for using word-based estimates , using a thesaurus constructed by employing MLE , and using a thesaurus constructed by our method . 
 Note that the coverage of ` MDL-Thesaurus ' significantly outperformed that of ` Word-Based , ' while basically maintaining high accuracy ( though it drops somewhat ) , indicating that using an automatically constructed thesaurus can improve disambiguation results in terms of coverage . 
 We also tested the method proposed in Li and Abe 1995 of learning case frames patterns using an existing thesaurus . 
 In particular , we used this method with WordNet Miller et al. 1993 and using the same training data , and then conducted pp-attachment disambiguation experiment using the obtained case frame patterns . 
 We show the result of this experiment as ` WordNet ' in Table  . 
 We can see that in terms of ` coverage , ' ` WordNet ' outperforms ` MDL-Thesaurus , ' but in terms of ` accuracy , ' ` MDL-Thesaurus ' outperforms ` WordNet ' . 
 These results can be interpreted as follows . 
 An automatically constructed thesaurus is more domain dependent and captures the domain dependent features better , and thus using it achieves high accuracy . 
 On the other hand , since training data we had available is insufficient , its coverage is smaller than that of a hand made thesaurus . 
 In practice , it makes sense to combine both types of thesauri . 
 More specifically , an automatically constructed thesaurus can be used within its coverage , and outside its coverage , a hand made thesaurus can be used . 
 Given the current state of the word clustering technique ( namely , it requires data size that is usually not available , and it tends to be computationally demanding ) , this strategy is practical . 
 We show the result of this combined method as ` MDL-Thesaurus + WordNet ' in Table  . 
 Our experimental result shows that employing the combined method does increase the coverage of disambiguation . 
 We also tested ` MDL-Thesaurus + WordNet + LA + Default , ' which stands for using the learned thesaurus and WordNet first , then the lexical association value proposed by Hindle and Rooth 1991 , and finally the default ( i.e. always attaching  to  ) . 
 Our best disambiguation result obtained using this last combined method somewhat improves the accuracy reported in Li and Abe 1995 (  ) . 
 We have proposed a method of clustering words based on large corpus data . 
 We conclude with the following remarks . 
 Our method of hierarchical clustering of words based on the MDL principle is theoretically sound . 
 Our experimental results show that it is better to employ MDL than MLE as estimation criterion in word clustering . 
 Using a thesaurus constructed by our method can improve pp-attachment disambiguation results . 
 At the current state of the art in statistical natural language processing , it is best to use a combination of an automatically constructed thesaurus and a hand made thesaurus for disambiguation purpose . 
 The disambiguation accuracy obtained this way was  . 
 In the future , hopefully with larger training data size , we plan to construct larger thesauri as well as to test other clustering algorithms . 
 We thank Mr. K. Nakamura , Mr. T. Fujita , and Dr. K. Kobayashi of NEC C & C Res. Labs.  for their constant encouragement . 
 We thank Dr. K. Yamanishi of C & C Res. Labs. for his comments . 
 We thank Ms. Y. Yamaguchi of NIS for her programming effort . 
