 We associate probabilities with feature structures , which are sets of maximal and non-maximal nodes generated by beginning from the starting node and successively expanding non-maximal leaves of the partial tree .
 Probabilities are assigned by the following inductive definition : If F is a feature structure , and F ' is a partial feature structure which differs from it only in that a single non-maximal node of type in F has been refined to type expanded to in F ' , then .
 Since items of type phrase are never introduced at that type , but only in the form of sub-types , there are no transitions from phrase in the corpus .
 In the PCFG the symmetry between the expansions of np and vp to singular and plural variants is implicit , whereas in the PTH the distribution of singular and plural variants is encoded at a single location , namely that at which num is refined .
 The independence assumption which is built into the training algorithm is that types are to be refined according to the same probability distribution irrespective of the context in which they are expanded .
 We have already seen a consequence of this : the PTH lumps together all occasions where num is expanded , irrespective of whether the enclosing context is np or vp .
 For the moment we are prepared to tolerate this because : Clarity : The decisions which we have made lead to a system with a clear probabilistic semantics .
