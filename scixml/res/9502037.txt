 The main claim it makes is that effective language processing requires a consideration of both the structural and statistical aspects of language , whereas traditional competence grammars rely only on the former , and standard statistical techniques such as n-gram models only on the latter .
 DOP attempts to combine these two traditions and produce `` performance grammars '' , which : `` ... should not only contain information on the structural possibilities of the general language system , but also on details of actual language use in a language community . '' This approach entails however that a corpus has first to be pre-analyzed ( ie. hand-parsed ) , and the question immediately arises as to the formalism to be used for this .
 In this class of grammars every rule has the form A a B ( A , B { non-terminals } , a { terminals } ) and all trees have the simple structure : Or : [TABLE] ( with an equivalent vertical alignment , henceforth to be used in this paper , on the right ) .
 In probabilistic terms , a finite-state grammar corresponds to a first-order Markov process , where given a sequence of states , , ... drawn from a finite set of possible states { , ... , } the probability of a particular state occurring depends solely on the identity of the previous state .
 If however , in line with most current theories , categories are taken to be bundles of features and crucially if one of these features has the value of a stack of categories , then this hierarchical structure can indeed be represented .
 Using the notation to represent a state of basic category A carrying a category B on its stack , the hierarchical structure of the sentence : can be represented as : Intuitively , syntactic links between non-adjacent words , impossible in a standard finite-state grammar , are here established by passing categories along on the stack `` through '' the state of intervening words .
 For example , in a bigram model trained on sufficient data the probability of the bigram ` dog barked ' could be expected to be significantly higher than ` cat barked ' , and this slice of `` world knowledge '' is something our model lacks .
