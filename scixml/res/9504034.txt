 In two of the tasks , the training data is generated by a probabilistic context-free grammar and in both tasks our algorithm outperforms the other techniques .
 The third task involves naturally-occurring data , and in this task our algorithm does not perform as well as n-gram models but vastly outperforms the Inside-Outside algorithm .
 In applications such as speech recognition , handwriting recognition , and spelling correction , performance is limited by the quality of the language model utilized Bahl et al. 1978 , Baker 1975 , Kernighan et al. 1990, Srihari and Baltus 1992 .
 In n-gram models and the Inside-Outside algorithm , this issue is evaded by bounding the size and form of the grammars considered , so that the `` optimal '' grammar cannot be expressed .
 We maintain this property throughout the search process , that is , for every symbol A ' that we add to the grammar , we also add a rule .
 This assures that the sentential symbol can expand to every symbol ; otherwise , adding a symbol will not affect the probabilities that the grammar assigns to strings .
 In the first domain , we created this grammar by hand ; the grammar was a small English-like probabilistic context-free grammar consisting of roughly 10 nonterminal symbols , 20 terminal symbols , and 30 rules .
