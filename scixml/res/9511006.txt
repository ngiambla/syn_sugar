 Consider , for example , the cluster containing attorney , counsel , trial , court , and judge , used by Brown et al. 1992 to illustrate a `` semantically sticky '' group of words .
 However , there are problems with the simple path-length definition of semantic similarity , and experiments using WordNet show that other measures of semantic similarity , such as the one employed here , provide a better match to human similarity judgments than simple path length does Resnik 1995a .
 Given two words and , their semantic similarity is calculated as where is the set of WordNet synsets that subsume ( i.e. , are ancestors of ) both and , in any sense of either word .
 For each pair considered , the most informative subsumer is identified , and this pair is only considered as supporting evidence for those senses that are descendants of that concept .
 Notice that by equation , support [ i , k ] is a sum of log probabilities , and therefore preferring senses with high support is equivalent to optimizing a product of probabilities .
 Third , unlike Sussna 's algorithm , the semantic similarity / distance computation here is not based on path length , but on information content , a choice that I have argued for elsewhere Resnik 1993, Resnik 1995a .
 Fourth , the combinatorics are handled differently : Sussna explores analyzing all sense combinations ( and living with the exponential complexity ) , as well as the alternative of sequentially `` freezing '' a single sense for each of and using those choices , assumed to be correct , as the basis for disambiguating .
