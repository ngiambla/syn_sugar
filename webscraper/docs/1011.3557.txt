0
1
0
2
 
v
o
N
6
1
 
 
 
]
I
A
.
s
c
[
 
 
1
v
7
5
5
3
.
1
1
0
1
:
v
i
X
r
a
A Probabilistic Approach for Learning Folksonomies from Structured Data
Anon Plangprasopchok,  Kristina Lerman,  and Lise Getoor 
(Dated: November 15, 2010)
Learning structured representations has emerged as an important problem in many domains,
including document and Web data mining, bioinformatics, and image analysis. One approach to
learning complex structures is to integrate many smaller, incomplete and noisy structure fragments.
In this work, we present an unsupervised probabilistic approach that extends a nity propagation [7]
to combine the small ontological fragments into a collection of integrated, consistent, and larger
folksonomies. This is a challenging task because the method must aggregate similar structures
while avoiding structural inconsistencies and handling noise. We validate the approach on a real-
world social media dataset, comprised of shallow personal hierarchies speci ed by many individual
users, collected from the photosharing website Flickr. Our empirical results show that our proposed
approach is able to construct deeper and denser structures, compared to an approach using only the
standard a nity propagation algorithm. Additionally, the approach yields better overall integration
quality than a state-of-the-art approach based on incremental relational clustering.
I.
INTRODUCTION
Learning structure from data has emerged as an im-
portant problem in many domains, for example, learning
gene networks from microarray data [5] and learning the
structure of probabilistic networks from knowledge bases
[10]. Here we focus on learning complex structures from
data that may already be explicitly structured, albeit
more simply.
Learning complex structures from collections of many
small, simple structures may provide insights into data
that individual small structures cannot provide. To infer
complex structures one needs machinery to manipulate
and combine structured data. For example, in order to
 nd communities of authors of scienti c papers, one must
 rst identify individual entities appearing among author
names in co-authorship network [1], then aggregate co-
authorship relations between the identi ed entities. To
learn complex structures of a speci c form, such as a tree
or a directed acyclic graph, the integration method must
have extra machinery to avoid structural inconsistencies,
that are likely to appear when data is combined arbitrar-
ily. The task becomes even more challenging when data
comes from numerous heterogeneous sources. Such data
is inherently noisy and inconsistent, and there is certainly
no single, uni ed structure to be found that explains all
the data.
One instance of such a task is learning a taxonomy
from many smaller trees generated by many people: the
so-called folksonomy learning task [16].
In folksonomy
learning, the input, structured metadata in the form
of hierarchies of conceptual terms created by individual
users, is combined into a global taxonomy that re ects
how a community organizes knowledge. Users who cre-
ate personal hierarchies to organize content may use id-
 Electronic address: anon.plangprasopchok@nectec.or.th
 Electronic address: lerman@isi.edu
 Electronic address: getoor@cs.umd.edu
iosyncratic categorization schemes [9] and naming con-
ventions. Simply combining nodes with similar names
is very likely to lead to ill-structured graphs containing
loops and shortcuts (multiple paths from one node to
another), rather than a tree. The folksonomy learning
problem has been addressed in recent work [17] using a
bottom-up approach to heuristically construct the folk-
sonomy.
In this paper, we present a more  exible probabilistic
framework for learning complex structures with a spe-
ci c form from fragments of structured data by exploit-
ing structural information. Our approach extends a n-
ity propagation [7] to use structural information to guide
the inference process to combine data concurrently into
more complex structures with a desired form. We exam-
ine two strategies for introducing structural information
into a nity propagation: through the similarity function
and through constraints.
II. LEARNING FOLKSONOMIES BY
INTEGRATING STRUCTURED METADATA
We take as our motivating example user-generated
structured metadata on the Social Web. We assume that
groups of users share common conceptualizations of the
world, which can be represented as a taxonomy or hier-
archy of concepts. Figure 1(a) depicts one such common
conceptualization about  animal  and its  bird  subcon-
cepts shared by a group of users. When users organize
the content they create, e.g., photographs on Flickr, they
select some portions of the common taxonomy for cat-
egorization. We observe these categories through the
shallow personal hierarchies Flickr users create. There
personal hierarchies, which we refer to as saplings, are
similar to how users organize their computer  les within
folders and subfolders. Figure 1(b) depicts some of the
saplings speci ed by di erent users to organize their  an-
imal  and  bird  images. Our ultimate goal is to infer the
common conceptual hierarchy related to  animal  from
the individual saplings. One natural solution is to aggre-
Animal
Animal
Animal
Animal
Animal
Animal
Animal
Animal
Animal
Animal
Animal
Animal
Animal
Animal
Animal
Animal
Animal
Animal
A. Personal Hierarchies in Flickr
2
Wildlife
Wildlife
Pet
Pet
Wildlife
Wildlife
Pet
Pet
Wildlife
Wildlife
Pet
Pet
Wildlife
Wildlife
Wildlife
Wildlife
Pet
Pet
Bird
Bird
Mammal
Mammal
Bird
Bird
Bird
Bird
Mammal
Mammal
Bird
Bird
Bird
Bird
Bird
Bird
Pet
Pet
Quail
Quail
Wren
Wren
Hawk
Hawk
Corvid Pigeon
Corvid Pigeon
Bird
Bird
Bird
Bird
Bird
Bird
Corvid
Corvid
Pigeon
Pigeon
Quail
Quail
Wren
Wren
Hawk
Hawk
(a)
(b)
FIG. 1: Illustrative examples on (a) a commonly shared con-
ceptual categorization (hierarchy) system; (b) personal hier-
archies expressed by the users based on the conceptual cate-
gorization in (a). For illustrative purposes, nodes with similar
names have similar color.
gate saplings shown in Figure 1(b) together into a deeper
and bushier tree shown in Figure 1(a).
To learn a common tree by aggregating saplings, we
need a strategy that measures the degree to which
two sapling nodes are similar, and therefore, should be
merged. Suppose that we have a very simple aggregation
strategy that says two nodes are similar if they have sim-
ilar names as in the prior work [16]. From Figure 1(b),
we will end up with a graph containing one loop and two
paths from  animal  to  bird , rather than the tree shown
in Figure 1(a). Suppose that we can also access tags with
which users annotated photos within saplings, and that
photos one of the  bird  nodes have tags like  pet  and
 domestic  in common, while photos belonging to the
other  bird  node have tags like  wildlife  and  forest  in
common. A cleverer similarity function that, in addition
to node names, takes tag statistics within a node into
consideration, should split  bird  nodes into two di erent
groups:
 pet bird  and  wild bird , which are put under
 pet  and  wildlife  nodes respectively.
The similarity function plays a crucial role in sapling
integration process, and a sophisticated enough similar-
ity function that can di erentiate node senses may poten-
tially correctly integrate the  nal tree. However,  nding
and tuning such function is very di cult. Moreover, data
is often inconsistent, noisy and incomplete, especially on
the Social Web, where data is generated by many di er-
ent users.
One possible way to tackle this challenge is to use
a simple similarity function and incorporate constraints
during the merging process.
Intuitively, we would not
consider merging the  bird  node under  pet  with the one
under  wildlife  because it will result in multiple paths
from  animal . Speci cally, we can impose constraints
that will prevent two nodes from being merged if (1) this
will lead to links from di erent parent concepts or (2)
this will lead to an incoming link to the root node of a
tree. These constraints guarantee that there is, at most,
a single path from one node to another.
1, ..li
Structured data in the form of shallow hierarchies is
ubiquitous on the Social Web. The social bookmark-
ing site Delicious allows users to bundle related tags to-
gether. On Flickr, users can group related photos into
sets and then group related sets in collections. Some
users create multi-level hierarchies containing collections
of collections, etc., but the vast majority of users who use
collections create shallow hierarchies, consisting of collec-
tions and their constituent sets. These personal hierar-
chies generally represent subclass and part-of relations.
We use the term sapling to refer to the tree represent-
ing a usually shallow personal hierarchy. A sapling is
composed of a root node ri and its child, or leaf, nodes
hli
ji. The root node corresponds to a user s collection,
and inherits its name, while the leaf nodes correspond to
the collection s constituent sets and inherit their names.
We assume that hierarchical relations between a root and
its children, ri   li
j, specify broader-narrower relations.
On Flickr, users can attach tags only to photos.
A sapling s leaf node corresponds to a set of pho-
tos, and the tag statistics of the leaf are aggregated
from that set s constituent photos.
Tag statistics
are then propagated from leaf nodes to the parent
node. We de ne a tag statistic of node x as  x :=
{(t1, ft1), (t2, ft2),       (tk, ftk )}, where tk and ftk are tag
and its frequency respectively. Hence, a root node s tags,
 ri, are aggregated from all the leaves  tags,  li
. These
tag statistics can also be used as a feature for determining
if two nodes are similar (of the same concept).
j
Any method that aggregates structured social meta-
data to learn folksonomies has to address a number of
challenges. Social metadata is usually very sparse, with
each individual user providing just a small amount of evi-
dence, in the form of tags or nodes, for folksonomy learn-
ing. Vocabulary noise, due to idiosyncratic naming con-
ventions, misspellings, and the like, is common, and so is
ambiguity and synonymy. Moreover, there is structural
noise, with users employing varying, and even con ict-
ing, categorization conventions. Varying levels of exper-
tise and expressiveness are also common, with some users
creating  ne-grained, expressive categorization schemes,
and other users coarse-grained, more general categoriza-
tion schemes [17].
B. SAP: Incremental Clustering Approach
In a recent work [17], we investigated a relational clus-
tering method that constructs folksonomies from many
personal hierarchies in an incremental manner. The folk-
sonomy construction starts with a seed term (which will
become a root of the learned folksonomy).
Individual
saplings whose roots have the same name as the seed are
clustered by using some similarity measure, along with a
prede ned threshold for merging or splitting nodes. At
this step, each merged sapling corresponds to a di erent
sense of the seed term. It also assumes that if root nodes
are to be merged, their leaves with similar names will also
be merged. One of the merged saplings, i.e., a particu-
lar sense of the root term, is then selected as the start-
ing point for growing the folksonomy for that concept.
Each leaf name is then used to retrieve other saplings
whose roots are similar to the name. Subsequently, these
saplings are clustered, and one whose root is most sim-
ilar to the leaf is then attached to the leaf. Structural
inconsistencies, such as loops and shortcuts have to be
removed if the attachment process creates them. This
procedure is done sequentially until the learned folkson-
omy reaches at a certain depth.
Since the folksonomy has been constructed incremen-
tally from top to bottom, decisions to merge or split
saplings at the top of the folksonomy has to be made
and  xed before its lower portions can be learned. Con-
sequently, only a small portion of the folksonomy is con-
sidered at each integration step, which can lead to a sub-
optimal structure.
In this paper, we propose a probabilistic framework for
folksonomy learning that overcomes the di culties of ex-
isting approaches. Speci cally, by considering each con-
cept term as a node, or a data point, within a complex
structure, we allow all similar nodes to merge simulta-
neously. Consequently, a complex structure will appear
as nodes are combined. However, since clustering nodes
arbitrarily may lead to a structure with some undesired
form, e.g., a graph with loops and shortcuts rather than
a tree, we propose a method which exploits structural
information to guide the clustering procedure to produce
a structure with a desired form.
III. PROBABILISTIC INTEGRATION OF
STRUCTURED DATA
A key idea of folksonomy learning through sapling inte-
gration is to merge similar nodes from di erent saplings.
Merging similar root nodes expands the width of the
learned tree, while merging the leaf of one sapling to the
root of another extends its depth. The merging process
has two key sub-components: (1) a similarity function
that evaluates how similar a pair of nodes is; (2) a pro-
cedure that decides if two nodes should or should not be
merged, based on their similarity.
Structural information plays an important role in the
merging process. Consider the case where two leaf nodes
from di erent saplings are about to be merged. If their
parent nodes belong to di erent clusters, then the merg-
ing process will result in a structure which has two paths
going to the merged node, i.e., not a tree. But, how
should structural information be used?
it
can be speci ed within the similarity function used eval-
uate the decision to merge nodes. For example, similarity
between two leaf nodes may contain information about
similarity of their parent (and/or sibling) nodes. There-
fore, leaf nodes whose parents are not very similar will
Intuitively,
3
be less likely to merge. Alternatively, structural informa-
tion can be speci ed explicitly through constraints. Such
constraints will prevent leaf nodes from being merged if
their parents belong to di erent clusters.
In this section we present a probabilistic framework
for distributed inference, and then investigate in detail
alternative ways to introduce structural information into
the inference process in order to learn deep, bushy trees
from many smaller, shallow trees.
A. A nity Propagation
As described in the previous section, we need an infer-
ence procedure to merge nodes, while exploiting struc-
tural information to guide the clustering to order the
integrated data in a speci c form, a tree in this context.
A nity Propagation (AP) [7] o ers a natural framework
to incorporate structural information.
AP is a powerful clustering algorithm that identi es a
set of exemplar points that well represent all the points
in the data set. The exemplars emerge as messages are
passed between data points, with each point assigned to
an exemplar. AP tries to  nd the exemplar set which
maximizes the net similarity, or the overall similarity be-
tween all exemplars and data points assigned to them.
Each exemplar and its data points is considered to be a
cluster.
We describe AP in terms of a factor graph [12] on bi-
nary variables. As recently introduced by Givoni and
Frey [8], the model is comprised of a square matrix of bi-
nary variables, along with a set of factor nodes imposed
on each row and column in the matrix. Following no-
tation of Ref. [8], let cij be a binary variable indicating
whether node i belongs to node j (or, j is an exemplar
of i). Let N be a number of data points; consequently,
the size of the matrix is N   N .
There are two types of constraints that enforce cluster
consistency. The  rst type, Ii, which is imposed on the
row i, indicates that a data point can belong to only
one exemplar (Pj cij = 1). The second type, Ej , which
is imposed on the column j, indicates that if a point
other than j chooses j as its exemplar, then j must be
its own exemplar (cjj = 1). AP avoids forming exemplars
and assigning cluster memberships, which violates these
constraints. Particularly, if the con guration at row i
violates I constraint, Ii will become   , which is not a
very optimal con guration (and similarly for Ej).
In addition to constraints, a similarity function S(.)
indicates how similar a certain node is to its exemplar. If
cij = 1, then S(cij ) is a similarity between nodes i and j;
otherwise, S(cij ) = 0. S(cjj) evaluates  self-similarity, 
also called  preference , which should be less than the
maximum similarity value in order to avoid all single-
ton points becoming exemplars, since that con guration
would yield the highest net similarity.
In general, the
higher the value of the preference for a particular point,
the more likely that point will become an exemplar. In
E1
E1
Ej
Ej
EN
EN
Ej
Ej
4
lar data points are grouped together into clusters by AP,
while relations between data points are grouped together
if their child nodes belong to the same cluster and their
parent nodes also belong to the same cluster. Neverthe-
less, combining saplings in this way could produce an
arbitrary graph rather than a tree form. This is because
AP does not have an explicit procedure to avoid creating
loops and shortcuts.
 
 
c1j
c1j
 
 
c11
c11
c1N
c1N
     
     
ci1
ci1
ciN
ciN
     
     
cN1
cN1
cNN
cNN
s1N
s1N
 
 
siN
siN
 
 
sij
sij
 
 
s1j
s1j
 
 
cij
cij
cNj
cNj
sNj
sNj
sNN
sNN
(a)
s11
s11
si1
si1
sN1
sN1
ij
ij
ij
ij
cij
cij
ij
ij
ij
ij
Ii
Ii
I1
I1
Ii
Ii
IN
IN
s(c ij)
s(c ij)
sij
sij
(b)
B. Expressing Structure through Similarity
FIG. 2: The original binary variable model for a nity propa-
gation proposed by Givoni and Frey[8]: (a) a matrix of binary
hidden variables (circles) and their factors(boxes); (b) incom-
ing and outgoing messages of a hidden variable node from/to
its associated factor nodes.
addition, we can set the same self-similarity value to all
data points, which indicates that all points are equally
likely to become exemplars.
A graphical model of a nity propagation is depicted
in Figure 2 in terms of a Factor Graph. In a log-domain,
the global objective function, which measures how good
the present con guration (a set of exemplars and cluster
assignments) is, can be written as a summation of all
local factors:
S(c11,       , cN N ) = Xi,j
+ Xj
Sij (cij) +Xi
Ii(ci1,       , ciN )
Ej (c1j,       , c1N ).
(1)
Optimizing this objective function identi es the con g-
uration that maximizes the net similarity S, while not
violating I and E constraints.
The original work uses max-sum algorithm to optimize
this global objective function, and it requires to update
and pass  ve messages as shown in Figure 2(b). Since
each hidden node cij is a binary variable (with two pos-
sible values), one can pass a scalar message   the dif-
ference between the messages when cij = 1 and cij = 0,
instead of carrying two messages at a time. The equa-
tions to update these messages are described in greater
detail in the Section 2 of Ref. [8].
Once the clustering process terminates, the MAP con-
 guration (exemplars and data points assigned to them)
can be recovered as follows. First, we identify an exem-
plar set by considering the sum of all incoming messages
of each cjj (each node in the diagonal of the variable
matrix). If the sum is greater than 0 (there is a higher
probability that node j is an exemplar), j is an exem-
plar. Once the set of exemplars K is recovered, each
non-exemplar point i is assigned to the exemplar k if the
sum of all incoming messages of cik is the highest com-
pared to the other exemplars.
One can directly apply AP to combine saplings into a
more complex structure.
In particular, each node in a
sapling is treated as a data point. Subsequently, simi-
Following our previous work [17], we de ne a similar-
ity measure between nodes in di erent saplings, which
exploits heterogeneous evidence available in the struc-
ture of the input data. Basically, the similarity function
is a combination of local similarity and structural simi-
larity. The local similarity between two nodes i and j,
localSim(i, j), is based on the intrinsic features of i and
j, such as their tag distributions. The structural similar-
ity, structSim(i, j) is based on features of neighboring
nodes. If i is a root of a sapling, its neighboring nodes
are all of its children. If i is a leaf node, the neighboring
nodes are its parent and siblings. The similarity between
nodes i and j is:
nodesim(i, j) = (1    )   localSim(i, j)
(2)
+     structSim(i, j),
where 0       1 is a weight for adjusting contributions
from localSim(, ) and structSim(, ). To reduce the com-
putational complexity, we assume that nodes with dif-
ferent stemmed names belong to di erent concepts, and
as a result, their similarity is 0. Thus, we only need to
evaluate the similarity between a pair of nodes with the
same stemmed names to decide whether the nodes refer
to the same or di erent concepts (meanings).
1. Local Similarity
To compute localSim(i, j), let tij be a number of com-
mon tags in the top K most frequent tags of nodes i and
j:
localSim(i, j) = min(1.,
tij
J
),
(3)
where J is a threshold on a number of common tags.
2. Structural Similarity
Structural similarity of two nodes depends on their po-
sitions within their saplings. We de ne three versions:
structSimRR(, ) which computes structural similarity
between two root nodes (root-to-root similarity), struct 
SimLL(, ), which evaluates structural similarity be-
tween a leaf of one sapling to that of another, and
structSimLR(, ) which evaluates structural similarity
between a root of one sapling and the leaf of another
(leaf-to-root similarity).
Root-to-Root similarity Two saplings A and B are
likely to describe the same concept if their root nodes rA
and rB have similar names and some of their leaf nodes
also have similar names. In this case, there is no need
to compute local similarity of these leaf nodes. Struc-
tural similarity between two root nodes is then de ned
as follows:
structSimRR(rA, rB)
1
 (name(lA
i ), name(lB
j )),
=
Z Xi,j
(4)
where  (., .) returns 1 if the both arguments are exactly
the same; otherwise, it returns 0; name(lA
i ) is a func-
tion that returns the name of a leaf node lA
i of sapling
A. Z is a normalizing constant, which is de ned as
Z = min(|lX |, |lY |), where |lX | is a number of children of
X. We use min(, ) instead of union. When merging with
a relatively small sapling with a larger one, the fraction
of common nodes may be very low compared to total
number of child nodes. Hence, the normalization coef-
 cient with the union (Z = union(lX, lY )), as de ned
in Jaccard similarity, results in overly penalizing small
saplings. min(, ), on the other hand, seems to correctly
consider the proportion of children of the smaller sapling
that overlap with the larger sapling.
Leaf-to-Leaf similarity Two leaf nodes, lA and lB are
likely to describe the same concept if they have a similar
name and some of their siblings also have similar names.
Structural similarity between two leaf nodes is de ned as
follows:
structSimLL(lA, lB)
(5)
=
1
Z   1
((Xi,j
 (name(lA
i ), name(lB
j )))   1).
This is similar to structSimRR(, ) but we have to sub-
tract one for excluding the present pair of leaf nodes.
Root-to-Leaf similarity Merging the root of one sam-
pling, rB, with the leaf, lA, of another extends the depth
of the learned folksonomy. Since we consider a pair of
nodes with di erent roles, their neighboring nodes also
have di erent roles. This would appear to make them
structurally incompatible. Nevertheless, we expect the
root of sapling A to share some common features with
the root rB. Consequently, we simply de ne the similar-
ity as,
structSimLR(rB, lA) = localSim(rB, rA).
(6)
3. Structural Similarity with Cluster Labels
Structural similarity described above does not take the
cluster (or concept) of the term into account. For a given
5
pair of terms, we can use cluster labels of their neighbor-
ing terms to help decide whether or not they should be-
long to the same cluster. Intuitively, the more neighbor-
ing terms share common cluster labels, the more similar
the node pair is. This is along the same line to the earlier
work on collective entity resolution [1], where the entity
identi cation decision is based on common neighboring
entities rather than references  features.
Let clust(i) be a function which returns the clus-
ter label of node i. For the root-to-root structural
similarity using cluster labels, we modify Eq. 4 sim-
ply by replacing name() with clust().
In other words,
structSimRR(rA, rB) is a normalized intersection be-
tween cluster labels of A s leaves and B s leaves.
For the leaf-to-leaf similarity on a pair of leaf nodes,
we can only consider the cluster label of their roots
rather than all of their siblings. This is because the clus-
ter labels of their root nodes have already taken clus-
ter labels of their siblings into account. Consequently,
structSimLL(lA, lB) with cluster labels is simply com-
puted from  (clust(rA, clust(rB)).
4. Negative Similarity
The structural similarity above does not provide an
explicit force to discourage clustering that may cause in-
coming links from di erent parent concepts.
In some
settings, e.g.,[1], the negative similarity can be applied to
discourage the merge that violates a constraint. In our
context, nevertheless, this similarity is inapplicable since
it is imposed on pairwise basis. Speci cally, suppose that
a root node is formed as a representative (exemplar) of
a certain cluster, leaf nodes having di erent parent clus-
ters can still be legally merged to the cluster. This is due
to the fact that AP only considers similarity of nodes to
their exemplars. As a result, such con guration does not
violate any constraints; therefore, incoming links from
di erent clusters are still permitted.
C. Expressing Structure through Constraints:
Relational A nity Propagation (RAP)
We extend AP to add structural constraints that will
ensure that the learned folksonomy makes sense   no
loops, and, to the extent possible, forms a hierarchy.
Since we want the learned structure to be a tree, all
nodes assigned to some exemplar must have their par-
ent nodes in the same cluster, i.e., assigned to the same
exemplar. To achieve this, we must enforce the following
two constraints: (1) merging should not create incoming
links to a cluster, or concept, from more than one parent
cluster (single parent constraint); (2) merging should not
create an incoming link to the root of the induced tree
(no root parent constraint). For the second constraint,
we can simply discard all sapling leaves that are named
similar to the tree root. Hence, we only need to enforce
A
A
B
B
C
C
D
D
E
E
A
A
B
B
C
C
D
D
E
E
F  Constraint
F  Constraint
(a)
pr(C) = A
pr(C) = A
pr(D) = B
pr(D) = B
Impose F constraint only 
Impose F constraint only 
at leaf-type nodes
at leaf-type nodes
= 0
= 0
= 1
= 1
Ej
Ej
ij
ij
ij
ij
Ii
Ii
cij
cij
ij
ij
s(c ij)
s(c ij)
sij
sij
ij
ij
ij
ij
ij
ij
Fj
Fj
(b)
FIG. 3: Relational A nity Propagation (RAP) proposed in
this paper. (a) Schematic diagram of the matrix of binary
hidden variables (circles). Variables within the green area
correspond to leaf nodes, while those within the pink area
correspond to root nodes of saplings. Filled-in circles
stand for exemplars. We omit E, I and S factors to
simplify the diagram. (b) Factor graph representation of
RAP.
the  rst constraint. The  rst constraint will be violated if
leaf nodes of two saplings are merged, i.e., assigned to the
same exemplar, while the root nodes of these saplings are
assigned to di erent exemplars. Consequently, the leaf
cluster will have multiple parents pointing to it, which
leads to an undesirable con guration.
Let pa(.) be a function that returns the index of the
parent node of its argument, and explr(.) be a function
that return the index of the argument s exemplar. The
factor F ,  single parent constraint , checks the violation
of multiple parent concepts pointing to a given concept.
The constraint is formally de ned as follows:
Fj(c1j ,       , cN j) =   
 
    i, k : cij = 1;ckj = 1;
explr(pa(i)) 6= explr(pa(k)),
0
otherwise.
(7)
Figure 3(a) illustrates the way we impose the new con-
straint on the binary variable matrix. The con guration
shown in the  gure is valid since both C and D belong to
the same exemplar E and their parents, A and B, belong
to the same exemplar A. However, if cBB = 1, then the
con guration is invalid, because parents of nodes in the
cluster of exemplar E will belong to di erent exemplars
(A and B). This constraint is imposed only on leaf nodes,
because merging root nodes will never lead to multiple
parent. The global objective function for RAP is basi-
cally Eq. 1 plus Pj Fj (c1j,       , cN j). The F-constraint
acts as a penalty factor that penalizes the merging that
leads to multiple parents. Integrating saplings in such a
way that maximizes this objective function will produce
a structure with clusters of similar nodes, while all nodes
in each cluster must have their parents coming from the
same cluster. As in AP, we use max-sum algorithm to
optimize this global objective function, which requires
passing two additional messages.
To extend AP, we modify the equations for updating
the messages  ,   and also derive 2 additional messages:
6
  and   to take into account this additional constraint.
Following the max-sum message update rule from a vari-
able node to a factor node (cf., eq. 2.4 in Chapter 8
of [2]), the message update formulas for  ,   and   are
simply:
 ij = S(i, j) +  ij +  ij ,
 ij = S(i, j) +  ij +  ij,
(8)
(9)
 ij = S(i, j) +  ij +  ij .
(10)
For deriving the message update equation for   , we
have to consider two cases: i = j and i 6= j, i.e., the
  message to the nodes on the diagonal and   for the
rest. For simplicity, we also assume that all leaf nodes
have their index numbers less than any roots. Hence, leaf
node indices run from 1 to L, where L is the number of
leaves.
For the case i = j (for the diagonal nodes cjj ), we
have to consider the update message for   in two pos-
sible settings: cjj = 1 and cjj = 0 ( jj (1) and  jj (0)
respectively), and then  nd the best con guration for
these settings. The max-sum message update rule from
a factor node to a variable node when cjj = 1 is [2]:
 jj (1) = max
S{j} (cid:0) Xk S{j};k6=j
 kj (1) + Xl / S{j};l6=j
 lj (0)(cid:1). (11)
For cjj = 0, it is
 jj (0) = Xk=1:L;k6=j
 kj (0),
(12)
where S{j} is a subset of leaf nodes (including j) that
share the same parent exemplar. Formally, S{j}   T;
T   {1,       , L}; {j}   S{j} and all k in S{j} share the
same parent exemplar. For Eq. 11, it favors the  valid 
con guration (the values of ckj ), which maximizes the
summation of all incoming messages to the factor node
Fj. The example of a valid con guration in this case is
as follows. Suppose we have only 3 leaf nodes: k and k 
and k  . We would say < ckj = 1, ck j = 1, ck  j = 0 >
is a valid con guration if k and k  have their parents
belonging to the same exemplar.
For Eq. 12, since no other nodes can belong to j, the
valid con guration simply sets all ckj to 0. Note that we
omit Fj from the above equations since invalid con gu-
rations are not very optimal, so that they will never be
chosen. Thus, Fj is always 0.
From Eq. 11 and Eq. 12, the scalar message  jj is sim-
ply:
 jj =  jj (1)    jj (0) = max(cid:26) maxS{j} Pk S{j};k6=j  kj
0
(13)
For i 6= j, we also have to consider the same subcases.
is to monitor the stability of the net similarity value,
7
For cij = 1, we have:
 ij (1) = max
Sx (cid:0) Xk Sx;k6=i
 kj (1) + Xl / Sx;l6=i
 lj (0)(cid:1).
(14)
For cij = 0, we have
 ij (0) = max(cid:0)Xk6=i
 kj (0), max
S (cid:0) Xk S;k6=i
 kj (1) (15)
+ Xl / S;l6=i
 lj (0)(cid:1)(cid:1),
where S   T; T   {1,       , L}, and all k in S share the
same parent exemplar without the restriction that S must
contain x. When j is a root node, the leaf node i will
never have the multiple-parent con ict with j, but we still
need to check whether other merging leaf nodes share the
same parent exemplar to i. Therefore, we set x = {j} for
this case. Speci cally, Sx in Eq. 14 is replaced by S{j}.
When j is a leaf node, however, we have to make sure
that node i, j and other merging leaf nodes have the
same parent exemplar. Thus, we set x = {i, j}. In other
words, we substitute S{i,j} for Sx in Eq. 14. In cij = 0
case, the best con guration may or may not have j as
the exemplar, which is di erent from the cij = 1 case
that requires the best con guration necessarily having j
as the exemplar.
The scalar message  ij , which is a di erence between
 ij (1) (Eq. 14) and  ij (0) (Eq. 15) is as follows:
 ij = min(cid:0) max
Sx Xk Sx;k6=i
 kj ,
(16)
Sx Xk Sx;k6=i
(cid:0) max
 kj   max
S Xl / S;l6=i
 lj(cid:1)(cid:1).
From the above equation, since the  rst argument of the
formula is always larger than, or equal to the second one,
its shorter form is simply:
max
Sx Xk Sx;k6=i
 kj   max
S Xl / S;l6=i
 lj .
(17)
There is one speci c case that the above equation does
not cover. The case appears when both i and j are
leaf nodes and do not share the same parent exemplars.
Therefore, the case cij = 1 should never happen, and
that makes  ij     . In other words, we will always
prefer cij = 0 to cij = 1. As a result, the scalar message
for this case is de ned as,
 ij (explr(pa(i)) 6= explr(pa(j))) =   .
(18)
For sake of simplifying implementation, we can use any
negative value instead of    to simply tell the inference
procedure that we always favor cij = 0 in this case.
The inference of exemplars and cluster assignments
starts by initializing all messages to zero and keeps up-
dating all messages for all nodes iteratively until conver-
gence. One possible way to determine the convergence
Pi,j Sij (cij ), as in the original AP.
Recovering MAP exemplars and cluster assignments
can be done in a slightly di erent way to the original
AP with one extra step, in order to guarantee that the
 nal graph is in a tree form. In particular, for a certain
exemplar, we sort its members by their similarity value
in descending order. The parent exemplar of a cluster
of nodes is determined as follows. If the exemplar of the
cluster is a leaf node, the parent exemplar of the cluster
is the parent exemplar of the exemplar. Otherwise, the
parent exemplar of the highest-ranked leaf node will be
chosen. We then split all member nodes that have dif-
ferent parent exemplars to that of the cluster. Note that
a more sophisticated approach to this task may be ap-
plied: e.g., once split,  nd the next best valid exemplar
to join. However, this more complex procedure is very
cumbersome   the decision to re-join a certain cluster
may recursively result in the invalidity of other clusters.
Note that RAP can be extended to induce other struc-
ture types such as DAG. In DAG case, we simply change
the condition in Eq. 7. In particular, for a certain exem-
plar, its leaf nodes can now have multiple parents, but
there will be no descendant nodes of its root nodes be-
longing to the same exemplar to some ancestor nodes of
its leaf nodes.
Computational Complexity
Both AP and RAP use similarity between pairs of
nodes to make cluster decisions. Standard similarity
function that only relies on node features can be pre-
computed at the  rst iteration, and reused through-
out the inference process. On the other hand, class
label-based similarity has to be evaluated at every
iteration.[22] Therefore, the computational complexity
of computing class label-based similarity grows linearly
with the number of iterations.
Let N be a number of all nodes (data points) in the
data set. Generally, it requires O(N 2) operations to com-
pute all pairwise similarities. Nevertheless, one can apply
the blocking idea, e.g., [14], to signi cantly reduce the
number of such pairwise computations. We use a sim-
ple blocking scheme, only comparing sapling nodes that
share the same stemmed name (we assume that terms
having di erent stemmed names will never get clustered
together). Let M be the number of unique stem terms.
Hence, for each stem term, there are N
M nodes to be com-
pared on average; as a result, the computational complex-
ity of pairwise similarity reduces to O(( N
M )2).
To determine the computational complexity of the
clustering procedure, in each iteration AP requires to
pass messages to O(( N
M )2) nodes. Therefore, the num-
ber of operations is proportional to the number of node
pairs to be compared. RAP, however, uses additional op-
erations to update   messages. Speci cally, it needs to
(1) update all cluster labels; (2) group nodes that share
the same parent. For each node group with the same
stem name, the  rst operation requires sorting nodes by
their message values, which can be done in O( N
M log( N
M ))
operations. The second step can be done in O( N
M ) oper-
ations with a proper data structure. Consequently, RAP
requires an additional O(N (1 + log N
M )) operations per
iteration compared to AP.
IV. EVALUATION ON REAL-WORLD DATA
We evaluate the di erent settings described in the pre-
vious section on real-world data collected from Flickr and
used in recent studies [16, 17]. This data set contains col-
lections and their constituent sets (or collections) created
by a subset of Flickr users who are members of seventeen
nature and wildlife photography groups. These users had
many other common interests, such as travel and sports,
arts and crafts, and people and portraiture. All the tags
associated with images in the set were also extracted.
We stemmed tags, set, and collection names. In all, the
data set contains 20, 759 saplings created by 7, 121 users.
A small fraction of these saplings are multi-level. We
manually selected 32 seed terms and used the following
heuristic to identify relevant saplings. First, we selected
saplings whose root names were similar to the seed term.
We then used the leaf node names of these saplings to
identify other saplings whose root names were similar to
these names, and so on, for two iterations.
To compare the di erent strategies for exploiting struc-
tural information, we apply the two clustering proce-
dures, AP and RAP, with di erent similarity functions,
to these data sets. We used the following similarity func-
tions: (1) local : only local similarity; and (2) hybrid :
local and structural similarity; and (3) class-hybrid :
lo-
cal and structural similarity using class labels. To make
this work comparable to [17], we used the following pa-
rameter values in the similarity functions: in local simi-
larity Eq. 3, we set the number of top tags K = 40, and
the number of common tags J = 4; in the hybrid sim-
ilarity function, the weight combination between local
and structural similarity is   = 0.9 when comparing two
nodes that are both roots or leaves, and   = 0.2 when
one node is a root and the other a leaf. Note that unlike
[17], there is no need to set the clustering threshold, since
exemplars emerge and compete against each other to at-
tract other similar nodes.
In all, we have six di erent
settings (two clustering procedures with three similarity
schemes).
We apply a strategy similar to [17] to remove inconsis-
tent nodes. Speci cally, a inconsistent leaf node is iden-
ti ed by the number of users who speci ed it, Nl, and
its parent, Nr. If Nl
< 0.01, the leaf node term is highly
Nr
idiosyncratic, and we classify it as inconsistency. More-
over, if there is only one leaf node and a few root nodes
in a certain cluster, we will split the leaf node out of the
cluster. This heuristic helps to remove concepts that are
less relevant to the seed term of the folksonomy.
8
Metric Similarity Avg
Metric Similarity Avg
Scheme Rank
Scheme Rank
LR
local
hybrid
class
local
mTO
hybrid
class
local
Con ict
hybrid
class
local
NetSim hybrid
class
(a) AP
1.71
1.39
1.87
1.55
2.32
1.81
2.84
1.39
1.68
1.48
2.45
2.03
LR
local
hybrid
class
local
mTO
hybrid
class
local
Con ict
hybrid
class
local
NetSim hybrid
class
(b) RAP
1.61
1.39
1.94
1.61
2.26
1.81
2.29
1.39
2.10
1.39
2.35
2.23
TABLE I: The table compares the performance of (a) AP and
(b) RAP, when using di erent similarity schemes on vari-
ous metrics. The numbers show the average ranks across
all 32 seeds. The lower rank, the better performance.
A. Evaluation Methodology
We measure the performance of the di erent learning
strategies by measuring the properties of the learned tree.
Speci cally, we evaluate both the quality and structure of
the learned tree (folksonomy). The quality of the learned
folksonomy is determined by comparing it to a refer-
ence taxonomy. Following methodology described in [17],
we use the taxonomy for classifying web pages from the
Open Directory Project (ODP)[23] as a reference hierar-
chy. Since the ODP hierarchy is relatively large, we only
consider the portion of it that overlaps the Flickr data
set. We apply two metrics: modi ed Taxonomic Over-
lap (mTO) [16], and Lexical Recall (LR). Lexical Recall
measures term overlap between the learned and reference
taxonomies, independent of their structure. mTO mea-
sures how well the learned hierarchy preserves parent-
child relations found in the reference taxonomy.
For structural evaluation, we apply two metrics: (1)
net similarity (NetSim); (2) the number of structural
con icts (Con icts). Net similarity measures how well
the approach can combine similar smaller structures. It
is computed by summing similarities of all nodes to their
exemplars. To make all settings comparable, we use Jac-
card similarity of the top tags to compute NetSim. The
number of con icts measures the structural integrity of
the learned tree.
It is given by the number of nodes
whose parents belong to di erent clusters. This number
is calculated at the end of the  nal iteration, just before
the last step that removes structural con icts that may
still appear. The smaller the value, the more consistent
the learned structure.
B. Results
We measure how using structural information, either
through structural similarity or through structural con-
straints, a ects the quality of the learned folksonomy.
To begin, we  rst evaluate the performance of di erent
similarity schemes (with or without structural informa-
tion) by running them with AP and RAP. Since all learn-
ing stratategies tend to produce more than one tree, we
average their performance across all induced trees. We
report performance of each learning strategy on a partic-
ular metric by ranking it against all other strategies and
averaging the rankings across all data sets. This gives a
measure of how often a strategy outperforms others. Av-
erage rankings are summarized as in Table I(a) for AP
and in Table I(b) for RAP.
From Table I, all similarity schemes perform in a simi-
lar manner in both AP and RAP. Speci cally, structural
information in the similarity function (hybrid and class-
hybrid ) does help reduce the number of structural con-
 icts in both AP and RAP. Nevertheless, these similarity
functions performed worse on mTO and NetSim. This is
because they are more stringent than local, and cluster
fewer saplings together in the folksonomy learning task
where individual saplings are rather sparse. Therefore,
 similar  structures are less collapsed as indicated by
lower NetSim. Not surprisingly, these similarity functions
do not improve mTO scores over local. This is because
mTO favors deeper trees to shorter ones if the nodes are
ordered correctly. Nevertheless, we hypothesize that in
domains where individual structures contain rich infor-
mation, hybrid similarity should outperform local simi-
larity.
For LR, structural information through hybrid simi-
larity can help recover more concepts. This is because
learning strategies with similarity function can exploit
structural information when local information is not suf-
 cient. However, class-hybrid performs worse than hybrid
in LR and in the other metrics. We speculate that class
labels at the beginning of the learning process may not be
reliable enough, and that leads to the worse performance.
The results of keeping the similarity function  xed, and
studying the e ectiveness of the clustering strategy are
shown in Table II. RAP generally outperforms AP on
almost all measures. Speci cally, it recovers more con-
cepts (better LR score), learns structures better aligned
with the reference hierarchy (better mTO ), produces sig-
ni cantly more consistent structures (fewer Con icts).
However, RAP produces trees with lower net similar-
ity (NetSim), since it contains more stringent criteria to
merge saplings than AP. Note that
Next, we compare RAP with local similarity, found
to be superior to alternative clustering schemes, to the
previous folksonomy learning approach SAP [17]. Unlike
SAP, the methods proposed in this paper generally return
more than one tree. We simply evaluate the most popular
tree, which has the largest number of merged nodes at
the root level. Figure 4 displays an example of the most
9
Clustering Scheme
Measure AP
1.35
LR
mTO 1.42
Con ict 1.97
RAP
1.35
1.29
1.00
NetSim 1.39
1.55
(a) Local Similarity
Clustering Scheme
Measure AP
1.29
LR
mTO 1.48
Con ict 1.97
NetSim 1.48
RAP
1.29
1.26
1.00
1.45
(b) Hydrid Similarity
Clustering Scheme
Measure AP
1.29
LR
mTO 1.48
Con ict 1.97
RAP
1.19
1.29
1.00
NetSim 1.32
(c) Class-Hydrid Similarity
1.58
TABLE II: The table compares the performance between AP
and RAP when using (a) local, (b) hybrid and (c) class-
hybrid similarity on various metrics. The numbers show
the average ranks across all 32 seeds. The lower rank,
the better performance.
popular tree of bird, which is induced by RAP with local
similarity.
Due to space limitations, we only report the quality of
the learned folksonomy, as measured by mTO scores and
the number of overlapping paths (#OPaths) to the refer-
ence hiearchy. For #OPaths, we consider two paths are
 overlapping  if their root (source) nodes share the same
name; and their leaf (sink) nodes share the same name.
Therefore, the number of overlapping paths are enumer-
ated by counting how many leaves in the learned folkson-
omy share similar names to some leaves in the reference
hierarchy. Since mTO is computed from the overlapping
paths, the approach that yields higher mTO and higher
#OPaths at the same time is preferable. Note that we
cannot compare RAP with SAP on Con icts and NetSim
metrics because of their algorithmic di erence. In partic-
ular, RAP provides an approximate solution, which often
contains some con icts in a  di cult  case. In SAP, how-
ever, con icts are heuristically removed as a tree grows.
Moreover, it s impossible to compute NetSim on a SAP
tree since SAP does not identify any exemplars.
As shown in Table III, RAP with local similarity can
produce more consistent taxonomies compared to SAP
(15 vs. 12 cases). Moreover, if considering both numbers
of comparable paths (#OPaths) and mTO, RAP+local is
clearly superior to SAP (14 vs. 4 cases). Speci cally, the
former produces more consistent structures on a higher
number of comparable paths, with respect to the refer-
ence hierarchy.
Nevertheless, AUT (a metric for measuring how de-
tailed a folksonomy from its bushiness and depth [17])
10
SAP
RAP + local
#OPaths mTO
#OPaths mTO
27
92
85
27
22
0
27
0
2
0
4
0
1
0.895
0.659
0.788
0.665
0.755
0.000
0.587
0.000
0.754
0.000
0.665
0.000
1.000
301
0.670
31
0
18
1
5
1
118
7
3
3
15
27
82
55
0
0
0.490
0.000
0.481
1.000
0.924
1.000
0.576
0.735
0.622
0.600
0.832
0.647
0.724
0.749
0.000
0.000
475
0.461
37
106
43
46
38
0
47
1
6
0
1
14
4
133
14
7
28
9
18
26
0.869
0.656
0.785
0.672
0.714
0.000
0.689
0.508
0.863
0.000
0.000
0.400
1.000
0.596
0.529
0.672
0.512
0.783
0.836
0.752
182
0.683
11
0.795
4
4
28
114
135
133
4
3
44
0.625
0.600
0.637
0.649
0.620
0.823
0.603
1.000
0.432
seeds
africa
anim
asia
australia
bird
build
canada
cat
c. america
citi
countri
craft
dog
europ
fauna
 sh
 ora
 ower
insect
invertebr
n. america
plant
reptil
s. africa
s. america
sport
u. kingdom
u. state
urban
vertebr
world
Summary
1429(sum) 0.557(avg) 1240(sum) 0.629(avg)
TABLE III: The table compares the performance on mTO of
the proposed approach,RAP with local similarity scheme,
to the previous work, SAP [17]. The table also reports
a number of comparable paths,#OPaths to the reference
hierarchies.
erarchies which are explicitly speci ed by users. Conse-
quently, our work attempts to  nd the best  alignment 
or  integration,  which maximizes the similarity between
concepts and has no structural inconsistencies.
In the social web domain, most of the previous
work utilizes tag statistics as evidence for learning
broader/narrower relations between concepts [15, 19].
Since these works are based on tag statistics, they are
likely to su er from the  popularity vs generality  prob-
lem, where a tag may be used more frequently not be-
cause it is more general, but because it is more popular
among users. Moreover, the approaches only focus on
learning pair-wise relations rather than constructing full
hierarchies. These are all di erent from the present work,
which focuses on exploiting existing relations and com-
bining them together into full hierarchies.
Folksonomy integration is similar to ontology align-
ment [6, 20] in that both identify matches between con-
FIG. 4: A folksonomy learned for bird using RAP with lo-
cal similarity. Due to space limiations, concepts with a
similar name that do not merge together are visualized
in a single node with a number in parentheses that enu-
merates their number.
and LR scores (not presented in the table) of trees pro-
duced by RAP are inferior to SAP (6 vs. 24 cases, and
12 vs. 19 cases respectively). This is because of the
nature of AP and its extension, RAP, that allows di er-
ent trees to emerge simultaneously. In many cases, these
trees attract the most similar structures to it. Compared
to SAP, which greedily grows one tree at a time, attract-
ing all similar concepts to it, RAP assigns concepts to
di erent trees with which they have the best  t. Since
we only consider one of the trees in the evaluation, there
is a high chance that the selected tree contains relatively
fewer unique concepts and so is not bushier than SAP s
tree.
The overall experimental results clearly suggest that
the proposed approach (RAP), which incorporates struc-
tural information through constraints during probabilis-
tic inference process can learn better, more consistent
structures. We speculate that RAP can be even more
advantageous in domains where heuristics for correcting
the learned structure to a speci c form are di cult to
specify and expensive to carry out.
V. RELATED WORK
Learning from structured data has emerged as a pop-
ular research area in machine learning. Closest to ours
is work on learning systems of concepts [11] and learning
hierarchical topics from words in documents [3]. Never-
theless, our work is fundamentally di erent from them
in that, we  align  or integrate many small shallow hi-
cepts in pairs of structures. Nevertheless, ontology align-
ment di ers from our problem, since in ontology align-
ment there are typically just a few structures to align,
and those structures are deep and semantically rich.
Here, we focus on the much noisier setting, where there
are many smaller fragments created by end users with
a variety of purposes in mind. A recent work [17] ad-
dressed this problem by applying the relational cluster-
ing approach to exploit structure and tag statistics to
incrementally attach relevant saplings to the learned folk-
sonomies. In that work, since the folksonomy has been
constructed incrementally from top to bottom, only a
small portion of the folksonomy is considered at each
integration step, which may lead to a sub-optimal struc-
ture. This is di erent from the method described in this
paper that integrates all fragments simultaneously into a
uni ed tree.
A nity propagation, on which the present work is
based, has been applied to many clustering problems,
e.g.
segmentation in computer visions [13], because it
provides a natural way to incorporate constraints while
simultaneously improving the net similarity of the cluster
assignments, which is not trivial to handle with standard
clustering techniques. In addition, no strong assumption
is required on the threshold, which determines whether
clusters should be merged or not. Moreover, the cluster
assignments can be changed during the inference process
as suggested by the emergence of exemplars, compared to
 incremental  clustering approaches (e.g., [1]), in which
previous clustering decisions cannot be changed. To the
best of our knowledge, ours is the  rst extension of AP al-
gorithm that can learn tree structures from many sparse
and shallow trees.
Several other statistical relational learning (SRL) ap-
proaches may be applicable to this class of problems. For
example, Markov Logic Networks (MLN) [18] and Prob-
abilistic Similarity Logic (PSL) [4], are generic frame-
works for solving probabilistic inference problems. They
may be used for folksonomy learning by translating sim-
ilarity function as well as constraints into logical predi-
cates. Since our similarity function is continuous, hybrid
MLN (HMLN) [21] would be required. Nevertheless, AP
11
framework is more preferable for the present problem due
to its simplicity. For some problems which require to
model multiple types of relations and constraints, MLN
and PSL may be more suitable.
VI. DISCUSSION AND CONCLUSION
We described a probabilistic approach, RAP, that ex-
tends the distributed inference approach used by a nity
propagation to combine a large number small structures
into a few, integrated complex structures. We studied
two di erent ways to incorporate structural information
into the inference process, and applied the approach to
the folksonomy learning problem. The experimental re-
sults suggest that, in folksonomy learning setting, the ap-
proach that incorporates structural information through
constraints, RAP, can help produce high quality folk-
sonomies, often better than those learned by the cur-
rent state-of-the-art approach. In addition, the proposed
approach is general enough for other domains, in which
partial structures are speci ed, such as tags bundles in
Delicious,  les and folders in personal workspaces and
semantic networks.
Regarding future work, we would like to extend the ap-
proach to induce other classes of structures, e.g., DAGs.
We would also like to extend RAP to apply to other struc-
ture learning problems, such as alignment of biological
data. Finally, we would like incorporate more e cient
inference algorithm and compare the aproach to other
statistical relational learning (SRL) approaches.
Acknowledgements
This material is based upon work supported by the Na-
tional Science Foundation under Grant No. IIS-0812677.
[1] I. Bhattacharya and L. Getoor. Collective entity reso-
lution in relational data. ACM Trans. Knowl. Discov.
Data, 1(1):5, 2007.
ture learning method for constructing gene networks.
Bioinformatics, 22:1367 1374, 2006.
[6] J. Euzenat and P. Shvaiko. Ontology Matching. Springer-
[2] C. M. Bishop. Pattern Recognition and Machine Learn-
Verlag, 2007.
ing. Springer-Verlag, 2006.
[3] D. M. Blei, T. L. Gri ths, M. I. Jordan, and J. B. Tenen-
baum. Hierarchical topic models and the nested chinese
restaurant process. In Proceedings of the Neural Infor-
mation Processing Systems, 2003.
[4] M. Broecheler, L. Mihalkova, and L. Getoor. Probabilis-
tic similarity logic. In Proceedings of the Conference on
Uncertainty in Arti cial Intelligence, 2010.
[7] B. J. Frey and D. Dueck. Clustering by passing messages
between data points. Science, 312:972 976, 2007.
[8] I. E. Givoni and B. J. Frey. A binary variable model for
a nity propagation. Neural Comput, 21(6):1589 1600,
2009.
[9] S. A. Golder and B. A. Huberman. Usage patterns of
collaborative tagging systems. J. Inf. Sci., 32:198 208,
2006.
[5] X. Chen, G. Anantha, and X. Wang. An e ective struc-
[10] D. Heckerman, D. Geiger, and D. Chickering. Learning
bayesian networks: The combination of knowledge & sta-
tistical data. Mach. Learn., 20:197 243, 1995.
[11] C. Kemp, J. Tenenbaum, T. L. Gri ths, T. Yamada,
and N. Ueda. Learning systems of concepts with an in-
 nite relational model. In Proceedings of the American
Association for Arti cial Intelligence, 2006.
[12] F. Kschischang, B. J. Frey, and H.-A. Loeliger. Factor
graphs and the sum-product algorithm. IEEE Transac-
tions on Information Theory, 47:498 519, 2001.
[13] N. Lazic, I. Givoni, B. Frey, and P. Aarabi. Floss: Facility
location for subspace segmentation. In Proceedings of the
International Conference on Computer Vision, 2009.
[14] A. McCallum, K. Nigam, and L. H. Ungar. E cient
clustering of high-dimensional data sets with application
to reference matching.
In Proceedings of International
Conference on Knowledge Discovery and Data Mining,
2000.
[15] P. Mika. Ontologies are us: A uni ed model of social
networks and semantics. J. Web Sem., 5(1):5 15, 2007.
[16] A. Plangprasopchok and K. Lerman. Constructing folk-
sonomies from user-speci ed relations on  ickr. In Pro-
ceedings of the World Wide Web conference, 2009.
12
[17] A. Plangprasopchok, K. Lerman, and L. Getoor. Grow-
ing a tree in the forest: Constructing folksonomies by
integrating structured metadata. In Proceedings of ACM
SIGKDD Conference on Knowledge Discovery and Data
Mining, 2010.
[18] M. Richardson and P. Domingos. Markov logic networks.
Mach. Learn., 62:107 136, 2006.
[19] P. Schmitz. Inducing ontology from  ickr tags. In Pro-
ceedings of the WWW workshop on Collaborative Web
Tagging Workshop, 2006.
[20] O. Udrea, L. Getoor, and R. J. Miller. Leveraging data
and structure in ontology integration. In SIGMOD Con-
ference, 2007.
[21] J. Wang and P. Domingos. Hybrid markov logic net-
works. In Proceedings of Association for the Advancement
of Arti cial Intelligence, 2008.
[22] For more accurate similarity, one can re-evaluate similar-
ity once one of relevant nodes reassigned to a di erent
cluster, but this would require much more computation.
[23] http://rdf.dmoz.org/, as of September 2008
Animal
Animal
Wildlife
Wildlife
Pet
Pet
Bird[0]
Bird[0]
  Bird[16]  
  Bird[16]  
Bird[55]
Bird[55]
Bird[5]
Bird[5]
Bird[30]
Bird[30]
Quail
Quail
Wren
Wren
 
 
Heron
Heron
Hawk
Hawk
Humming
Humming
Corvid Pigeon
Corvid Pigeon
Animal
Animal
Wildlife
Wildlife
Pet[0]
Pet[0]
Pet[1]   Pet[22]
Pet[1]   Pet[22]
Bird[1]
Bird[1]
Bird[3]   Bird[50]
Bird[3]   Bird[50]
Hawk
Hawk
Heron
Heron
Quail
Quail
Corvid
Corvid
Pigeon
Pigeon
 
 
Wren
Wren
