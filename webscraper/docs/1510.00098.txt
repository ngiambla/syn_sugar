Transfer Learning from Deep Features for Remote Sensing and Poverty Mapping
Michael Xie and Neal Jean and Marshall Burke and David Lobell and Stefano Ermon
Department of Computer Science, Stanford University
{xie, nealjean, ermon}@cs.stanford.edu
Department of Earth System Science, Stanford University
{mburke,dlobell}@stanford.edu
6
1
0
2
 
b
e
F
7
2
 
 
 
]
V
C
.
s
c
[
 
 
2
v
8
9
0
0
0
.
0
1
5
1
:
v
i
X
r
a
Abstract
The lack of reliable data in developing countries is a
major obstacle to sustainable development, food secu-
rity, and disaster relief. Poverty data, for example, is
typically scarce, sparse in coverage, and labor-intensive
to obtain. Remote sensing data such as high-resolution
satellite imagery, on the other hand, is becoming in-
creasingly available and inexpensive. Unfortunately,
such data is highly unstructured and currently no tech-
niques exist to automatically extract useful insights to
inform policy decisions and help direct humanitarian ef-
forts. We propose a novel machine learning approach to
extract large-scale socioeconomic indicators from high-
resolution satellite imagery. The main challenge is that
training data is very scarce, making it dif cult to apply
modern techniques such as Convolutional Neural Net-
works (CNN). We therefore propose a transfer learn-
ing approach where nighttime light intensities are used
as a data-rich proxy. We train a fully convolutional
CNN model to predict nighttime lights from daytime
imagery, simultaneously learning features that are use-
ful for poverty prediction. The model learns  lters iden-
tifying different terrains and man-made structures, in-
cluding roads, buildings, and farmlands, without any su-
pervision beyond nighttime lights. We demonstrate that
these learned features are highly informative for poverty
mapping, even approaching the predictive performance
of survey data collected in the  eld.
Introduction
New technologies fueling the Big Data revolution are cre-
ating unprecedented opportunities for designing, monitor-
ing, and evaluating policy decisions and for directing hu-
manitarian efforts
(Abelson, Varshney, and Sun 2014;
Varshney et al. 2015). However, while rich countries are
being  ooded with data, developing countries are suffer-
ing from data drought. A new data divide is emerging, with
huge differences in the quantity and quality of data avail-
able. For example, some countries have not taken a census in
decades, and in the past  ve years an estimated 230 million
births have gone unrecorded (Independent Expert Advisory
Group Secretariat 2014). Even high-pro le initiatives such
as the Millennium Development Goals (MDGs) are affected
Copyright c(cid:13) 2016, Association for the Advancement of Arti cial
Intelligence (www.aaai.org). All rights reserved.
(United Nations 2015). Progress based on poverty and infant
mortality rate targets can be dif cult to track. Often, poverty
measures must be inferred from small-scale and expensive
household surveys, effectively rendering many of the poor-
est people invisible.
Remote sensing, particularly satellite imagery, is perhaps
the only cost-effective technology able to provide data at a
global scale. Within ten years, commercial services are ex-
pected to provide sub-meter resolution images everywhere
at a fraction of current costs (Murthy et al. 2014). This level
of temporal and spatial resolution could provide a wealth
of data towards sustainable development. Unfortunately, this
raw data is also highly unstructured, making it dif cult to
extract actionable insights at scale.
In this paper, we propose a machine learning approach
for extracting socioeconomic indicators from raw satellite
imagery. In the past  ve years, deep learning approaches
applied to large-scale datasets such as ImageNet have rev-
olutionized the  eld of computer vision, leading to dramatic
improvements in fundamental tasks such as object recogni-
tion (Russakovsky et al. 2014). However, the use of con-
temporary techniques for the analysis of remote sensing im-
agery is still largely unexplored. Modern approaches such
as Convolutional Neural Networks (CNN) can, in principle,
be directly applied to extract socioeconomic factors, but the
primary challenge is a lack of training data. While such data
is readily available in the United States and other developed
nations, it is extremely scarce in Africa where these tech-
niques would be most useful.
We overcome this lack of training data by using a se-
quence of transfer learning steps and a convolutional neu-
ral network model. The idea is to leverage available datasets
such as ImageNet to extract features and high-level repre-
sentations that are useful for the task of interest, i.e., ex-
tracting socioeconomic data for poverty mapping. Similar
strategies have proven quite successful in the past. For ex-
ample, image features from the Overfeat network trained
on ImageNet for object classi cation achieved state-of-the-
art results on tasks such as  ne-grained recognition, image
retrieval, and attribute detection (Razavian et al. 2014).
Pre-training on ImageNet is useful for learning low-level
features such as edges. However, ImageNet consists only
of object-centric images, while satellite imagery is captured
from an aerial, bird s-eye view. We therefore employ a sec-
ond transfer learning step, where nighttime light intensities
are used as a proxy for economic activity. Speci cally, we
start with a CNN model pre-trained for object classi ca-
tion on ImageNet and learn a modi ed network that pre-
dicts nighttime light intensities from daytime imagery. To
address the trade-off between  xed image size and informa-
tion loss from image scaling, we use a fully convolutional
model that takes advantage of the full satellite image. We
show that transfer learning succeeds in learning features rel-
evant not only for nighttime light prediction but also for
poverty mapping. For instance, the model learns  lters iden-
tifying man-made structures such as roads, urban areas, and
 elds without any supervision beyond nighttime lights, i.e.,
without any labeled examples of roads or urban areas (Fig-
ure 2). We demonstrate that these features are highly infor-
mative for poverty mapping and capable of approaching the
predictive performance of survey data collected in the  eld.
Problem Setup
We begin by reviewing transfer learning and convolutional
neural networks, the building blocks of our approach.
Transfer Learning
We formalize transfer learning as in (Pan and Yang 2010): A
domain D = {X , P (X )} consists of a feature space X and
a marginal probability distribution P (X ). Given a domain,
a task T = {Y, f ( )} consists of a label space Y and a pre-
dictive function f ( ) which models P (y|x) for y   Y and
x   X . Given a source domain DS and learning task TS, and
a target domain DT and learning task TT , transfer learning
aims to improve the learning of the target predictive function
fT ( ) in TT using the knowledge from DS and TS, where
DS (cid:54)= DT , TS (cid:54)= TT , or both. Transfer learning is particu-
larly relevant when, given labeled source domain data DS
and target domain data DT , we  nd that |DT| (cid:28) |DS|.
In our setting, we are interested in more than two re-
lated learning tasks. We generalize the formalism by repre-
senting the multiple source-target relationships as a transfer
learning graph. First, we de ne a transfer learning prob-
lem P = (D,T ) as a domain-task pair. The transfer learn-
ing graph is then de ned as follows: A transfer learning
graph G = (V,E) is a directed acyclic graph where ver-
tices V = {P1,    ,Pv} are transfer learning problems and
E = {(Pi1,Pj1 ),    , (Pie,Pje)} is an edge set. For each
transfer learning problem Pi = (Di,Ti)   V, the aim is to
improve the learning of the target predictive function fi( )
in Ti using the knowledge in  (j,i) EPj.
Convolutional Neural Networks
Deep learning approaches are based on automatically learn-
ing nested, hierarchical representations of data. Deep feed-
forward neural networks are the typical example of deep
learning models. Convolutional Neural Networks (CNN) in-
clude convolutional operations over the input and are de-
signed speci cally for vision tasks. Convolutional  lters are
useful for encoding translation invariance, a key concept for
discovering useful features in images (Bouvrie 2006).
A CNN is a general function approximator de ned by a
set of convolutional and fully connected layers ordered such
that the output of one layer is the input of the next. For image
data, the  rst layers of the network typically learn low-level
features such as edges and corners, and further layers learn
high-level features such as textures and objects (Zeiler and
Fergus 2013). Taken as a whole, a CNN is a mapping from
tensors to feature vectors, which become the input for a  nal
classi er. A typical convolutional layer maps a tensor x  
Rh w d to gi   R h   w   d such that
gi = pi(fi(Wi   x + bi)),
where for the i-th convolutional layer, Wi   Rl l   d is a
tensor of  d convolutional  lter weights of size l   l, ( ) is
the 2-dimensional convolution operator over the last two di-
mensions of the inputs, bi is a bias term, fi is an element-
wise nonlinearity function (e.g., a recti ed linear unit or
ReLU), and pi is a pooling function. The output dimensions
 h and  w depend on the stride and zero-padding parameters
of the layer, which control how the convolutional  lters slide
across the input. For the  rst convolutional layer, the input
dimensions h, w, and d can be interpreted as height, width,
and number of color channels of an input image, respec-
tively.
In addition to convolutional layers, most CNN models
have fully connected layers in the  nal layers of the net-
work. Fully connected layers map an unrolled version of the
input  x   Rhwd, which is a one-dimensional vector of the
elements of a tensor x   Rh w d, to an output gi   Rk
such that
where Wi   Rk hwd is a weight matrix, bi is a bias term,
and fi is typically a ReLU nonlinearity function. The fully
connected layers encode the input examples as feature vec-
tors, which are used as inputs to a  nal classi er. Since the
fully connected layer looks at the entire input at once, these
feature vectors  summarize  the input into a feature vec-
tor for classi cation. The model is trained end-to-end using
minibatch gradient descent and backpropagation.
gi = fi(Wi  x + bi),
After training, the output of the  nal fully connected layer
can be interpreted as an encoding of the input as a feature
vector that facilitates classi cation. These features often rep-
resent complex compositions of the lower-level features ex-
tracted by the previous layers (e.g., edges and corners) and
can range from grid patterns to animal faces (Zeiler and Fer-
gus 2013; Le et al. 2012).
Combining Transfer Learning and Deep Learning
The low-level and high-level features learned by a CNN on a
source domain can often be transferred to augment learning
in a different but related target domain. For target problems
with abundant data, we can transfer low-level features, such
as edges and corners, and learn new high-level features spe-
ci c to the target problem. For target problems with limited
amounts of data, learning new high-level features is dif cult.
However, if the source and target domain are suf ciently
similar, the feature representation learned by the CNN on
the source task can also be used for the target problem. Deep
features extracted from CNNs trained on large annotated
datasets of images have been used as generic features very
effectively for a wide range of vision tasks (Donahue et al.
2013; Oquab et al. 2014).
Transfer Learning for Poverty Mapping
In our approach to poverty mapping using satellite imagery,
we construct a linear chain transfer learning graph with
V = {P1,P2,P3} and E = {(P1,P2), (P2,P3)}. The  rst
transfer learning problem P1 is object recognition on Im-
ageNet (Russakovsky et al. 2014); the second problem P2
is predicting nighttime light intensity from daytime satellite
imagery; the third problem P3 is predicting poverty from
daytime satellite imagery. Recognizing the differences be-
tween ImageNet data and satellite imagery, we use the inter-
mediate problem P2 to learn about the bird s-eye viewpoint
of satellite imagery and extract features relevant to socioe-
conomic development.
ImageNet to Nighttime Lights
ImageNet is an object classi cation image dataset of over
14 million images with 1000 class labels that, along with
CNN models, have fueled major breakthroughs in many vi-
sion tasks (Russakovsky et al. 2014). CNN models trained
on the ImageNet dataset are recognized as good generic fea-
ture extractors, with low-level and mid-level features such
as edges and corners that are able to generalize to many new
tasks (Donahue et al. 2013; Oquab et al. 2014). Our goal is
to transfer knowledge from the ImageNet object recognition
challenge (P1) to the target problem of predicting nighttime
light intensity from daytime satellite imagery (P2).
In P1, we have an object classi cation problem with
source domain data D1 = {(x1i, y1i)} from ImageNet that
consists of natural images x1i   X1 and object class labels.
In P2, we have a nighttime light intensity prediction prob-
lem with target domain data D2 = {(x2i, y2i}) that consists
of daytime satellite images x2i   X2 and nighttime light
intensity labels. Although satellite data is still in the space
of image data, satellite imagery presents information from a
bird s-eye view and at a much different scale than the object-
centric ImageNet dataset (P (X1) (cid:54)= P (X2)). Previous work
in domains with images fundamentally different from nor-
mal  human-eye view  images typically resort to curating a
new, speci c dataset such as Places205 (Zhou et al. 2014).
In contrast, our transfer learning approach does not require
human annotation and is much more scalable. Additionally,
unsupervised approaches such as autoencoders may waste
representational capacity on irrelevant features, while the
nighttime light labels guide learning towards features rele-
vant to wealth and economic development.
The National Oceanic and Atmospheric Administration
(NOAA) provides annual nighttime images of the world
with 30 arc-second resolution, or about 1 square kilometer
(NOAA National Geophysical Data Center 2014). The light
intensity values are averaged and denoised for each year to
ensure that ephemeral light sources do not affect the data.
The nighttime light dataset D2 is constructed as follows:
The Demographic Health Survey (DHS) Program conducts
Figure 1: Locations (in white) of 330,000 sampled daytime
images near DHS survey locations for the nighttime light
intensity prediction problem.
nationally representative surveys in Africa that focus mainly
on health outcomes (ICF International 2015). Predicting
health outcomes is beyond the scope of this paper; how-
ever, the DHS surveys offer the most comprehensive data
available for Africa. Thus, we use DHS survey locations as
guidelines for sampling training images (see Figure 1). Im-
ages in D2 are daytime satellite images randomly sampled
near DHS survey locations in Africa. Satellite images are
downloaded using the Google Static Maps API, each with
400   400 pixels at zoom level 16, resulting in images sim-
ilar in size to pixels in the NOAA nighttime lights data. The
aggregate dataset D2 consists of over 330,000 images, each
labeled with an integer nighttime light intensity value rang-
ing from 0 to 631. We further subsample and bin the data
using a Gaussian mixture model, as detailed in the compan-
ion technical report (Xie et al. 2015).
Nighttime Lights to Poverty Estimation
The  nal and most important learning task P3 is that of pre-
dicting poverty from satellite imagery, for which we have
very limited training data. Our goal is to transfer knowledge
from P2, a data-rich problem, to P3.
The target domain data D3 = {(x3i, y3i)} consists of
satellite images x3i   X3 from the feature space of satellite
images of Uganda and a limited number of poverty labels
y3i   Y3, detailed below. The source data is D2, the night-
time lights data. Here, the input feature space of images is
similar in both the source and target domains, drawn from
a similar distribution of images (satellite images) from re-
lated areas (Africa and Uganda), implying that X2 = X3,
P (X2)   P (X3). The source (lights) and target (poverty)
tasks both have economic elements, but are quite different.
The poverty training data D3 relies on the Living Stan-
dards Measurement Study (LSMS) survey conducted in
Uganda by the Uganda Bureau of Statistics between 2011
1Nighttime light intensities are from 2013, while the daytime
satellite images are from 2015. We assume that the areas under
study have not changed signi cantly in this two-year period, but
this temporal mismatch is a potential source of error.
and 2012 (Uganda Bureau of Statistics 2012). The LSMS
survey consists of data from 2,716 households in Uganda,
which are grouped into 643 unique location groups. The
average latitude and longitude location of the households
within each group is given, with added noise of up to 5km
in each direction. Individual household locations are with-
held to preserve anonymity. In addition, each household has
a binary poverty label based on expenditure data from the
survey. We use the majority poverty classi cation of house-
holds in each group as the overall location group poverty
label. For a given group, we sample approximately 100
1km 1km images tiling a 10km   10km area centered at
the average household location as input. This de nes the
probability distribution P (X3) of the input images for the
poverty classi cation problem P3.
Predicting Nighttime Light Intensity
Our  rst goal is to transfer knowledge from the ImageNet
object recognition task to the nighttime light intensity pre-
diction problem. We start with a CNN model with parame-
ters trained on ImageNet, then modify the network to adapt it
to the new task (i.e., change the classi er on the last layer to
re ect the new nighttime light prediction task). We train on
the new task using SGD with momentum, using ImageNet
parameters as initialization to achieve knowledge transfer.
We choose the VGG F model trained on ImageNet as the
starting CNN model
(Chat eld et al. 2014). The VGG F
model has 8 convolutional and fully connected layers. Like
many other ImageNet models, the VGG F model accepts a
 xed input image size of 224   224 pixels. Input images
in D2, however, are 400   400 pixels, corresponding to the
resolution of the nighttime lights data.
We consider two ways of adapting the original VGG F
network. The  rst approach is to keep the structure of the
network (except for the  nal classi er) and crop the input
to 224   224 pixels (random cropping). This is a reason-
able approach, as the original model was trained by cropping
224   224 images from a larger 256   256 image (Chat eld
et al. 2014). Ideally, we would evaluate the network at multi-
ple crops of the 400   400 input and average the predictions
to leverage the context of the entire input image. However,
doing this explicitly with one forward pass for each crop
would be too costly. Alternatively, if we allow the multiple
crops of the image to overlap, we can use a convolution to
compute scores for each crop simultaneously, gaining speed
by reusing  lter outputs at all layers. We therefore propose
a fully convolutional architecture (fully convolutional).
Fully Convolutional Model
Fully convolutional models have been used successfully for
spatial analysis of arbitrary size inputs (Wolf and Platt 1994;
Long, Shelhamer, and Darrell 2014). We construct the fully
convolutional model by converting the fully connected lay-
ers of the VGG F network to convolutional layers. This al-
lows the network to ef ciently  slide  across a larger input
image and make multiple evaluations of different parts of the
image, incorporating all available contextual information.
Given an unrolled h w d-dimensional input x   Rhwd,
fully connected layers perform a matrix-vector product
 x = f (W x + b)
where W   Rk hwd is a weight matrix, b is a bias term,
f is a nonlinearity function, and  x   Rk is the output. In
the fully connected layer, we take k inner products with the
unrolled x vector. Thus, given a differently sized input, it is
unclear how to evaluate the dot products.
We replace a fully connected layer by a convolutional
layer with k convolutional  lters of size h   w, the same
size as the input. The  lter weights are shared across all
channels, which means that the convolutional layer actually
uses fewer parameters than the fully connected layer. Since
the  lter size is matched with the input size, we can take
an element-wise product and add, which is equivalent to an
inner product. This results in a scalar output for each  l-
ter, creating an output  x   R1 1 k. Further fully connected
layers are converted to convolutional layers with  lter size
1  1, matching the new input  x   R1 1 k. Fully connected
layers are usually the last layers of the network, while all
previous layers are typically convolutional. After converting
fully connected layers to convolutional layers, the entire net-
work becomes convolutional, allowing the outputs of each
layer to be reused as the convolution slides the network over
a larger input. Instead of a scalar output, the new output is a
2-dimensional map of  lter activations.
In our fully convolutional model, the 400  400 input pro-
duces an output of size 2   2   4096, which represents the
scores of four (overlapping) quadrants of the image for 4096
features. The regional scores are then averaged to obtain a
4096-dimensional feature vector that becomes the  nal in-
put to the classi er predicting nighttime light intensity.
Training and Performance Evaluation
Both CNN models are trained using minibatched gradient
descent with momentum. Random mirroring is used for data
augmentation, along with 50% dropout on convolutional
layers replacing fully connected layers. The learning rate be-
gins at 1e-6, a hundredth of the ending learning rate of the
VGG model. All other hyperparameters are the same as in the
VGG model as described in (Chat eld et al. 2014). The VGG
model parameters are obtained from the Caffe Model Zoo,
and all networks are trained with Caffe (Jia et al. 2014). The
fully convolutional model is  ne-tuned from the pre-trained
parameters of the VGG F model, but it randomly initializes
the convolutional layers that replace fully connected layers.
In the process of cropping, the random cropping model
throws away over 68% of the input image when predict-
ing the class scores, losing much of the spatial context. The
random cropping model achieved a validation accuracy of
70.04% after 400,200 SGD iterations. In comparison, the
fully convolutional model achieved 71.58% validation accu-
racy after only 223,500 iterations. Both models were trained
in roughly three days. Despite reinitializing the  nal convo-
lutional layers from scratch, the fully convolutional model
exhibits faster learning and better performance. The  nal
fully convolutional model achieves a validation accuracy of
71.71%, trained over 345,000 iterations.
Figure 2: Left: Each row shows  ve maximally activating images for a different  lter in the  fth convolutional layer of the
CNN trained on the nighttime light intensity prediction problem. The  rst  lter ( rst row) activates for urban areas. The second
 lter activates for farmland and grid-like patterns. The third  lter activates for roads. The fourth  lter activates for water, plains,
and forests, terrains contributing similarly to nighttime light intensity. The only supervision used is nighttime light intensity,
i.e., no labeled examples of roads or farmlands are provided. Right: Filter activations for the corresponding images on the left.
Filters mostly activate on the relevant portions of the image. For example, in the third row, the strongest activations coincide
with the road segments. Best seen in color. See the companion technical report for more visualizations (Xie et al. 2015). Images
from Google Static Maps.
Visualizing the Extracted Features
Nighttime lights are used as a data-rich proxy, so absolute
performance on this task is not directly relevant for poverty
mapping. The goal is to learn high-level features that are
indicative of economic development and can be used for
poverty mapping in the spirit of transfer learning.
We visualize the  lters learned by the fully convolutional
network by inspecting the 25 maximally activating images
for each  lter (Figure 2, left and the companion technical
report for more visualizations (Xie et al. 2015)). Activation
levels for  lters in the middle of the network are obtained by
passing the images forward through the  lter, applying the
ReLU nonlinearity, and then averaging the map of activation
values. We  nd that many  lters learn to identify semanti-
cally meaningful features such as urban areas, water, roads,
barren land, forests, and farmland. Amazingly, these fea-
tures are learned without direct supervision, in contrast
to previous efforts to extract features from aerial imagery,
which have relied heavily on large amounts of expert-labeled
data, e.g., labeled examples of roads
(Mnih and Hinton
2010; 2012). To con rm the semantics of the  lters, we vi-
sualize their activations for the same set of images (Figure 2,
right). These maps con rm our interpretation by identifying
the image parts that are most responsible for activating the
 lter. For example, the  lter in the third row mostly acti-
vates on road segments. These features are extremely useful
socioeconomic indicators and suggest that transfer learning
to the poverty task is possible.
Poverty Estimation and Mapping
The  rst target task we consider is to predict whether the ma-
jority of households are above or below the poverty thresh-
old for 643 groups of households in Uganda.
Given the limited amount of training data, we do not at-
tempt to learn new feature representations for the target task.
Instead, we directly use the feature representation learned by
the CNN on the nighttime lights task (P2). Speci cally, we
evaluate the CNN model on new input images and feed the
feature vector produced in the last layer as input to a logis-
tic regression classi er, which is trained on the poverty task
(transfer model). Approximately 100 images in a 10km  
10km area around the average household location of each
group are used as input. We compare against the perfor-
mance of a classi er with features from the VGG F model
trained on ImageNet only (ImageNet model), i.e., without
transfer learning from nighttime lights. In both the ImageNet
model and the transfer model, the feature vectors are aver-
aged over the input images for each group.
The Uganda LSMS survey also includes household-
speci c data. We extract the features that could feasibly be
detected with remote sensing techniques, including roof ma-
terial, number of rooms, house type, distances to various in-
frastructure points, urban or rural classi cation, annual tem-
perature, and annual precipitation. These survey features are
then averaged over each household group. The performance
of the classi er trained with survey features (survey model)
represents the gold standard for remote sensing techniques.
We also compare with a classi er trained using the nighttime
light intensities themselves as features (lights model). The
nighttime light features consist of the average light intensity,
summary statistics, and histogram-based features for each
area. Finally, we compare with a classi er trained using a
concatenation of ImageNet features and nighttime light fea-
tures (ImageNet + lights model), an explicit way of com-
bining information from both source problems.
All models are trained using a logistic regression classi er
with L1 regularization using a nested 10-fold cross valida-
tion (CV) scheme, where the inner CV is used to tune a new
regularization parameter for each outer CV iteration. The
regularization parameter is found by a two-stage approach:
Figure 3: Left: Predicted poverty probabilities at a  ne-grained 10km   10km block level. Middle: Predicted poverty proba-
bilities aggregated at the district-level. Right: 2005 survey results for comparison (World Resources Institute 2009).
Survey
ImgNet
Lights
Accuracy
F1 Score
Precision
Recall
AUC
0.754
0.552
0.450
0.722
0.776
0.686
0.398
0.340
0.492
0.690
0.526
0.448
0.298
0.914
0.719
ImgNet
+Lights
0.683
0.400
0.338
0.506
0.700
Transfer
0.716
0.489
0.394
0.658
0.761
Table 1: Cross validation test performance for predicting
aggregate-level poverty measures. Survey is trained on sur-
vey data collected in the  eld. All other models are based
on satellite imagery. Our transfer learning approach outper-
forms all non-survey classi ers signi cantly in every mea-
sure except recall, and approaches the survey model.
a coarse linearly spaced search is followed by a  ner lin-
early spaced search around the best value found in the coarse
search. The tuned regularization parameter is then validated
on the test set of the outer CV loop, which remained unseen
as the parameter was tuned. All performance metrics are av-
eraged over the outer 10 folds and reported in Table 1.
Our transfer model signi cantly outperforms every model
except the survey model in every measure except recall. No-
tably, the transfer model outperforms all combinations of
features from the source problems, implying that transfer
learning was successful in learning novel and useful fea-
tures. Remarkably, our transfer model based on remotely
sensed data approaches the performance of the survey model
based on data expensively collected in the  eld. As a sanity
check, we  nd that using simple traditional computer vision
features such as HOG and color histograms only achieves
slightly better performance than random guessing. This fur-
ther af rms that the transfer learning features are nontrivial
and contain information more complex than just edges and
colors.
To understand the high recall of the lights model, we
analyze the conditional probability of predicting  poverty 
given that the average light intensity is zero: The lights
model predicts  poverty  almost 100% of the time, though
only 51% of groups with zero average intensity are actu-
ally below the poverty line. Furthermore, only 6% of groups
with nonzero average light intensity are below the poverty
line, explaining the high recall of the lights model. In con-
trast, the transfer model predicts  poverty  in 52% of groups
where the average nighttime light intensity is 0, more accu-
rately re ecting the actual probability. The transfer model
features (visualized in Figure 2) clearly contain additional,
meaningful information beyond what nighttime lights can
provide. The fact that the transfer model outperforms the
lights model indicates that transfer learning has succeeded.
Mapping Poverty Distribution
Using our transfer model, we can scalably and inexpen-
sively construct  ne-grained poverty maps at the country
or even continent level. We evaluate this capability by es-
timating a country-level poverty map for Uganda. We down-
load over 370,000 satellite images covering Uganda and esti-
mate poverty probabilities at 1km   1km resolution with the
transfer model. Areas where the model assigns a low proba-
bility of being impoverished are colored green, while areas
assigned a high risk of poverty are colored red. A 10km  
10km resolution map is shown in Figure 3 (left), smoothed at
a 0.5 degree radius for easy identi cation of dominant spa-
tial patterns. Notably, poverty reduction in northern Uganda
is lagging (Ministry of Finance 2014). Figure 3 (middle)
shows poverty estimates aggregated at the district level. As a
validity check, we qualitatively compare this map against the
most recent map of poverty rates available (Figure 3, right),
which is based on 2005 survey data (World Resources In-
stitute 2009). This data is now a decade old, but it loosely
corroborates the major patterns in our predicted distribution.
Whereas current maps are coarse and outdated, our method
offers much  ner temporal and spatial resolution and an in-
expensive way to evaluate poverty at a global scale.
Conclusion
We introduce a new transfer learning approach for analyz-
ing satellite imagery that leverages recent deep learning ad-
vances and multiple data-rich proxy tasks to learn high-level
feature representations of satellite images. This knowledge
is then transferred to data-poor tasks of interest in the spirit
of transfer learning. We demonstrate an application of this
idea in the context of poverty mapping and introduce a fully
convolutional CNN model that, without explicit supervision,
learns to identify complex features such as roads, urban ar-
eas, and various terrains. Using these features, we are able
to approach the performance of data collected in the  eld for
poverty estimation. Remarkably, our approach outperforms
models based directly on the data-rich proxies used in our
transfer learning pipeline. Our approach can easily be gen-
eralized to other remote sensing tasks and has great potential
to help solve global sustainability challenges.
Acknowledgements
We acknowledge the support of the Department of De-
fense through the National Defense Science and Engineering
Graduate Fellowship Program. We would also like to thank
NVIDIA Corporation for their contribution to this project
through an NVIDIA Academic Hardware Grant.
References
Abelson, B.; Varshney, K.; and Sun, J. 2014. Targeting direct
cash transfers to the extremely poor. In Proceedings of the
20th ACM SIGKDD international conference on Knowledge
discovery and data mining, 1563 1572. ACM.
Bouvrie, J. 2006. Notes on convolutional neural networks.
Chat eld, K.; Simonyan, K.; Vedaldi, A.; and Zisserman, A.
2014. Return of the devil in the details: Delving deep into
convolutional nets. arXiv preprint arXiv:1405.3531.
Donahue, J.; Jia, Y.; Vinyals, O.; Hoffman, J.; Zhang, N.;
Tzeng, E.; and Darrell, T. 2013. DeCAF: A deep con-
volutional activation feature for generic visual recognition.
CoRR abs/1310.1531.
ICF International. 2015. Demographic and health surveys
(various) [datasets].
Independent Expert Advisory Group Secretariat. 2014. A
world that counts: Mobilising the data revolution for sus-
tainable development. Technical report.
Jia, Y.; Shelhamer, E.; Donahue, J.; Karayev, S.; Long, J.;
Girshick, R. B.; Guadarrama, S.; and Darrell, T.
2014.
Caffe: Convolutional architecture for fast feature embed-
ding. CoRR abs/1408.5093.
Le, Q. V.; Ranzato, M.; Monga, R.; Devin, M.; Chen, K.;
Corrado, G. S.; Dean, J.; and Ng, A. Y. 2012. Building
high-level features using large scale unsupervised learning.
In International Conference on Machine Learning.
Long, J.; Shelhamer, E.; and Darrell, T.
Fully
2014.
convolutional networks for semantic segmentation. CoRR
abs/1411.4038.
Ministry of Finance. 2014. Poverty status report 2014:
Structural change and poverty reduction in Uganda.
Mnih, V., and Hinton, G. E. 2010. Learning to detect roads
in high-resolution aerial images. In Computer Vision ECCV
2010. Springer. 210 223.
Mnih, V., and Hinton, G. E. 2012. Learning to label aerial
images from noisy data. In Proceedings of the 29th Interna-
tional Conference on Machine Learning (ICML-12), 567 
574.
Murthy, K.; Shearn, M.; Smiley, B. D.; Chau, A. H.; Levine,
J.; and Robinson, D. 2014. Skysat-1: very high-resolution
In SPIE Remote Sensing,
imagery from a small satellite.
92411E 92411E. International Society for Optics and Pho-
tonics.
NOAA National Geophysical Data Center. 2014. F18 2013
nighttime lights composite.
Oquab, M.; Bottou, L.; Laptev, I.; and Sivic, J. 2014. Learn-
ing and transferring mid-level image representations using
convolutional neural networks. In Proceedings of the 2014
IEEE Conference on Computer Vision and Pattern Recogni-
tion, CVPR  14, 1717 1724. Washington, DC, USA: IEEE
Computer Society.
Pan, S. J., and Yang, Q. 2010. A survey on transfer learn-
ing. Knowledge and Data Engineering, IEEE Transactions
on 22(10):1345 1359.
Razavian, A. S.; Azizpour, H.; Sullivan, J.; and Carlsson, S.
2014. CNN features off-the-shelf: an astounding baseline
for recognition. CoRR abs/1403.6382.
Russakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.;
Ma, S.; Huang, Z.; Karpathy, A.; Khosla, A.; Bernstein, M.;
Berg, A. C.; and Fei-Fei, L. 2014.
ImageNet large scale
visual recognition challenge. International Journal of Com-
puter Vision 1 42.
Uganda Bureau of Statistics. 2012. Uganda national panel
survey 2011/2012.
United Nations. 2015. The millennium development goals
report 2015.
Varshney, K. R.; Chen, G. H.; Abelson, B.; Nowocin, K.;
Sakhrani, V.; Xu, L.; and Spatocco, B. L. 2015. Targeting
villages for rural development using satellite image analysis.
Big Data 3(1):41 53.
Wolf, R., and Platt, J. C. 1994. Postal address block loca-
tion using a convolutional locator network. In Advances in
Neural Information Processing Systems, 745 752. Morgan
Kaufmann Publishers.
World Resources Institute. 2009. Mapping a better fu-
ture: How spatial analysis can bene t wetlands and reduce
poverty in Uganda.
Xie, M.; Jean, N.; Burke, M.; Lobell, D.; and Ermon, S.
2015. Transfer learning from deep features for remote sens-
ing and poverty mapping. CoRR abs/1510.00098.
Zeiler, M. D., and Fergus, R. 2013. Visualizing and under-
standing convolutional networks. CoRR abs/1311.2901.
Zhou, B.; Lapedriza, A.; Xiao, J.; Torralba, A.; and Oliva,
A. 2014. Learning deep features for scene recognition using
In Advances in Neural Information Pro-
Places database.
cessing Systems, 487 495.
Appendix: Data Preparation
Of the over 330,000 images in D2, 58% of the images are
labeled with zero nighttime light intensity. This is a very un-
balanced dataset, which is dif cult to learn from. We thus
alter P (X2) by choosing to upsample images with higher
intensities and downsample images with zero intensity until
the least frequently occurring intensity has at least half the
occurrences of the most frequent class. Observing that ex-
amples at similar intensity levels are hard to distinguish, a
relabeled dataset with three integer label bins ranging from
0 to 2 was created by clustering the intensity levels by fre-
quency using a 3-component Gaussian mixture model and
then balancing the dataset for three classes. The 3-class bal-
anced dataset consists of 150,000 training images and 8,000
validation images. We  nd that binning by frequency sepa-
rates the intensity classes more intuitively. Because predict-
ing absolute nighttime light intensity is not the  nal target
task, it is more important for the model to extract features
that are semantically meaningful than to predict light inten-
sity with the highest accuracy.
Filter Visualizations
We provide 25 maximally activating images in the valida-
tion set and their activation maps for four  lters in our CNN
model (Figures 4,5,6,7). The activation maps indicate the lo-
cations where the  lter activated the most. These  lters seem
to activate to different terrain types, man-made structures,
and roads, all of which can be useful socioeconomic indica-
tors.
Figure 4: A set of 25 maximally activating images and their corresponding activation maps for a  lter in the  fth convolutional
layer of the network trained on the 3-class nighttime light intensity prediction task. This  lter seems to activate for urban areas,
which indicate economic development.
Figure 5: A set of 25 maximally activating images and their corresponding activation maps for a  lter in the  fth convolutional
layer of the network trained on the 3-class nighttime light intensity prediction task. This  lter seems to activate for roads, which
are indicative of infrastructure and economic development.
Figure 6: A set of 25 maximally activating images and their corresponding activation maps for a  lter in the  fth convolutional
layer of the network trained on the 3-class nighttime light intensity prediction task. This  lter seems to activate for water, barren,
and forested lands, which this  lter seems to group together as contributing similarly to nighttime light intensity.
Figure 7: A set of 25 maximally activating images and their corresponding activation maps for a  lter in the  fth convolutional
layer of the network trained on the 3-class nighttime light intensity prediction task. This  lter seems to activate for farmland
and for grid-like patterns, which are common in human-made structures.
