A Distributed Learning Dynamics in Social Groups

L. Elisa Celis∗1, Peter M. Kraﬀt2, and Nisheeth K. Vishnoi3

1,3 ´Ecole Polytechnique F´ed´erale de Lausanne (EPFL), Switzerland

2MIT

Abstract

We study a distributed learning process observed in human groups and other social animals.
This learning process appears in settings in which each individual in a group is trying to decide
over time, in a distributed manner, which option to select among a shared set of options. Specif-
ically, we consider a stochastic dynamics in a group in which every individual selects an option
in the following two-step process: (1) select a random individual and observe the option that
individual chose in the previous time step, and (2) adopt that option if its stochastic quality was
good at that time step. Various instantiations of such distributed learning appear in nature, and
have also been studied in the social science literature. From the perspective of an individual, an
attractive feature of this learning process is that it is a simple heuristic that requires extremely
limited computational capacities. But what does it mean for the group – could such a simple,
distributed and essentially memoryless process lead the group as a whole to perform optimally?
We show that the answer to this question is yes – this distributed learning is highly eﬀective
at identifying the best option and is close to optimal for the group overall. Our analysis also
gives quantitative bounds that show fast convergence of these stochastic dynamics. We prove
our result by ﬁrst deﬁning a (stochastic) inﬁnite population version of these distributed learning
dynamics and then combining its strong convergence properties along with its relation to the
ﬁnite population dynamics. Prior to our work the only theoretical work related to such learning
dynamics has been either in deterministic special cases or in the asymptotic setting. Finally, we
observe that our inﬁnite population dynamics is a stochastic variant of the classic multiplicative
weights update (MWU) method. Consequently, we arrive at the following interesting converse:
the learning dynamics on a ﬁnite population considered here can be viewed as a novel distributed
and low-memory implementation of the classic MWU method.

7
1
0
2

 

y
a
M
8

 

 
 
]

G
L
.
s
c
[
 
 

1
v
4
1
4
3
0

.

5
0
7
1
:
v
i
X
r
a

∗This research was supported in part by an SNF Project Grant (205121 163385).

1

1 Introduction

A powerful assumption – often leveraged in biology, ecology, and evolutionary psychology in order to reason
about why humans, animals, and other biological organisms behave in certain ways – is to suppose that be-
havior is tuned to alleviate evolutionary pressure. This assumption provides deductive power because once
a behavior is assumed to be optimally or near-optimally solving some problem, one can then attempt to dis-
cover which problem the system might be solving through the computational lens. We study this phenomenon
in the context of social behavior. In particular, we consider a class of distributed social learning dynamics
which are at once conspicuous in daily life, oft discussed in the social science literature, and also empirically
veriﬁed; yet are also simple to the point that they appear perhaps suboptimal. Consider the setting consisting
of a social group of N individuals presented with a set of m options of diﬀerent quality. The quality of each
option is assumed to be an independent random variable whose parameters are unknown to the individuals
and remain ﬁxed over time. At each time step each individual selects one option and observes a stochastic
indicator of that option’s quality. The goal of the individual is to identify the best option.

The distributed learning dynamics then boils down to individuals copying or imitating the behavior of others
in an eﬀort to solve this problem. Such dynamics roughly have the following two steps: At each time step,
each individual independently decides which option to select by ﬁrst

1. Sampling – observing the choice of a random member of the group at the last time step, and then

2. Adopting – deciding whether or not to adopt the recommended option as a function of the most recent

(stochastic) signal of that option’s quality.

Instances of such two-stage distributed learning dynamics in social settings have been widely proposed and
validated with data in the literature on human choice behavior (e.g., [7, 10, 29, 32, 34]) and animal behavior
(e.g., [40, 43]). They are cognitively simple because individuals need not maintain any history of previous
observations; rather they only use the most recent quality signal of one option. Furthermore, as with many
other distributed protocols observed in nature (e.g., [27, 35]), each step requires only limited communication
with other group members. In light of the fact that such distributed learning processes are ubiquitous, the
question arises – why? To answer this, we need to understand the following: What does such a distributed
learning dynamics imply for the group as a whole?

Our Contribution.

We consider a general model that captures a wide variety of distributed learning dynamics in social settings
as deﬁned above (see Section 2.1 for a formal deﬁnition) and study two fundamental algorithmic questions:

• do such learning dynamics (despite each individual having a limited memory) have the potential to

successfully converge to the best option for the collective population? and if so,

• how eﬃcient are such dynamics?

We prove that the answer to the ﬁrst question is yes and provide quantitative bounds for the second in the
most general case in which the population is ﬁnite and both the sampling and adopting step are stochastic.

In general, due to the fact that populations are ﬁnite and there is stochasticity in both steps, the social learning
dynamics may be chaotic, with no single option dominating, and the popularity of options rising and falling

2

over time. However, we prove that social learning leads the population, as a whole, to be competitive – pretty
quickly – as compared to the best strategy in hindsight; i.e., the dynamics have low regret (see section 2.1).
Prior to our work, many instances of such social learning dynamics have been proposed and validated (see,
e.g., [39] for an exposition), and, even though on the surface there seem to be several related processes, the
only theoretical work has been either in deterministic special cases or in the asymptotic setting where both
the size of the population and time goes to inﬁnity; such results (see [12, 22] and Section 3) eﬀectively focus
on deriving asymptotic (large deviation) bounds. To the best of our knowledge, ours is the ﬁrst rigorous
analysis of this distributed learning dynamics in a realistic setting when the size of the group is ﬁnite.

Key to our results are the following two realizations which we can use to understand the emergent behavior
of the distributed learning dynamics:

• In the inﬁnite population limit, by rewriting the underlying stochastic equations of the distributed
learning dynamics, the individuals are eﬀectively implementing a stochastic variant of the classic mul-
tiplicative weights update (MWU) method [4] at the group-level.1

• While the ﬁnite population distributed learning dynamics can be approximated with its inﬁnite pop-
ulation limit for short times, to ensure that the regret bounds remain valid for longer times, further
new ideas are required. Here, we show how we can appeal to the strong convergence properties of the
inﬁnite population stochastic dynamics to ascertain that the regret remains bounded for all times in the
ﬁnite population case.

Computationally, contrasting with typical implementations of the MWU method, in the learning dynamics
we consider, no individual keeps track of the weights. Rather, the popularity of the options in the previ-
ous time step serve as a proxy for weights, and suﬃce to propel the process forward. Thus, we arrive at
the following interesting conclusion – the learning dynamics in social groups considered here can inform
novel, low-memory, low-communication, distributed implementations of the MWU algorithm in the stochas-
tic setting; perhaps appropriate for low-power devices in distributed settings such as sensor networks or the
internet-of-things.

2 Model, Our Results and Overview

2.1 The Model

The learning environment we consider consists of a set of N individuals repeatedly choosing among m options
during a sequence of T discrete time steps. Each option has an unknown underlying quality, η1 ≥ η2 ≥
··· ≥ ηm ∈ [0, 1], which represents the probability that the option is “good” at any given time step; we let
Rt
j = Bernoulli(ηj) be the indicator random variable for the event that option j is good at time t. The goal
of each individual is to select the best option. Now, let X t
i j ∈ {0, 1} be the indicator random variable for the
event that individual i chooses to adopt option j at time t. The distributed learning dynamics we consider is
a two-stage model:

1This stochastic process is not to be mistaken with the standard deterministic MWU or its continuous

time limit, the replicator dynamics.

3

(1) Sampling. First, individuals select which option to consider, and then they choose whether or not to
adopt that option. To obtain an option to consider at time t + 1, with probability µ individual i selects an
option j ∈ [m] to consider uniformly at random, and with probability (1− µ) individual i selects an option
j ∈ [m] proportional to its current popularity:

Qt

j =

∑N
i=1 X t
k=1 ∑N

i j

i=1 X t
ik

∑m

m for all j.2 Note that this
is the fraction of the population that adopts option j at time t, and we assume Q0
can be implemented in a distributed manner as suggested in the introduction by letting i select a companion

j = 1

i′ ∈ {1, . . . , N} uniformly at random, and observing the choice of individual i′ at time t. The parameter µ > 0

is small and represents a fraction of the population which may take independent decisions; its role is to ensure
that the population does not get stuck in a bad option.

(2) Adopting.
In the second stage, after choosing an option j to consider, individual i must then decide
whether to commit to this option or to sit out during this time step. Individual i observes the most recent
quality signal associated with j, namely Rt+1
) ∈
{0, 1} where fi is a stochastic function such that E[ fi(1)] > E[ fi(0)]. Hence, for each i, we can express fi in
the following form:

and decides to adopt the option with probability fi(Rt+1

j

j

fi(Rt+1

j

) =


1 with probability βi if Rt+1
1 with probability αi if Rt+1
0

otherwise.

j = 1
j = 0

Here, αi ≤ βi are parameters of the model and represent how sensitive individuals are to the most recent signal
of goodness as compared to the weight they give to the recommendation. For simplicity in the exposition,
we assume that all fi are identical, and drop the index i. This assumption is not essential for our results – we
omit the details. Thus, the two relevant parameters are 0 ≤ α ≤ β ≤ 1.

Examples of the model. Many instances of social learning in the social sciences and economics literature
can be interpreted as special cases of the distributed learning dynamics introduced above. Here we give
two concrete examples – one direct and one indirect; see more discussion in Section 3. The simplest such
2 = η2 =··· = ηm.
The authors validate this model using observational data on the decisions of amateur investors on an online
platform in which users are able to copy the actions of others.

example [31] corresponds exactly to our model when α = 1−β for some β≥ 1

2 when η1 > 1

Another instance, which takes a bit of explanation, appears in the economics literature [22]. We present it
here because it illustrates two common ways in which more general-looking models, speciﬁcally ones with
continuous-valued rewards and reward diﬀerences across individuals, can often be reinterpreted in such a way
that our framework applies. The authors consider a learning setting where m = 2 and rewards rt
j are drawn
from a continuous-valued distribution F j. Furthermore, their model incorporates player-speciﬁc stochastic
shocks, so that if εt
i j ∼ G is the size of the shock to player i on option j at time t, then the reward to player i
is rt
i j. The sampling step (1) is similar, except that µ = 0. In the adoption step (2), if player i sampled

j + εt

2In the absence of prior knowledge, we initialize at the point where all options are equally popular. This
also simpliﬁes the exposition, but is not crucial to our results – our results hold from arbitrary initial condi-
tions.

4

1 + εt

i1 + εt

i′1 > rt

2 + εt

i2 + εt

1 (and Rt

2) be the indicator random variable for the event that rt

player i′, then player i adopts option 1 if rt
i′2, and adopts option 2 otherwise. To
convert this to our setting, let Rt
2 (and
2); this occurs with some probability p (and 1− p) and deﬁnes our parameters η1 = p > 1− p = η2.3
rt
1 < rt
Note that in the adoption step, we can replace εt
i′2 by a continuous random variable ξ. Then
2] and α = P [ξ > rt
β = P [ξ > rt
i js are i.i.d., ξ has zero mean and is
symmetric, hence α < β and our results apply. The authors consider the inﬁnite population version of this
model where a constant fraction of the population updates at each time step, and analyze its asymptotic
properties as T → ∞.

i1 + εt
i′1 − εt
2 − rt
1|rt
2 > rt

i2 − εt
1]. As the εt

2 − rt

1|rt

1 > rt

1 > rt

2.2 Our Results and Overview

In order to explain our results for the distributed learning dynamics we ﬁrst need to quantify a measure of
optimality. In the remainder of the paper we let α = 1 − β. This just simpliﬁes the reading and has no
impact on the statements of the theorem – the same bounds hold with a dependence on β
α as opposed to β
1−β .
Let Qt
j be the random variable which measures the fraction of individuals that chose option j at time t, as
deﬁned above. Wishfully, we might like to prove that as t becomes large, Qt
js, for all j 6= 1 are close to zero
(assuming η1 > η2). However, simple examples show that this may not be the case; the stochastic process is
non-monotone, even when there is a signiﬁcant gap between η1 and η2, and may step away signiﬁcantly from
Qt
1 ≈ 1 even for large t. Instead, we consider the average expected performance of the group when compared
to that of the best option:

RegretN(T ) := η1 −

1

T

T

∑
t=1

m

∑
j=1

E[Qt−1

j Rt

j].

As the name suggests, this is nothing but the average regret of this process; namely, the diﬀerence between
the group’s expected average reward if all individuals who adopted an option select the optimal j = 1, and
the group’s expected average reward selected according to the distributed learning process up to time T . The
following is the main technical contribution of the paper; see Theorem 4.4 for a formal statement of this
result.

and for all T ≥ ln m

Distributed learning achieves near-optimal regret in a social group. For a range of parameters 1/2 ≤ β ≤
e/(e+1), µ a small constant, N roughly at least m1/δ2
δ2 , RegretN(T ) is at most 6δ where
δ = ln(cid:16) β

1−β(cid:17). Thus, the closer β is to 1/2, the better the regret.

The proof of the above result relies on the following connection between our distributed learning dynamics
in a ﬁnite population and what can be thought of as an inﬁnite population variant of the distributed learning
dynamics. The latter can also be seen as a stochastic variant of the MWU method (see Lemma 4.5) which
we explain below. Consider m experts where expert j generates a stochastic reward Rt+1
at time t that is 1
j
with probability ηj and 0 otherwise. In this setting, a single player maintains weight W t
j for option j at time
t, which is updated multiplicatively in the following manner:4

3Note that in their model the Rt

1 and Rt

2 are correlated as exactly one of them is 1 in every time step;

however independence across t remains which suﬃces for our results.

4It is worth noticing the similarity between the weights update (in particular the ﬁrst term) and that used
in the result of Christiano et al. [18] who developed a variant of the MWU method in the design of a fast
algorithm for a ﬂow problem.

5

W t+1

j

j +

:= (1− µ)W t
{z
|

deterministic sampling

µ
m

m

∑
k=1

W t

βRt+1

j

j (1− β)1−Rt+1
}

{z

stochastic rewards

|

k!
}

and W 0
j = 1 for all j. We arrive at this update equation by the (non-rigorous) thought process that in the
inﬁnite population case, we can replace the stochastic quantities by their expectation in the sampling stage:
in this case, the expected fraction of individuals picking the option j in the sampling stage of the social
learning dynamics is proportional to (1− µ)W t
k . This gives us the ﬁrst term in the right-hand
side above. The second term is just f (Rt+1
) with respect to parameter β. We note that, since the rewards
are stochastic, it is not the standard adversarial MWU setting and, since all the information is known to the
population as a whole, it is also not the standard setting of stochastic bandits.

k=1 W t

j + µ

m ∑m

j

Even though these weight update equations are arrived at by a heuristic calculation, we can prove that for
short time periods, there is a coupling between the inﬁnite and the ﬁnite distributed learning dynamics such
that the stochastic trajectories corresponding to the weights in the inﬁnite population case remain close to
that of the ﬁnite population case.

Inﬁnite vs. ﬁnite population distributed social learning. If (Pt
by the weights W t

j after time t for a given sequence of rewards, then for all j, 1− 5t

j)m

√N ≤ Pt

j/Qt

j ≤ 1 + 5t
√N

.

j=1 is the probability distribution induced

The proof of this crucially relies on the fact that µ is strictly positive. On the other hand, the fact that µ > 0
also makes the analysis quite messy. Note that the closeness deteriorates very quickly and, in particular,
the bound becomes uninteresting after about log N time steps. On the other hand, for ﬁxed t, as N → ∞,
the trajectories of the two processes are identical; put another way, the distributed learning process over
inﬁnite populations is identical to the corresponding stochastic MWU method. This is typically the kind
of asymptotic result that exists in the literature. The more interesting and challenging direction is to show
convergence when N is large, but ﬁxed, and T goes to inﬁnity.

In order to leverage this connection between inﬁnite and ﬁnite population variants of the distributed learning
dynamics, we ﬁrst analyze the corresponding regret of the stochastic MWU method, which is deﬁned to be

Regret∞(T ) := η1 −

1

T

T

∑
t=1

m

∑
j=1

E[Pt−1

j Rt

j].

We can establish the following regret bound in this case; see Theorem 4.3.

Inﬁnite population distributed social learning dynamics achieves near-optimal regret. For a range of
δ2 , Regret∞(T ) is at most 3δ where

parameters 1/2 ≤ β ≤ e/(e+1), µ a small constant, and for all T ≥ ln m
1−β(cid:17).
δ = ln(cid:16) β

The proof of this obtained by adapting the proof of the MWU method to take into account stochastic rewards.

At this point we would be done except that the closeness between the probability distributions of ﬁnite and
inﬁnite distributed learning dynamics holds only for short times – if we run the process for about ln m
δ2 steps,
then for the probability distributions to be close we would need N ≥ mO(1/δ2). What about when T ≥ ln m
δ2 ?
To tackle this problem, we need a new idea. The ﬁrst observation is that, in fact, the regret bound for the

6

inﬁnite population case can be made stronger: roughly, as long as the starting distribution has enough en-
tropy, the regret becomes small in about the same number of steps. Thus, we can break T into epochs of
size approximately ln m
δ2 and observe that at the beginning of each epoch, each option will have about µ/m
probability – again we need crucially that µ > 0. Thus, we can show convergence starting from such a prob-
ability distribution in ln(m/µ)
iterations. As a consequence, in each epoch, we can bound the regret by about
6δ for slightly larger N. Rewriting it a diﬀerent way, having a ﬁnite population will have an additive error
term of approximately m
to the regret obtained by the inﬁnite population. Thus, we rely on the strong
attractive properties of the inﬁnite population stochastic dynamics to obtain quantitative regret bounds for
the ﬁnite population social learning dynamics. This contributes to the growing set of connections between
using attractors of dynamical systems to analyze stochastic processes [8, 38, 49, 50]. Details of the proof of
Theorem 4.4 appear in Section 4.3.

1/δ2
√N

δ2

3 Related Work

There is a growing body of work in theoretical computer science, and distributed computing theory in particu-
lar, that studies problems arising in the sciences through the computational lens; see, e.g., [1,17,19,27,35,46].
Such studies have also on occasion contributed back to computer science by providing insights into existing
techniques, or giving rise to novel bio-inspired algorithms. Our work touches upon both of these aspects.

Among related studies in the sciences, while dynamics that only have a sampling step [14, 23] or only an
adoption step [30] have been studied, combining both has been shown empirically to result in a better strategy
[34]. Indeed, in line with these observations, one can formally see from our analysis that if we only have
sampling (β = 1− α = 1) or only have adoption (µ = 1), the process does not always converge to the best
option. Hence, both steps of the process seem crucial, and many models in sociology and economics are
such distributed two-step processes [5, 15, 22, 31, 41]. While some models a priori look diﬀerent, many can
be captured by our formulation; for example, models that have continuous rewards but whose adoption rule
depends on whether the reward is above or below a threshold [11, 12, 26] can be converted to a binary reward
structure in a standard way. Similarly, diﬀerences across individuals can be captured in the functions fi (see
the second example in Section 2.1). While some of these models consider the aggregate popularity of options
over time, many (including models for human behavior, e.g., [31]) consider only the current popularity. In
the economics literature, some similar ﬁnite population models have been studied, many of which also fall
into our framework. Their analysis has only been asymptotic as N, T → ∞; such results (see, e.g., [9, 12, 22])
eﬀectively focus on deriving large deviation bounds. In contrast, the main technical contribution of our work
includes quantitative bounds of the social learning dynamics for ﬁnite populations (N < ∞). Asymptotic and
inﬁnite-population results follow as corollaries.

There has been a large body of work on the distributed consensus problem; see for instance [3, 6] and the
references therein. The goal in such problems is for all individuals to agree on a single opinion, and various
distributed dynamics for doing so have been proposed and analyzed. Our setting diﬀers in that there is
additional information – the repeated stochastic signals associated to the quality of each opinion.

In evolutionary game theory, similar-looking deterministic imitator dynamics have been considered (see
[37,44] for an overview). A key diﬀerence with this work is the learning environment – we are not attempting
to select a strategy in a game, rather are trying to identify the best option of out a collection. Hence, the reward
of an individual depends on her choice j, while in evolutionary game theory the reward of an individual

7

depends on the choices of the entire population. In this setting one can still consider the regret of an individual
(see, e.g., [13]). While fast convergence of similar dynamics has been shown for some special cases (e.g.,
potential games [21] and selﬁsh routing [24]), for general games we cannot expect to always converge quickly
unless PPAD ⊆ P. In fact, it has been shown that in some games, versions of the replicator dynamics may
converge to outcomes that are not optimal, or not even equilibria [26, 42].

The fact that an MWU-like method emerges from a simple distributed behavior of individuals in a social
setting, is somewhat reminiscent of unrelated results that arise in the context of biological evolution [16]
and task allocation among ants [47]. The MWU algorithm [4], or its well-known continuous time limit, the
replicator dynamics [45, 48] can also be seen as a special case of our distributed learning dynamics if we
remove the randomness from both the sampling and adopting steps and the rewards (eﬀectively taking the
process to a deterministic and inﬁnite setting). However, as-is, MWU-type dynamics are diﬀerent because
each individual must eﬀectively maintain full weights (or a mixed strategy vector) at every time step. Fur-
thermore, the lack of stochasticity and the inﬁnite population setting avoid the key technical hurdles in the
analysis of our model.

Our results suggest that the distributed learning dynamics in ﬁnite populations can be viewed as a novel
distributed and approximate implementation of the MWU method. While parallelized implementations for
solving multi-armed bandit problem exist (see, e.g., [2, 25, 28, 33, 36]), in such works each node explicitly
maintains a weight vector on all options. The most distinctive aspect of the distributed MWU interpretation
of the learning dynamics we consider is that no such memory is required – the weights are represented
implicitly by the popularity of the various options, and the sampling and adopting processes require almost
no memory. This diﬀerence distinguishes our distributed learning dynamics from prior work on distributed
MWU or bandit methods.

4 Technical Details and Proofs

4.1 Basic Facts

We ﬁrst recall a few theorems and deﬁnitions that will be useful in our proofs.
Theorem 4.1 (Chernoﬀ-Hoeﬀding bounds [20]). Let Z1, Z2, . . . , Zn be independent Bernoulli random vari-
ables with mean γi. Let γ := 1
Deﬁnition 4.1. For real numbers A, B, and c ≥ 0, the notation A
Fact 4.2. For ﬁxed 0 ≤ δ ≤ 1 and for all 0 ≤ x ≤ 1, eδx ≤ 1 + (eδ − 1)x.

i=1 γi. When 0 < δ ≤ 1, we have P(cid:2)(cid:12)(cid:12) 1

i=1 Zi − γ(cid:12)(cid:12) > γδ(cid:3) ≤ 2 exp(cid:0)−nγδ2/3(cid:1) .

c∼ B denotes 1

c ≤ A

B ≤ c.

n ∑n

n ∑n

4.2 The Inﬁnite Population Distributed Learning Dynamics

Consider the following stochastic process: W 0

j := 1 for all 1 ≤ j ≤ m. For t > 0 and for all 1 ≤ j ≤ m

W t+1

j

:= (1− µ)W t

j +

µ
m

m

∑
k=1

W t

k! βRt+1

j (1− β)1−Rt+1

j

.

(1)

This deﬁnition parallels the two-step ﬁnite population distributed learning dynamics and can be thought of as
an inﬁnite population distributed learning dynamics: W t+1
j (with weight 1−

is ﬁrst updated according to W t

j

8

µ) and with uniform additive factor ∑m
function of β and Rt+1

k=1 W t

j

. The stochasticity is now solely with respect to the Rt

k/m (with weight µ), and then (stochastically) as the corresponding
js. Now, consider the following
, which corresponds to the fraction of

probability distribution corresponding to these weights, Pt
the (inﬁnite) population that has adopted option j at time t.

j :=

W t
j
k=1 W t

k

∑m

Let δ := ln(cid:16) β

1−β(cid:17), and note that we can re-write the above as

W t+1

j

:= (1− β) (1− µ)W t

j +

µ
m

m

∑
k=1

W t

k! eδRt+1

j

.

This now takes a more standard form as a multiplicative update; the (1− β) term can be ignored as it is
cancels out in Pt
term that takes up a µ fraction
of the weight. One can think of this as a regularizing term and bears some similarity to what was considered
in the recent breakthrough result on computing ﬂows [18].

j, so the main diﬀerence with the standard MWU is the

k=1 W t
m

∑m

k

Assuming that η1 ≥ ηj for all j 6= 1, the optimal strategy for the population is to select option 1. If they did
this, then the average expected gain of the population is η1. On the other hand, the average expected gain of
ji . We now
the population over T iterations while following this stochastic process is 1
proceed to understand how the latter compares to the former – in other words, bound the regret of the inﬁnite
population stochastic process. Formally, in the remainder of this subsection, we prove the following result:
Theorem 4.3 (Regret of Inﬁnite Population Distributed Learning Dynamics). Let η1 > η2 ≥ ··· ≥ ηm, let
2 < β ≤ e
e+1 (and hence 0 < δ ≤ 1), and let 6µ ≤ δ2. Let P0 be the uniform distribution on {1, . . . , m} and
{Pt}T
t=1 be the probability distributions produced by the inﬁnite population distributed learning dynamics
with stochastic rewards {Rt}T

δ2 , the average expected regret after T steps is

t=1. Then for T ≥ ln m

j=1 EhPt−1

t=1 ∑m

j Rt

T ∑T

1

Furthermore, 1

T ∑T

t=1 E(cid:2)Pt−1

1

The proof of Theorem 4.3 appears in Section 5

Regret∞(T ) = η1 −
(cid:3) ≥ 1− 3δ

η1−η2

.

1
T ·

T

∑
t=1

m

∑
j=1

j Rt

EhPt−1

ji ≤ 3δ.

4.3 Main Result: Regret of the Distributed Learning Dynamics in Finite Populations

2 < β ≤ e

Theorem 4.4 (Regret of the Distributed Learning Process in Finite Populations). Let η1 > η2 ≥ ··· ≥ ηm,
let 1
(1−β)µ, and N is such
that

(1−β)µN , c := 240m

e+1 (and hence 0 < δ ≤ 1), let 6µ ≤ δ2, and let δ′′ :=q 60m ln N
µ(1− β)δ3 .

µ(1−β)(cid:17) 2 ln 5

ln N ≥ (cid:16)c

and N 10 ≥

24m ln m

δ′′2

N

4m

δ2

Let Q0 be the uniform distribution on {1, . . . , m} and {Qt}T
the ﬁnite population distributed learning dynamics with stochastic rewards {Rt}T
ji ≤ 6δ.
the average expected regret after T steps is η1 − 1

j=1 EhQt−1

T · ∑T

t=1 ∑m

j Rt

t=1 be the probability distributions produced by
mδ ≥ T ≥ ln m
δ2 ,

t=1, then for N10

9

Here, N 10 is arbitrary and can be made as large as required at the expense of constants.

In order to prove this result, we ﬁrst give an analysis which holds for T = ln m
this result for large T .

δ2 , and then show how to leverage

4.3.1 Small T

To conduct the analysis, we ﬁrst set up some deﬁnitions and which correspond to the two diﬀerent stages of
the ﬁnite population distributed learning process, and show that, in each step, the ﬁnite population dynamics
is approximated by the inﬁnite population distributed learning dynamics/MWU stochastic process.

j denote the number of people committed to option j at time t and let Qt

Stage 1. Let Dt
probability distribution capturing the relative popularity of option j at time t. Let S t+1
of people who select j after the ﬁrst stage in the sampling process in step t + 1 and let St+1
Y t+1
i j

be the
j ⊆ [N] denote the set
|. Let
be the indicator random variable for the event that i chooses j in stage one at time step t + 1. Note that

:= |S t+1

j :=

∑m

j

j

k

Dt
j
k=1 Dt

are independent conditioned on everything up to time t with

thesenY t+1
i j oN

i=1

Since St+1

j = ∑N

i=1 Y t+1

i j

i j = 1(cid:12)(cid:12)(cid:12) ti =(cid:16)(1− µ)Qt
PhY t+1

m(cid:17) ≥
, it follows from linearity of expectation that EhSt+1

µ
m

j +

µ

.

j

(2)

(cid:12)(cid:12)(cid:12) ti =(cid:16)(1− µ)Qt

j + µ

m(cid:17) N.

j

1+2δ′

N10 , St+1

Proposition 4.1. Let t ≥ 0 be ﬁxed. For δ′ :=q 30m ln N
µN ≤ 1
∼ (cid:16)(1− µ)Qt
on everything up to time t), for all j St+1
with probability 1− 2m
Proof. The proof follows from Chernoﬀ-Hoeﬀding bound (Theorem 4.1), noting that γ ≥ µ
m from (2), and
taking a union bound over all j ∈ [m]. For the latter part, note that for any ﬁxed t and for all j, with probability
at least 1− 2m
N10 , we have
j ≥ µN
St+1
2m .

2 , with probability at least 1− 2m
m(cid:17) N. Thus, we deduce that (unconditionally)
j + µ

2 , this implies that with probability at least 1− 2m

m . Since δ′ ≤ 1

j ≥ 1
1+2δ′

N10 (conditioned

j ≥ µN

2m for all j.

N10 , St+1

µN

Stage 2. There are two outcomes for each option j: Rt+1
random variable for the event that i commits to j in stage two of the process. Note that

j = 1 and Rt+1

j = 0. Let Zt+1

i j

be the indicator

j

j

j

S t+1

, Rt+1

i j = 1(cid:12)(cid:12)(cid:12)
PhZt+1
since β ≥ 1/2, and PhZt+1

,ti = βRt+1
i j = 1(cid:12)(cid:12)(cid:12)
if i ∈ S t+1
(cid:12)(cid:12)(cid:12)
follows from linearity of expectation that EhDt+1
Proposition 4.2. Let t ≥ 0 be ﬁxed. For δ′′ :=q 60m ln N

tioned on everything up to time t, S t+1

, Rt+1
(1−β)µN ≤ 1

, Rt+1

, Rt+1

S t+1

S t+1

j

j

j

j

), for and all j Dt+1

j (1− β)1−Rt+1
j ≥ (1− β)
,ti = 0 otherwise. Let Dt+1
,ti = St+1
j (1− β)1−Rt+1
2 , with probability greater than 1− 4m
∼ St+1

j (1− β)1−Rt+1

j = ∑ j∈S t+1

j βRt+1

j βRt+1

1+2δ′′

.

.

j

j

j

j

j

j

j

(3)

Zt+1
i j

. It

N10 (condi-

10

Proof. The proof again follows from Chernoﬀ-Hoeﬀding bound (Theorem 4.1), noting that γ ≥ 1− β from
(3), and taking a union bound over all j ∈ [m]. Here we have also used proposition 4.1 that St+1
2m for
large enough N for all j with probability at least 1− 2m
N10 .
Proposition 4.3. Let t ≥ 0 be ﬁxed. Let δ′′ :=q 60m ln N
∼ (cid:16)(1− µ)Qt

(1−β)µN . With probability greater than 1− 6m

Combining proposition 4.1 and proposition 4.2 we obtain the following.

on everything up to time t, Rt+1

m(cid:17) NβRt+1

j (1− β)1−Rt+1

N10 (conditioned

j ≥ µN

j + µ

) Dt+1

1+6δ′′

.

j

j

j

Proof. Follows directly from proposition 4.1 and proposition 4.2 and noting that δ′ ≤ δ′′ ≤ 1

2 . Thus

(1 + 2δ′)(1 + 2δ′′) ≤ 1 + 2δ′ + 2δ′′ + 4δ′δ′′ ≤ 1 + 6δ′′.

Now we establish a relationship between the probability distributions Pt and Qt . Note that both processes
start with P0 = Q0. Let δt := 5tδ′′.
Lemma 4.5. There is a coupling such that Pt
j

j with probability at least 1− 6tm
j and Qt

j so that the realizations of the Rt

N10 for all choices of {Rt

j}s.
js is the same in

1+δt∼ Qt
Proof. As suggested by our notation, we couple Pt
both processes for all j and all t.

j

j

∑m

Pt+1

j + µ

k + µ

N10 , Pt

Thus, with probability at least 1− 6tm
N10 ,

j. Let us condition on this event. Recall that
j (1− β)1−Rt+1
k (1− β)1−Rt+1

The proof proceeds by induction on t. The statement holds when t = 0 as, by deﬁnition, P0 = Q0. Assume
it holds for t. Thus, with probability at least 1− 6tm
j = (cid:16)(1− µ)Pt
k=1(cid:0)(1− µ)Pt
(cid:16)(1− µ)Qt
k=1(cid:0)(1− µ)Qt
∼ (cid:16)(1− µ)Qt
N10 . Thus, we obtain that with probability at least 1− 6(t+1)m

1+δt∼ Qt
m(cid:17) βRt+1
m(cid:1) βRt+1
m(cid:17) βRt+1
m(cid:1) βRt+1
m(cid:17) NβRt+1

Here we used induction for both the numerator and the denominator. From proposition 4.2, we know that

for all j with probability at least 1− 6m

j (1− β)1−Rt+1
k (1− β)1−Rt+1

j (1− β)1−Rt+1

(1+δt )2
∼

k + µ

j + µ

Dt+1

Pt+1

1+6δ′′

∑m

j +

N10

µ

.

.

j

k

j

k

j

j

j

∼

Pt+1

(1+δt)2(1+6δ′′)2

Dt+1
k=1 Dt+1
∑m
Now note that, assuming that δt = 5tδ′′ ≤ 1 and δ′′ ≤ 1
40 ,
(1+δt)2(1+ 6δ′′)2 ≤ (1+ 3δt)(1+ 13δ′′)≤ 1+ 3δt + 13δ′′ + 39δ′′δt ≤ 1+ 4δt + 13δ′′ ≤ 1+ 5t+1δ′′ = 1+δt+1
for t ≥ 2. For t = 1 the bound can be checked by a direct calculation.

= Qt+1

.

k

j

j

11

Hence, for any ﬁxed set of Rt

js, the trajectories Pt and Qt remain close. Thus, using Theorem 4.3 and
δT + 2δ. Here, the ﬁrst neg-
ative term in the left hand side of the equation occurs when Lemma 4.5 applies and the second negative term
N10 . Since

j=1 EhQt−1

δT + 2δ + 5T δ′′ + 6mT

ji− 6T 2m

Lemma 4.5, we obtain that η1 −(cid:0)1 + 5Tδ′′(cid:1) 1
is when it does not. Rearranging, we obtain η1 − 1
δ′′ :=q 240m ln N

(1−β)µN ≤q c ln N

j=1 EhQt−1
δ2 , 5T δ′′ ≤ m

NT ≤ ln m
ji ≤ ln m

(1−β)µ. Thus, when T = ln m

N for c = 240m

δ2 √c ln N
√N

t=1 ∑m

t=1 ∑m

T ∑T

T ∑T

j Rt

j Rt

that

ln 5

. Hence, when N is such

(4)

(5)

N
ln N ≥

2 ln 5
cm
δ2
δ′′2

and N 10 ≥

6m ln m

δ3

,

then

RegretN(T ) = η1 −

1

T

T

∑
t=1

m

∑
j=1

This concludes the proof of Theorem 4.4 when T = ln m
δ2 .

4.3.2 Large T

j Rt

EhQt−1

ji ≤ 5δ.

For large T we need one new ingredient; here we focus on this additional aspect. We ﬁrst need a slight
generalization of Theorem 4.3 to handle P0 that are not uniform. This is similar to the version of MWU with
restricted distributions as in Theorem 2.4 in [4].
Theorem 4.6 (Regret of Inﬁnite Population Distributed Learning Dynamics with Nonuniform Start). Let
j ≥ ζ for all j ∈
t=1 be the probability distributions produced by the MWU process with stochastic re-
, the average expected regret after T steps is Regret∞(T ) = η1 − 1
T ·

2 < β ≤ e
t=1. Then for T ≥ ln(1/ζ)

e+1 (and hence 0 < δ ≤ 1), and let 6µ ≤ δ2. Let P0

δ2

η1 ≥ η2 ≥ ··· ≥ ηm, let 1
{1, . . . , m} and {Pt}T
wards {Rt}T
j=1 EhPt−1
t=1 ∑m
∑T

ji ≤ 3δ.

j Rt

The proof closely follows from that of Theorem 4.3, and we omit the details.

Similarly, the results in Section 4.3.1 for small T follow analogously with nonuniform start. This just requires

us to chose N slightly bigger in particular, instead of (4), we would need N such that N
and N 10 ≥
6 ln m
ζδ3 . This gives that the regret of the ﬁnite population distributed learning dynamics is at most 5δ when T =
ln(1/ζ)
. We now complete the proof of Theorem 4.4 for large T , by breaking the time into epochs consisting

ln N ≥

δ′′2

δ2

δ2

of ln(1/ζ)
time steps. In each epoch, we couple an inﬁnite population distributed learning dynamics with the
ﬁnite population distributed learning dynamics such that the starting points are identical at the beginning of
each epoch, and both observe the same sequence of rewards Rt

js.

δ2

ζ(cid:17) 2 ln 5
c(cid:16) 1

It remains to lower bound ζ appropriately. From proposition 4.3, it follows that for any t, with probability at
j ≥ µ(1−β)
least 1− 6m
N10 , for all j Qt
gives us regret η1 − 1
T ∑T
t=1 ∑m
N10 is precisely due to the
fact that with probability 6m
N10 , the above inequality will not be satisﬁed, in which case the regret could be at
most 1 for that epoch. Finally, note that N 10 is arbitrary and can be made as large as required at the expense
of constants. This concludes the proof of Theorem 4.4.

4m , and hence our epochs are of length

ji ≤ 5δ + T m

N10 . The additive term of T m

4m . We let ζ := µ(1−β)

j=1 EhQt−1

µ(1−β)(cid:17)
ln(cid:16) 4m

j Rt

. This

δ2

12

5 Proof of Theorem 4.3

Let us deﬁne the potential function ΦT := ∑m

j=1 W T

j , and recall that eδ = β

1−β. Then,

ΦT

=

=

=

0≤RT

j ≤1 , Fact 4.2

≤

=

0≤RT
j ≤1
≤

m

∑
k=1
µ

m

∑
j=1

W T
j

(1− β)

m

j=1 (1− µ)W T−1

∑

j +

µ
m

j

j

W T−1

k ! eδRT
m(cid:17) eδRT
m(cid:17)(cid:16)1 + (eδ − 1)RT
j(cid:17)
j!
m(cid:17) RT

j +

µ

µ

m

m

∑

∑

j +

(1− β)ΦT−1

j=1(cid:16)(1− µ)PT−1
j=1(cid:16)(1− µ)PT−1
(1− β)ΦT−1
(1− β)ΦT−1 1 + (eδ − 1)
j=1(cid:16)(1− µ)PT−1
(1− β)ΦT−1 1 + µ(eδ − 1) + (1− µ)(eδ − 1)

j +

∑

m

m

∑
j=1

PT−1
j

RT

j! .

Now, we let δ′ := (1−µ)(eδ−1)

1+µδ

and obtain that

ΦT

δ≤eδ−1

≤

1+δ′x≤eδ′x

Φ0=m

≤
≤

(1− β)(1 + µ(eδ − 1))ΦT−1 1 +
(1− β)(1 + µ(eδ − 1))ΦT−1eδ′ ∑m
(1− β)T (1 + µ(eδ − 1))T meδ′ ∑T

(1− µ)(eδ − 1)

1 + µδ

m

∑
j=1

PT−1
j

RT

j!

j=1 PT−1

j

RT
j

t=1 ∑m

j=1 Pt−1

j Rt
j .

On the other hand,

ΦT ≥ (1− β)T (1− µ)T eδ ∑T

t=1 Rt
1.

Combining the lower bound and upper bound and taking logarithms we obtain

Thus,

δ

T

∑
t=1

Rt

1 ≤ ln m + T ln  1 + µ(eδ − 1)
1− µ

! + δ′

T

∑
t=1

m

∑
j=1

Pt−1
j Rt
j.

δ

T

∑
t=1

1 − δ′
Rt

T

∑
t=1

m

∑
j=1

Pt−1
j Rt

j ≤ ln m + T ln  1 + µ(eδ − 1)
1− µ

! .

Now, for µ ≤ 1

2 we know that

1

1−µ ≤ 1 + 2µ. Also, 1 + µ(eδ − 1) ≤ 1 + µ(e− 1). Thus, as µ ≤ 1
2 ,

1 + µ(eδ − 1)

1− µ

≤ (1 + (e− 1)µ)(1 + 2µ) ≤ 1 + (e + 1)µ+ 2(e− 1)µ2 ≤ 1 + 2eµ ≤ 1 + 6µ.

13

Hence, using the inequality ln(1 + x) ≤ x for all x ≥ 0, we obtain
! ≤ 6µ.

ln  1 + µ(eδ − 1)
1− µ

Further, using the fact that eδ − 1 ≤ δ + δ2 for 0 ≤ δ ≤ 1, which is implied by the assumption that β ≤ e
1+e ,

it can be seen that

Therefore,

Note that ∑T

t=1 ∑m

j=1 Pt−1

j Rt

≤ δ(1 + δ).
j! ≤ ln m + 6µT.

(1− µ)δ(1 + δ)

1 + µδ

T

m

≤

δ′ =

∑
t=1

1 + µδ

Rt
1 − (1 + δ)

(1− µ)(eδ − 1)
δ  T
j ≤ T , hence, the above implies that
δ  T

Pt−1
j Rt

Pt−1
j Rt

Rt
1 −

∑
t=1

∑
t=1

∑
t=1

∑
j=1

∑
j=1

m

T

j! ≤ ln m + (δ2 + 6µ)T.

Taking expectations and dividing by Tδ we obtain the following regret bound.

Assuming 6µ ≤ δ2 we obtain

η1 −

1
T ·

T

∑
t=1

m

∑
j=1

j Rt

EhPt−1

ji ≤

ln m
δT

+(cid:18)δ +

6µ

δ (cid:19) .

η1 −

1
T ·

T

∑
t=1

m

∑
j=1

j Rt

EhPt−1

ji ≤

ln m
δT

+ 2δ.

Thus, for T ≥ ln m

δ2 , we obtain the desired bound

Regret∞(T ) ≤ 3δ.

From this we can derive the lower bound on the probability that the best option j = 1 is selected as stated in
the second part of the theorem. Firstly, it follows (as Rt

j is independent of Pt−1

j

) that for all T ≥ 1

Thus,

From this we obtain

η1 1−

and consequently for T ≥ ln m
δ2 ,

This completes the proof.

1

T

T

∑
t=1

1

E(cid:2)Pt−1

η1 −

1
T ·

T

∑
t=1

T

1

∑
t=1

1
E(cid:2)Pt−1
T ·
(η1 − η2) 1−

m

j

∑
j=1

ηj EhPt−1
(cid:3)!−

η2
T ·

∑
t=1

T

1
T ·

T

∑
t=1

ln m
δT

i ≤

+ 2δ.

+ 2δ,

+ 2δ.

m

∑
j=2

ln m
δT

i ≤
EhPt−1
(cid:3)! ≤

j

ln m
δT

3δ

.

η1 − η2

1

E(cid:2)Pt−1
(cid:3) ≥ 1−

14

6 Conclusion and Future Work

In this work we study a fundamental distributed learning dynamics prevalent in various social and biological
contexts and provide the ﬁrst convergence and regret bounds for it in the ﬁnite population setting. The
connection between this learning dynamics and the MWU method suggests a novel distributed and essentially
memoryless implementation of the MWU method. Another interpretation of our result comes by looking at
the inﬁnite population limit of the distributed learning dynamics; while an individual can be eﬀectively
solving a stochastic multi-armed bandit problem, the population as a whole is solving a full-information
version of the problem, and hence can be very eﬃcient on the group-level.

Several important directions remain open. The ﬁrst is to extend our results to the social network setting
where individuals can only sample in step (1) from their neighbors. The question here would be whether,
and to what extent, the eﬃciency of the group remains as a function of the network topology. It would also
be interesting to explore the distributed learning algorithms when the parameters controlling the quality of
the options (ηis) are allowed to change, or when there is dependence across options and time (e.g., when
the options represent stocks). Lastly, we note that as an algorithm designer, if we were to implement these
learning dynamics as a distributed approximation to the stochastic version of MWU method, we can optimize

β to attain the usual O(cid:16)pln m/T(cid:17) regret; in the distributed learning dynamics, we are constrained by the

behavior of the group – the regret bound will only be as good as the β they use. This naturally raises the
question of whether human groups match the ideal values for β, perhaps in a context-speciﬁc manner, to
achieve good regret bounds.

Acknowledgments

The authors would like to thank Ashish Goel for useful discussions, and the BIRS-CMO 2016 Workshop on
Models and Algorithms for Crowds and Networks, where part of this work was done.

References

[1] Yehuda Afek, Noga Alon, Omer Barad, Eran Hornstein, Naama Barkai, and Ziv Bar-Joseph. A biolog-

ical solution to a fundamental distributed computing problem. Science, 331(6014):183–185, 2011.

[2] Animashree Anandkumar, Nithin Michael, Ao Kevin Tang, and Ananthram Swami. Distributed algo-
rithms for learning and cognitive medium access with logarithmic regret. IEEE Journal on Selected
Areas in Communications, 29(4):731–745, 2011.

[3] Dana Angluin, James Aspnes, Zo¨e Diamadi, Michael J. Fischer, and Ren´e Peralta. Computation in

networks of passively mobile ﬁnite-state sensors. Distributed Computing, 18(4):235–253, 2006.

[4] Sanjeev Arora, Elad Hazan, and Satyen Kale. The multiplicative weights update method: a meta-

algorithm and applications. Theory of Computing, 8(1):121–164, 2012.

[5] Abhijit V Banerjee. A simple model of herd behavior. The Quarterly Journal of Economics, pages

797–817, 1992.

15

[6] L. Becchetti, A. Clementi, E. Natale, F. Pasquale, and L. Trevisan. Stabilizing consensus with many
opinions. In Proceedings of the Twenty-seventh Annual ACM-SIAM Symposium on Discrete Algorithms,
SODA ’16, pages 620–635, Philadelphia, PA, USA, 2016. Society for Industrial and Applied Mathe-
matics.

[7] Bret Alexander Beheim, Calvin Thigpen, and Richard McElreath. Strategic social learning and the
population dynamics of human behavior: The game of Go. Evolution and Human Behavior, 35(5):351–
357, 2014.

[8] Michel Bena¨ım, Josef Hofbauer, and Sylvain Sorin. Stochastic approximations and diﬀerential inclu-

sions. SIAM J. Control and Optimization, 44(1):328–348, 2005.

[9] Michel Bena¨ım and J¨orgen W Weibull. Deterministic approximation of stochastic evolution in games.

Econometrica, 71(3):873–903, 2003.

[10] Ginestra Bianconi and Albert-L´aszl´o Barab´asi. Bose-Einstein condensation in complex networks. Phys-

ical Review Letters, 86(24):5632, 2001.

[11] Ken Binmore and Larry Samuelson. Muddling through: Noisy equilibrium selection. journal of eco-

nomic theory, 74(2):235–265, 1997.

[12] Jonas Bj¨ornerstedt and J¨orgen Weibull. Nash equilibrium and evolution by imitation. Technical report,

Research Institute of Industrial Economics, 1994.

[13] Avrim Blum, MohammadTaghi Hajiaghayi, Katrina Ligett, and Aaron Roth. Regret minimization and
the price of total anarchy. In Proceedings of the fortieth annual ACM symposium on Theory of comput-
ing, pages 373–382. ACM, 2008.

[14] Robert Boyd and Peter J Richerson. Culture and the evolutionary process. University of Chicago Press,

1988.

[15] Antonio Cabrales. Stochastic replicator dynamics. International Economic Review, 41(2):451–481,

2000.

[16] Erick Chastain, Adi Livnat, Christos Papadimitriou, and Umesh Vazirani. Algorithms, games, and

evolution. Proceedings of the National Academy of Sciences, 111(29):10620–10623, 2014.

[17] Bernard Chazelle. Natural algorithms and inﬂuence systems. Commun. ACM, 55(12):101–110, De-

cember 2012.

[18] Paul Christiano, Jonathan A. Kelner, Aleksander Madry, Daniel A. Spielman, and Shang-Hua Teng.
Electrical ﬂows, laplacian systems, and faster approximation of maximum ﬂow in undirected graphs.
In Proceedings of the 43rd ACM Symposium on Theory of Computing, STOC 2011, San Jose, CA, USA,
6-8 June 2011, pages 273–282, 2011.

[19] Alejandro Cornejo, Anna Dornhaus, Nancy Lynch, and Radhika Nagpal. Task allocation in ant colonies.

In International Symposium on Distributed Computing, pages 46–60. Springer, 2014.

[20] Devdatt P Dubhashi and Alessandro Panconesi. Concentration of measure for the analysis of random-

ized algorithms. Cambridge University Press, 2009.

16

[21] Peter Duersch, J¨org Oechssler, and Burkhard Schipper. Once beaten, never again: Imitation in two-
player potential games. Technical report, University of California, Davis, Department of Economics,
2011.

[22] Glenn Ellison and Drew Fudenberg. Word-of-mouth communication and social learning. The Quarterly

Journal of Economics, pages 93–125, 1995.

[23] David M Estlund. Opinion leaders, independence, and condorcet’s jury theorem. Theory and Decision,

36(2):131–162, 1994.

[24] Simon Fischer and Berthold V ¨ocking. On the evolution of selﬁsh routing. In European Symposium on

Algorithms, pages 323–334. Springer, 2004.

[25] Yi Gai and Bhaskar Krishnamachari. Decentralized online learning algorithms for opportunistic spec-
trum access. In Global Telecommunications Conference (GLOBECOM 2011), 2011 IEEE, pages 1–6.
IEEE, 2011.

[26] John Gale, Kenneth G Binmore, and Larry Samuelson. Learning to be imperfect: The ultimatum game.

Games and Economic Behavior, 8(1):56–90, 1995.

[27] Mohsen Ghaﬀari, Cameron Musco, Tsvetomira Radeva, and Nancy A. Lynch. Distributed house-
In Proceedings of the 2015 ACM Symposium on Principles of Distributed

hunting in ant colonies.
Computing, PODC 2015, Donostia-San Sebasti´an, Spain, July 21 - 23, 2015, pages 57–66, 2015.

[28] Daniel Golovin, Matthew Faulkner, and Andreas Krause. Online distributed sensor selection. In Pro-
ceedings of the 9th ACM/IEEE International Conference on Information Processing in Sensor Networks,
pages 220–231. ACM, 2010.

[29] Boris Granovskiy, Jason M Gold, David JT Sumpter, and Robert L Goldstone. Integration of social

information by human groups. Topics in Cognitive Science, 7(3):469–493, 2015.

[30] Joseph Henrich. Cultural transmission and the diﬀusion of innovations: Adoption dynamics indicate
that biased cultural transmission is the predominate force in behavioral change. American Anthropolo-
gist, 103(4):992–1013, 2001.

[31] Peter M Kraﬀt, Julia Zheng, Wei Pan, Nicol´as Della Penna, Yaniv Altshuler, Erez Shmueli, Joshua B
Tenenbaum, and Alex Pentland. Human collective intelligence as distributed bayesian inference. arXiv
preprint arXiv:1608.01987, 2016.

[32] Coco Krumme, Manuel Cebrian, Galen Pickard, and Alex Pentland. Quantifying social inﬂuence in an

online cultural market. PLoS ONE, 7(5):e33785, 2012.

[33] Keqin Liu and Qing Zhao. Decentralized multi-armed bandit with multiple distributed players.

In

Information Theory and Applications Workshop (ITA), 2010, pages 1–10. IEEE, 2010.

[34] Richard McElreath, Adrian V Bell, Charles Eﬀerson, Mark Lubell, Peter J Richerson, and Timothy
Waring. Beyond existence and aiming outside the laboratory: Estimating frequency-dependent and
pay-oﬀ-biased social learning strategies. Philosophical Transactions of the Royal Society B: Biological
Sciences, 363(1509):3515–3528, 2008.

17

[35] Cameron Musco, Hsin-Hao Su, and Nancy Lynch. Ant-inspired density estimation via random walks.
In Proceedings of the 2016 ACM Symposium on Principles of Distributed Computing, pages 469–478.
ACM, 2016.

[36] Naumaan Nayyar, Dileep Kalathil, and Rahul Jain. On regret-optimal learning in decentralized multi-

player multi-armed bandits. arXiv preprint arXiv:1505.00553, 2015.

[37] Martin A Nowak. Evolutionary dynamics. Harvard University Press, 2006.

[38] Robin Pemantle. When are touchpoints limits for generalized p´olya urns? Proceedings of the American

Mathematical Society, pages 235–243, 1991.

[39] Alex Pentland. Social Physics: How Good Ideas Spread-The Lessons from a New Science. Penguin,

2014.

[40] Stephen C Pratt, David JT Sumpter, Eamonn B Mallon, and Nigel R Franks. An agent-based model of
collective nest choice by the ant Temnothorax albipennis. Animal Behaviour, 70(5):1023–1036, 2005.

[41] Alan R Rogers. Does biology constrain culture? American Anthropologist, 90(4):819–831, 1988.

[42] Larry Samuelson and Jianbo Zhang. Evolutionary stability in asymmetric games. Journal of economic

theory, 57(2):363–391, 1992.

[43] Thomas D Seeley and Susannah C Buhrman. Group decision making in swarms of honey bees. Behav-

ioral Ecology and Sociobiology, 45(1):19–31, 1999.

[44] Karl Sigmund et al. Evolutionary Game Dynamics: American Mathematical Society Short Course,

January 4-5, 2011, New Orleans, Louisiana, volume 69. American Mathematical Soc., 2011.

[45] John Maynard Smith. Evolution and the Theory of Games. Cambridge university press, 1982.

[46] Damian Straszak and Nisheeth K Vishnoi. IRLS and slime mold: Equivalence and convergence. Invited

at the Innovations in Theoretical Computer Science, 2017.

[47] Hsin-Hao Su. Algorithms for Fundamental Problems in Computer Networks. PhD thesis, ETH Z ¨urich,

2015.

[48] Peter D Taylor. Evolutionarily stable strategies with two types of player. Journal of applied probability,

pages 76–83, 1979.

[49] Nisheeth K Vishnoi. The speed of evolution. In Proc. 26th Annual ACM-SIAM Symp. Discret. Algo-

rithms (SODA), pages 1590–1601, 2015.

[50] Nicholas C Wormald. Diﬀerential equations for random processes and random graphs. The annals of

applied probability, pages 1217–1235, 1995.

18

