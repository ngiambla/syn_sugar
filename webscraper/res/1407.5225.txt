 A recent study demonstrated that emotions are contagious on social media [Kramer et al. 2014]: elusive bots could easily in ltrate a population of unaware humans and manipulate them to affect their perception of reality, with unpredictable results.
 Indirect social and economic effects of social bot activity include the alteration of social media analytics, adopted for various purposes such as TV ratings,3 expert nding [Wu et al. 2013], and scienti c impact measurement.4 Act like a human, think like a bot One of the greatest challenges for bot detection in social media is in understanding what modern social bots can do [Boshmaf et al. 2012].
 A taxonomy of social bot detection systems For all the reasons outlined above, the computing community is engaging in the design of advanced methods to automatically detect social bots, or to discriminate between humans and bots.
 The strategies currently employed by social media services appear inadequate to contrast this phenomenon and the efforts of the academic community in this direction just started.
 In the following, we propose a simple taxonomy that divides the approaches pro- posed in literature into three classes: (i) bot detection systems based on social network information; (ii) system based on crowd-sourcing and leveraging human intelligence; (iii) machine learning methods based on the identi cation of highly-revealing features that discriminate between bots and humans.
 Graph-based social bot detection The challenge of social bot detection has been framed by various teams in an adver- sarial setting [Alvisi et al. 2013].
 One example of this framework is represented by the Facebook Immune System [Stein et al. 2011]: an adversary may control multiple social bots (often referred to as Sybils in this context) to impersonate different identi- ties and launch an attack or in ltration.
 In some ways, the Renren approach [Wang et al. 2013a; Yang et al. 2014] combines the best of network- and behavior-based con- ceptualizations of Sybil detection.
 Some researchers [Freitas et al. 2014], for example, reverse-engineer social bots reporting alarming results: simple automated mechanisms that produce contents and boost fol- lowers yield successful in ltration strategies and increase the social in uence of the bots.
 Others teams are creating bots themselves: Tim Hwang s [Hwang et al. 2012] and Sune Lehmann s9 groups continuously challenge our understanding of what strate- gies effective bots employ, and help quantify the susceptibility of people to their in u- ence [Wagner et al. 2012; Wald et al. 2013].
