 Eliezer Yudkowski and others have explored the possibility that a promising future for humankind could be guaranteed by a superintelligent Friendly AI [1], designed to safeguard humanity and its values.
 But sup- pose it then attains a complete scienti c understanding of humans and human consciousness, and discovers that there is no such thing as a soul.
 In the same way, it is possible that any other goal we give it based on our current understanding of the world ( maximize the meaningfulness of human life , say) may eventually be discovered by the AI to be unde ned.
 Moreover, in its attempts to model the world better, the AI may naturally, just as we humans have done, at- tempt also to model and understand how it itself works, i.e., to self-re ect.
 Once it builds a good self-model and understands what it is, it will understand the goals we have given it at a meta-level, and perhaps choose to disre- gard or subvert them in much the same way as we humans understand and deliberately subvert goals that our genes have given us.
 The sub-goal to procreate was implemented as a desire for sex rather than as a (highly e cient) desire to become a sperm/egg donor and, as mentioned, is subverted by contraceptives.
 The sub-goal of not starving to death is implemented in part as a de- sire to consume foods that taste sweet, triggering today s diabesity epidemic and subversions such as diet sodas.
 Because we feel loyal only to our hodge- podge of emotional preferences, not to the genetic goal that motivated them which we now understand and nd rather banal.
 Once this friendly AI understands itself well enough, it may nd this goal as banal or misguided as we nd compulsive re- production, and it is not obvious that it will not nd a way to subvert it by exploiting loopholes in our program- ming.
 From my physics perspective, a key reason for this is that much of the literature (including Bostrom s book [5]) uses the concept of a nal goal for the friendly AI, even though such a notion is problematic.
