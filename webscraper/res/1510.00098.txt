 We overcome this lack of training data by using a se- quence of transfer learning steps and a convolutional neu- ral network model.
 The idea is to leverage available datasets such as ImageNet to extract features and high-level repre- sentations that are useful for the task of interest, i.e., ex- tracting socioeconomic data for poverty mapping.
 For target problems with abundant data, we can transfer low-level features, such as edges and corners, and learn new high-level features spe- ci c to the target problem.
 Our goal is to transfer knowledge from the ImageNet object recognition challenge (P1) to the target problem of predicting nighttime light intensity from daytime satellite imagery (P2).
 In P1, we have an object classi cation problem with source domain data D1 = {(x1i, y1i)} from ImageNet that consists of natural images x1i X1 and object class labels.
 In P2, we have a nighttime light intensity prediction prob- lem with target domain data D2 = {(x2i, y2i}) that consists of daytime satellite images x2i X2 and nighttime light intensity labels.
 The target domain data D3 = {(x3i, y3i)} consists of satellite images x3i X3 from the feature space of satellite images of Uganda and a limited number of poverty labels y3i Y3, detailed below.
 The VGG model parameters are obtained from the Caffe Model Zoo, and all networks are trained with Caffe (Jia et al. 2014).
 The fully convolutional model is ne-tuned from the pre-trained parameters of the VGG F model, but it randomly initializes the convolutional layers that replace fully connected layers.
 Figure 4: A set of 25 maximally activating images and their corresponding activation maps for a lter in the fth convolutional layer of the network trained on the 3-class nighttime light intensity prediction task.
