 Not all crashes will be avoided, though, and some crashes will require AVs to make dif cult ethical decisions, in cases which involve unavoidable harm [7].
 The utilitarian course of action, in that situation, would be for the AV to swerve and kill its passenger but AVs programmed to follow this course of action might discourage buyers who believe their own safety should trump other considerations.
 Even though such situations may be exceedingly rare, their emotional saliency is likely to give them broad public exposure and a disproportionate weight in individual and public decisions about AVs.
 Overall, participants strongly agreed that it would be more moral for AVs to sacri ce their own passengers, when this sacri ce would save a greater number of lives overall.
 In Study 1 (n = 182), 76% of participants thought that it would be more moral for AVs to sacri ce one passenger, rather than kill ten pedestrians (with a 95% con dence interval of 69 82).
 In Study 2 (n = 451), participants were presented with dilemmas that varied the number of pedestrians lives that could be saved, from 1 to 100.
 Just as the high-valued algorithm, it received high marks for morality (median budget share = 50), and was considered a good algorithm for other people to have (median budget share = 50).
 Participants considered scenarios in which either a human driver or a control algorithm had an opportunity to self-sacri ce in order to save 1 or 10 pedestrians (Fig 3c).
 If a manufacturer offers different versions of its moral algorithm, and a buyer knowingly chose one of them, is the buyer to blame for the harmful consequences of the algorithm s decisions?
 The car must decide between (a) killing several pedestrians or one passer by, (b) killing one pedestrian or killing its own passenger, (c) killing several pedestrians or killing its own passenger.
 In studies 1 and 2, when asked which would be the most moral way to program AVs, participants expressed a preference for AVs programmed to kill their passenger for the greater good.
 On average, participants were more con dent that AVs should pursue the greater good than they were con dent that AVs would be programmed to do so.
 This moral preference was robust to situations in which participants imagined themselves in the AV in the company of a coworker, a family member, or their own child.
