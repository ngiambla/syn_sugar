 The utilitarian course of action, in that situation, would be for the AV to swerve and kill its passenger but AVs programmed to follow this course of action might discourage buyers who believe their own safety should trump other considerations.
 Even though such situations may be exceedingly rare, their emotional saliency is likely to give them broad public exposure and a disproportionate weight in individual and public decisions about AVs.
 To align moral algorithms with human values, we must start a collective discussion about the ethics of AVs, that is, the moral algorithms that we are willing to accept as citizens and to be subjected to as car owners.
 In Study 2 (n = 451), participants were presented with dilemmas that varied the number of pedestrians lives that could be saved, from 1 to 100.
 Participants did not think that AVs should sacri ce their passenger when only one pedestrian could be saved (with an average approval rate of 23%), but their moral approval increased with the number of lives that could be saved (p < 0.001) up to approval rates consistent with the 76% observed in Study 1 (Fig. 2b),.
 Participants approval of passenger sacri ce was even robust to treatments in which they had to imagine themselves and another person in the AV, a family member in particular (Study 3, n = 259).
 Just as the high-valued algorithm, it received high marks for morality (median budget share = 50), and was considered a good algorithm for other people to have (median budget share = 50).
 Once more, it appears that people praise utilitarian, self-sacri cing AVs, and welcome them on the road, without actually wanting to buy one for themselves.
 If a manufacturer offers different versions of its moral algorithm, and a buyer knowingly chose one of them, is the buyer to blame for the harmful consequences of the algorithm s decisions?
 In studies 1 and 2, when asked which would be the most moral way to program AVs, participants expressed a preference for AVs programmed to kill their passenger for the greater good.
