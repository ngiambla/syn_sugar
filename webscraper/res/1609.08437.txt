 Enhancing Big Data in the Social Sciences with Crowdsourcing 2 what we can do with and get out of the data (value), are often lacking in big data research (Monroe 2013; Yin and Kaynak 2015).
 Characteristic of social science skepticism around big data are concerns that â€œ[t]he reliability, statistical validity and generalizability of new forms of data are not well understood.
 Without clear approaches to quantify and increase the validity and value of big data, we believe social science skepticism of big data will remain high.
 Sentiment analysis, in this case, serves as an automated way to gain additional information about big data (the posts), augmenting its value for research purposes.
 Finally, in light of the inconsistent and frequently incomplete reporting of online crowdsourcing procedures, we provide a recommended reporting template for online crowdsourcing as an academic data augmentation platform.
 In general, the primary means of assessing and increasing the validity and value of data in the social sciences is undertaken through what we refer to as data augmentation.
 Our goal is to develop intuition for the benefits of big data augmentation through online crowdsourcing and how researchers can best move forward with such projects.
 In all, we used MTurk data augmentation to check 2,043 automated classifications of faculty member fields, at a total cost of $590 including fees and pilot costs.
 Our content analysis of published social science papers that use MTurk indicated that such evaluations have generated a set of informal norms around design and reporting for experimental and survey-style MTurk studies.
 We argued that online crowdsourcing as a data augmentation platform holds unique potential to add validity and value to big data at low cost, and our content analysis suggests that researchers are beginning to use it for these purposes.
 In this section, we consider the implications of both the content analysis and our three case studies in the context of past recommendations about online crowdsourcing.
 Strengths and Limitations of Using Online Crowdsourcing for Data Augmentation Our three case studies test whether and when online crowdsourcing is practical for adding validity and value to big data projects.
 The variety of big data, their relative lack of structure, and the priority of computer science and engineering over the social sciences in the field have contributed to inconsistent reporting.
 We provide a recommended reporting template in the appendix with both standard items that should be included in reporting all online crowdsourcing studies and items to use in reporting specifically for big data augmentation.
 Enhancing Big Data in the Social Sciences with Crowdsourcing 30 guidance and best practices for academic research that uses online crowdsourcing for data augmentation and a standardized reporting framework.
