 L’estimation des coefﬁcient de pondération, dite codage par- cimonieux (sparse coding en anglais), est un problème non convexe et NP-difﬁcile à cause de la contrainte de type ℓ0 pour imposer la parcimonie.
 Le présent article vise à résoudre d’une manière exacte le problème de l’apprentissage de dictionnaire, c’est à dire en considérant la norme ℓ0 sans aucune relaxation.
 La section 3 montre la per- tinence de la méthode proposée en débruitage d’image, et la dernière section conclut l’article. 2 Représentation parcimonieuse 2.2 Optimisation globale du problème 2.1 Énoncé du problème Soit Y = [y1, . . . , yℓ] ∈ Rn×ℓ, une matrice contenant ℓ signaux yi, i = 1, . . . , ℓ, de dimension n.
 Une manière classique d’aborder ce problème (1) d’estimation jointe de D et X consiste à utiliser une procédure de relaxation alternée en deux phases [1].
 L’idée est d’associer à tous les éléments xi du vecteur x, une variable binaire zi égale à 0 si xi = 0 et égale à un sinon.
 Supposons que l’on connaisse un réel M > 0 suf- ﬁsamment grand, de sorte que, si x⋆ est solution du problème (2), alors kx⋆k∞ < M .
 Cette solution peut être utilisée comme une bonne initialisation des variables à optimiser et du para- mètre M , permettant aux solveurs d’accéder plus rapidement au minimum globale du problème [2].
 Elle est construite à partir de l’opé- rateur proximal associé à la contrainte kxk0 6 T : proxT : Rp −→ Rp x 7−→ proxT (x) = arg min kuk06T 1 2 ku − xk2.
 L’algorithme de descente de gradient proximal consiste alors à mettre en œuvre, pour un pas ρ, les itérations suivantes [3] : xk+1 ∈ proxT(cid:0)xk − ρDT (Dxk − y)(cid:1).
 Soulignons que, comme pour la plupart des méthodes itéra- tives, il existe de nombreuses variantes de l’algorithme permet- tant d’accélérer la convergence.
 Connaissant x⋆ le point de convergence de l’algorithme proximal, il est possible d’en déduire une initialisation pour le vecteur z et le paramètre M .
 Par exemple, pour un ε > 0 donné, on peut initialiser z avec zj = 0 si |x⋆ j | 6 ε et zj = 1 sinon.
 La constante M peut être choisie telle que M = (1 + α)kx⋆k∞ avec α > 0 choisi le plus petit possible.
 Nous avons construit la matrice Y des données d’appren- tissage à l’aide des cinq images simultanément en utilisant, comme dans la littérature [6], des imagettes de taille 8 × 8 se chevauchant.
 Nous avons ﬁxé expérimentalement le nombre d’atomes du dictionnaire à p = 100 et le coefﬁcient de parci- monie à T = 20.
 Nous avons testé différents niveaux de bruit additif gaussien sur les images en utilisant trois différents ni- veaux de bruit avec des valeurs d’écart type σ = 10, 20 et 50.
 Pour la phase de codage parcimonieux, nous avons comparé notre approche MIQP avec les deux méthodes références de la littérature, K-SVD [6] et la méthode proximale [3], toutes choses égales par ailleurs.
 Nous avons aussi ﬁxé α = 1,5 pour des raisons de stabilité. 1. http://sipi.usc.edu/database/database.php?volume=misc La TABLE 1 résume nos résultats.
 Cependant, pour un faible niveau de bruit (σ = 10), notre méthode ne réussit pas à améliorer les résultats de K-SVD alors que, pour un niveau intermédiaire (σ = 20) les résultats sont plus contrastés et dépendent de l’image considérée.
 Pour arriver à ce résultat, nous avons proposé deux techniques d’accélération du traitement des MIQP, la reformulation des contraintes pour mieux structurer le problème, et l’initialisation efﬁcace de la procédure grâce à un algorithme proximal.
