we develop results for the use of lasso and post-lasso methods to formfirst-stage predictions and estimate optimal instruments in linear instrumentalvariables (iv) models with many instruments, $p$. our results apply even when$p$ is much larger than the sample size, $n$. we show that the iv estimatorbased on using lasso or post-lasso in the first stage is root-n consistent andasymptotically normal when the first-stage is approximately sparse; i.e. whenthe conditional expectation of the endogenous variables given the instrumentscan be well-approximated by a relatively small set of variables whoseidentities may be unknown. we also show the estimator is semi-parametricallyefficient when the structural error is homoscedastic. notably our results allowfor imperfect model selection, and do not rely upon the unrealistic "beta-min"conditions that are widely used to establish validity of inference followingmodel selection. in simulation experiments, the lasso-based iv estimator with adata-driven penalty performs well compared to recently advocatedmany-instrument-robust procedures. in an empirical example dealing with theeffect of judicial eminent domain decisions on economic outcomes, thelasso-based iv estimator outperforms an intuitive benchmark.  in developing the iv results, we establish a series of new results for lassoand post-lasso estimators of nonparametric conditional expectation functionswhich are of independent theoretical and practical interest. we construct amodification of lasso designed to deal with non-gaussian, heteroscedasticdisturbances which uses a data-weighted $\ell_1$-penalty function. usingmoderate deviation theory for self-normalized sums, we provide convergencerates for the resulting lasso and post-lasso estimators that are as sharp asthe corresponding rates in the homoscedastic gaussian case under the conditionthat $\log p = o(n^{1/3})$.