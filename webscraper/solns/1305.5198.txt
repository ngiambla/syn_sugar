statistical and machine learning theory has developed several conditionsensuring that popular estimators such as the lasso or the dantzig selectorperform well in high-dimensional sparse regression, including the restrictedeigenvalue, compatibility, and $\ell_q$ sensitivity properties. however, someof the central aspects of these conditions are not well understood. forinstance, it is unknown if these conditions can be checked efficiently on anygiven data set. this is problematic, because they are at the core of the theoryof sparse regression.  here we provide a rigorous proof that these conditions are np-hard to check.this shows that the conditions are computationally infeasible to verify, andraises some questions about their practical applications.  however, by taking an average-case perspective instead of the worst-case viewof np-hardness, we show that a particular condition, $\ell_q$ sensitivity, hascertain desirable properties. this condition is weaker and more general thanthe others. we show that it holds with high probability in models where theparent population is well behaved, and that it is robust to certain dataprocessing steps. these results are desirable, as they provide guidance aboutwhen the condition, and more generally the theory of sparse regression, may berelevant in the analysis of high-dimensional correlated observational data.