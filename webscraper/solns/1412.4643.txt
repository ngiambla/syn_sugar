  when we use machine learning for public policy, we find that many useful
variables are associated with others on which it would be ethically problematic
to base decisions. this problem becomes particularly acute in the big data era,
when predictions are often made in the absence of strong theories for
underlying causal mechanisms. we describe the dangers to democratic
decision-making when high-performance algorithms fail to provide an explicit
account of causation. we then demonstrate how information theory allows us to
degrade predictions so that they decorrelate from protected variables with
minimal loss of accuracy. enforcing total decorrelation is at best a near-term
solution, however. the role of causal argument in ethical debate urges the
development of new, interpretable machine-learning algorithms that reference
causal mechanisms.
