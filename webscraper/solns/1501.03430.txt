here we present an expository, general analysis of valid post-selection orpost-regularization inference about a low-dimensional target parameter,$\alpha$, in the presence of a very high-dimensional nuisance parameter,$\eta$, which is estimated using modern selection or regularization methods.our analysis relies on high-level, easy-to-interpret conditions that allow oneto clearly see the structures needed for achieving valid post-regularizationinference. simple, readily verifiable sufficient conditions are provided for aclass of affine-quadratic models. we focus our discussion on estimation andinference procedures based on using the empirical analog of theoreticalequations $$m(\alpha, \eta)=0$$ which identify $\alpha$. within this structure,we show that setting up such equations in a manner such that theorthogonality/immunization condition $$\partial_\eta m(\alpha, \eta) = 0$$ atthe true parameter values is satisfied, coupled with plausible conditions onthe smoothness of $m$ and the quality of the estimator $\hat \eta$, guaranteesthat inference on for the main parameter $\alpha$ based on testing or pointestimation methods discussed below will be regular despite selection orregularization biases occurring in estimation of $\eta$. in particular, theestimator of $\alpha$ will often be uniformly consistent at the root-$n$ rateand uniformly asymptotically normal even though estimators $\hat \eta$ willgenerally not be asymptotically linear and regular. the uniformity holds overlarge classes of models that do not impose highly implausible "beta-min"conditions. we also show that inference can be carried out by inverting testsformed from neyman's $c(\alpha)$ (orthogonal score) statistics.