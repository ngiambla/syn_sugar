  the reproduction and replication of research results has become a major issue
for a number of scientific disciplines. in computer science and related
computational disciplines such as systems biology, the challenges closely
revolve around the ability to implement (and exploit) novel algorithms and
models. taking a new approach from the literature and applying it to a new
codebase frequently requires local knowledge missing from the published
manuscripts and transient project websites. alongside this issue, benchmarking,
and the lack of open, transparent and fair benchmark sets present another
barrier to the verification and validation of claimed results.
  in this paper, we outline several recommendations to address these issues,
driven by specific examples from a range of scientific domains. based on these
recommendations, we propose a high-level prototype open automated platform for
scientific software development which effectively abstracts specific
dependencies from the individual researcher and their workstation, allowing
easy sharing and reproduction of results. this new e-infrastructure for
reproducible computational science offers the potential to incentivise a
culture change and drive the adoption of new techniques to improve the quality
and efficiency -- and thus reproducibility -- of scientific exploration.
