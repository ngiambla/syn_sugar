we study a family of regularized score-based estimators for learning thestructure of a directed acyclic graph (dag) for a multivariate normaldistribution from high-dimensional data with $p\gg n$. our main resultsestablish support recovery guarantees and deviation bounds for a family ofpenalized least-squares estimators under concave regularization withoutassuming prior knowledge of a variable ordering. these results apply to avariety of practical situations that allow for arbitrary nondegeneratecovariance structures as well as many popular regularizers including the mcp,scad, $\ell_{0}$ and $\ell_{1}$. the proof relies on interpreting a dag as arecursive linear structural equation model, which reduces the estimationproblem to a series of neighbourhood regressions. we provide a novelstatistical analysis of these neighbourhood problems, establishing uniformcontrol over the superexponential family of neighbourhoods associated with agaussian distribution. we then apply these results to study the statisticalproperties of score-based dag estimators, learning causal dags, and inferringconditional independence relations via graphical models. our resultsyield---for the first time---finite-sample guarantees for structure learning ofgaussian dags in high-dimensions via score-based estimation.