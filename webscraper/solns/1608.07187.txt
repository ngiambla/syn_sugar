artificial intelligence and machine learning are in a period of astoundinggrowth. however, there are concerns that these technologies may be used, eitherwith or without intention, to perpetuate the prejudice and unfairness thatunfortunately characterizes many human institutions. here we show for the firsttime that human-like semantic biases result from the application of standardmachine learning to ordinary language---the same sort of language humans areexposed to every day. we replicate a spectrum of standard human biases asexposed by the implicit association test and other well-known psychologicalstudies. we replicate these using a widely used, purely statisticalmachine-learning model---namely, the glove word embedding---trained on a corpusof text from the web. our results indicate that language itself containsrecoverable and accurate imprints of our historic biases, whether these aremorally neutral as towards insects or flowers, problematic as towards race orgender, or even simply veridical, reflecting the {\em status quo} for thedistribution of gender with respect to careers or first names. theseregularities are captured by machine learning along with the rest of semantics.in addition to our empirical findings concerning language, we also contributenew methods for evaluating bias in text, the word embedding association test(weat) and the word embedding factual association test (wefat). our resultshave implications not only for ai and machine learning, but also for the fieldsof psychology, sociology, and human ethics, since they raise the possibilitythat mere exposure to everyday language can account for the biases we replicatehere.