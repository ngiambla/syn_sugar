  artificial intelligence and machine learning are in a period of astounding
growth. however, there are concerns that these technologies may be used, either
with or without intention, to perpetuate the prejudice and unfairness that
unfortunately characterizes many human institutions. here we show for the first
time that human-like semantic biases result from the application of standard
machine learning to ordinary language---the same sort of language humans are
exposed to every day. we replicate a spectrum of standard human biases as
exposed by the implicit association test and other well-known psychological
studies. we replicate these using a widely used, purely statistical
machine-learning model---namely, the glove word embedding---trained on a corpus
of text from the web. our results indicate that language itself contains
recoverable and accurate imprints of our historic biases, whether these are
morally neutral as towards insects or flowers, problematic as towards race or
gender, or even simply veridical, reflecting the {\em status quo} for the
distribution of gender with respect to careers or first names. these
regularities are captured by machine learning along with the rest of semantics.
in addition to our empirical findings concerning language, we also contribute
new methods for evaluating bias in text, the word embedding association test
(weat) and the word embedding factual association test (wefat). our results
have implications not only for ai and machine learning, but also for the fields
of psychology, sociology, and human ethics, since they raise the possibility
that mere exposure to everyday language can account for the biases we replicate
here.
