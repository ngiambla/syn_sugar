  the importance of big data is a contested topic among social scientists.
proponents claim it will fuel a research revolution, but skeptics challenge it
as unreliably measured and decontextualized, with limited utility for
accurately answering social science research questions. we argue that social
scientists need effective tools to quantify big data's measurement error and
expand the contextual information associated with it. standard research efforts
in many fields already pursue these goals through data augmentation, the
systematic assessment of measurement against known quantities and expansion of
extant data by adding new information. traditionally, these tasks are
accomplished using trained research assistants or specialized algorithms.
however, such approaches may not be scalable to big data or appease its
skeptics. we consider a third alternative that may increase the validity and
value of big data: data augmentation with online crowdsourcing. we present
three empirical cases to illustrate the strengths and limits of crowdsourcing
for academic research, with a particular eye to how they can be applied to data
augmentation tasks that will accelerate acceptance of big data among social
scientists. the cases use amazon mechanical turk to 1. verify automated coding
of the academic discipline of dissertation committee members, 2. link online
product pages to a book database, and 3. gather data on mental health resources
at colleges. in light of these cases, we consider the costs and benefits of
augmenting big data with crowdsourcing marketplaces and provide guidelines on
best practices. we also offer a standardized reporting template that will
enhance reproducibility.
