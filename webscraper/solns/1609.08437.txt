the importance of big data is a contested topic among social scientists.proponents claim it will fuel a research revolution, but skeptics challenge itas unreliably measured and decontextualized, with limited utility foraccurately answering social science research questions. we argue that socialscientists need effective tools to quantify big data's measurement error andexpand the contextual information associated with it. standard research effortsin many fields already pursue these goals through data augmentation, thesystematic assessment of measurement against known quantities and expansion ofextant data by adding new information. traditionally, these tasks areaccomplished using trained research assistants or specialized algorithms.however, such approaches may not be scalable to big data or appease itsskeptics. we consider a third alternative that may increase the validity andvalue of big data: data augmentation with online crowdsourcing. we presentthree empirical cases to illustrate the strengths and limits of crowdsourcingfor academic research, with a particular eye to how they can be applied to dataaugmentation tasks that will accelerate acceptance of big data among socialscientists. the cases use amazon mechanical turk to 1. verify automated codingof the academic discipline of dissertation committee members, 2. link onlineproduct pages to a book database, and 3. gather data on mental health resourcesat colleges. in light of these cases, we consider the costs and benefits ofaugmenting big data with crowdsourcing marketplaces and provide guidelines onbest practices. we also offer a standardized reporting template that willenhance reproducibility.