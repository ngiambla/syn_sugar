we show through case studies that it is easier to estimate the fundamentallimits of data processing than to construct explicit algorithms to achievethose limits. focusing on binary classification, data compression, andprediction under logarithmic loss, we show that in the finite space setting,when it is possible to construct an estimator of the limits with vanishingerror with $n$ samples, it may require at least $n\ln n$ samples to constructan explicit algorithm to achieve the limits.