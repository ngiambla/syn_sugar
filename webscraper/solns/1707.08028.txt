machine learning problems such as neural network training, tensordecomposition, and matrix factorization, require local minimization of anonconvex function. this local minimization is challenged by the presence ofsaddle points, of which there can be many and from which descent methods maytake inordinately large number of iterations to escape. this paper presents asecond-order method that modifies the update of newton's method by replacingthe negative eigenvalues of the hessian by their absolute values and uses atruncated version of the resulting matrix to account for the objective'scurvature. the method is shown to escape saddles in at most $1 + \log_{3/2}(\delta/2\varepsilon)$ iterations where $\varepsilon$ is the target optimalityand $\delta$ characterizes a point sufficiently far away from the saddle. thisbase of this exponential escape is $3/2$ independently of problem constants.adding classical properties of newton's method, the paper proves convergence toa local minimum with probability $1-p$ in $o\left(\log(1/p)) +o(\log(1/\varepsilon)\right)$ iterations.