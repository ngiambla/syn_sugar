using solely the information retrieved by audio fingerprinting techniques, wepropose methods to treat a possibly large dataset of user-generated audiocontent, that (1) enable the grouping of several audio files that contain acommon audio excerpt (i.e., are relative to the same event), and (2) giveinformation about how those files are correlated in terms of time and qualityinside each event. furthermore, we use supervised learning to detect incorrectmatches that may arise from the audio fingerprinting algorithm itself, whilstensuring our model learns with previous predictions. all the presented methodswere further validated by user-generated recordings of several differentconcerts manually crawled from youtube.