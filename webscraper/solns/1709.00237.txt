in this paper a spectrum sensing policy employing recency-based explorationis proposed for cognitive radio networks. we formulate the problem of finding aspectrum sensing policy for multi-band dynamic spectrum access as a stochasticrestless multi-armed bandit problem with stationary unknown rewarddistributions. in cognitive radio networks the multi-armed bandit problemarises when deciding where in the radio spectrum to look for idle frequenciesthat could be efficiently exploited for data transmission. we consider twomodels for the dynamics of the frequency bands: 1) the independent model wherethe state of the band evolves randomly independently from the past and 2) thegilbert-elliot model, where the states evolve according to a 2-state markovchain. it is shown that in these conditions the proposed sensing policy attainsasymptotically logarithmic weak regret. the policy proposed in this paper is anindex policy, in which the index of a frequency band is comprised of a samplemean term and a recency-based exploration bonus term. the sample mean promotesspectrum exploitation whereas the exploration bonus encourages for furtherexploration for idle bands providing high data rates. the proposed recencybased approach readily allows constructing the exploration bonus such that itwill grow the time interval between consecutive sensing time instants of asuboptimal band exponentially, which then leads to logarithmically increasingweak regret. simulation results confirming logarithmic weak regret arepresented and it is found that the proposed policy provides often improvedperformance at low complexity over other state-of-the-art policies in theliterature.