methods for distributed optimization have received significant attention inrecent years owing to their wide applicability in various domains. adistributed optimization method typically consists of two key components:communication and computation. more specifically, at every iteration (or everyseveral iterations) of a distributed algorithm, each node in the networkrequires some form of information exchange with its neighboring nodes(communication) and the computation step related to a (sub)-gradient(computation). the standard way of judging an algorithm via only the number ofiterations overlooks the complexity associated with each iteration. moreover,various applications deploying distributed methods may prefer a differentcomposition of communication and computation.  motivated by this discrepancy, in this work we propose an adaptive costframework which adjusts the cost measure depending on the features of variousapplications. we present a flexible algorithmic framework, where communicationand computation steps are explicitly decomposed to enable algorithmcustomization for various applications. we apply this framework to thewell-known distributed gradient descent (dgd) method, and show that theresulting customized algorithms, which we call dgd$^t$, near-dgd$^t$ andnear-dgd$^+$, compare favorably to their base algorithms, both theoreticallyand empirically. the proposed near-dgd$^+$ algorithm is an exact first-ordermethod where the communication and computation steps are nested, and when thenumber of communication steps is adaptively increased, the method converges tothe optimal solution. we test the performance and illustrate the flexibility ofthe methods, as well as practical variants, on quadratic functions andclassification problems that arise in machine learning, in terms of iterations,gradient evaluations, communications and the proposed cost framework.