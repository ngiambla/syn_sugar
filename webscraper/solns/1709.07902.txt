we present a factorized hierarchical variational autoencoder, which learnsdisentangled and interpretable representations from sequential data withoutsupervision. specifically, we exploit the multi-scale nature of information insequential data by formulating it explicitly within a factorized hierarchicalgraphical model that imposes sequence-dependent priors and sequence-independentpriors to different sets of latent variables. the model is evaluated on twospeech corpora to demonstrate, qualitatively, its ability to transform speakersor linguistic content by manipulating different sets of latent variables; andquantitatively, its ability to outperform an i-vector baseline for speakerverification and reduce the word error rate by as much as 35% in mismatchedtrain/test scenarios for automatic speech recognition tasks.