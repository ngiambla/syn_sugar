a method for statistical parametric speech synthesis incorporating generativeadversarial networks (gans) is proposed. although powerful deep neural networks(dnns) techniques can be applied to artificially synthesize speech waveform,the synthetic speech quality is low compared with that of natural speech. oneof the issues causing the quality degradation is an over-smoothing effect oftenobserved in the generated speech parameters. a gan introduced in this paperconsists of two neural networks: a discriminator to distinguish natural andgenerated samples, and a generator to deceive the discriminator. in theproposed framework incorporating the gans, the discriminator is trained todistinguish natural and generated speech parameters, while the acoustic modelsare trained to minimize the weighted sum of the conventional minimum generationloss and an adversarial loss for deceiving the discriminator. since theobjective of the gans is to minimize the divergence (i.e., distributiondifference) between the natural and generated speech parameters, the proposedmethod effectively alleviates the over-smoothing effect on the generated speechparameters. we evaluated the effectiveness for text-to-speech and voiceconversion, and found that the proposed method can generate more naturalspectral parameters and $f_0$ than conventional minimum generation errortraining algorithm regardless its hyper-parameter settings. furthermore, weinvestigated the effect of the divergence of various gans, and found that awasserstein gan minimizing the earth-mover's distance works the best in termsof improving synthetic speech quality.