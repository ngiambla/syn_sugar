principal components regression (pcr) is a traditional tool for dimensionreduction in linear regression that has been both criticized and defended. oneconcern about pcr is that obtaining the leading principal components tends tobe computationally demanding for large data sets. while random projections donot possess the optimality properties of the leading principal subspace, theyare computationally appealing and hence have become increasingly popular inrecent years. in this paper, we present an analysis showing that for randomprojections satisfying a johnson-lindenstrauss embedding property, theprediction error in subsequent regression is close to that of pcr, at theexpense of requiring a slightly large number of random projections thanprincipal components. column sub-sampling constitutes an even cheaper way ofrandomized dimension reduction outside the class of johnson-lindenstrausstransforms. we provide numerical results based on synthetic and real data aswell as basic theory revealing differences and commonalities in terms ofstatistical performance.