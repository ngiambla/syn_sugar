inferring information from a set of acquired data is the main objective ofany signal processing (sp) method. in particular, the common problem ofestimating the value of a vector of parameters from a set of noisy measurementsis at the core of a plethora of scientific and technological advances in thelast decades; for example, wireless communications, radar and sonar,biomedicine, image processing, and seismology, just to name a few. developingan estimation algorithm often begins by assuming a statistical model for themeasured data, i.e. a probability density function (pdf) which if correct,fully characterizes the behaviour of the collected data/measurements.experience with real data, however, often exposes the limitations of anyassumed data model since modelling errors at some level are always present.consequently, the true data model and the model assumed to derive theestimation algorithm could differ. when this happens, the model is said to bemismatched or misspecified. therefore, understanding the possible performanceloss or regret that an estimation algorithm could experience under modelmisspecification is of crucial importance for any sp practitioner. further,understanding the limits on the performance of any estimator subject to modelmisspecification is of practical interest. motivated by the widespread andpractical need to assess the performance of a mismatched estimator, the goal ofthis paper is to help to bring attention to the main theoretical findings onestimation theory, and in particular on lower bounds under modelmisspecification, that have been published in the statistical and econometricalliterature in the last fifty years. secondly, some applications are discussedto illustrate the broad range of areas and problems to which this frameworkextends, and consequently the numerous opportunities available for spresearchers.