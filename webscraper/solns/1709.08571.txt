the hessian-vector product has been utilized to find a second-orderstationary solution with strong complexity guarantee (e.g., almost linear timecomplexity in the problem's dimensionality). in this paper, we propose tofurther reduce the number of hessian-vector products for faster non-convexoptimization. previous algorithms need to approximate the smallest eigen-valuewith a sufficient precision (e.g., $\epsilon_2\ll 1$) in order to achieve asufficiently accurate second-order stationary solution (i.e.,$\lambda_{\min}(\nabla^2 f(\x))\geq -\epsilon_2)$. in contrast, the proposedalgorithms only need to compute the smallest eigen-vector approximating thecorresponding eigen-value up to a small power of current gradient's norm. as aresult, it can dramatically reduce the number of hessian-vector products duringthe course of optimization before reaching first-order stationary points (e.g.,saddle points). the key building block of the proposed algorithms is a novelupdating step named the ncg step, which lets a noisy negative curvature descentcompete with the gradient descent. we show that the worst-case time complexityof the proposed algorithms with their favorable prescribed accuracyrequirements can match the best in literature for achieving a second-orderstationary point but with an arguably smaller per-iteration cost. we also showthat the proposed algorithms can benefit from inexact hessian by developingtheir variants accepting inexact hessian under a mild condition for achievingthe same goal. moreover, we develop a stochastic algorithm for a finite orinfinite sum non-convex optimization problem. to the best of our knowledge, theproposed stochastic algorithm is the first one that converges to a second-orderstationary point in {\it high probability} with a time complexity independentof the sample size and almost linear in dimensionality.