we propose a theoretical framework for thinking about score normalization,which confirms that normalization is not needed under (admittedly fragile)ideal conditions. if, however, these conditions are not met, e.g. underdata-set shift between training and runtime, our theory reveals dependenciesbetween scores that could be exploited by strategies such as scorenormalization. indeed, it has been demonstrated over and over experimentally,that various ad-hoc score normalization recipes do work. we present a firstattempt at using probability theory to design a generative score-spacenormalization model which gives similar improvements to zt-norm on thetext-dependent rsr 2015 database.