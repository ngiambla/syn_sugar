two geometrical structures have been extensively studied for a manifold ofprobability distributions. one is based on the fisher information metric, whichis invariant under reversible transformations of random variables, while theother is based on the wasserstein distance of optimal transportation, whichreflects the structure of the distance between random variables. here, wepropose a new information-geometrical theory that is a unified frameworkconnecting the wasserstein distance and kullback-leibler (kl) divergence. weprimarily considered a discrete case consisting of $n$ elements and studied thegeometry of the probability simplex $s_{n-1}$, which is the set of allprobability distributions over $n$ elements. the wasserstein distance wasintroduced in $s_{n-1}$ by the optimal transportation of commodities fromdistribution ${\mathbf{p}}$ to distribution ${\mathbf{q}}$, where${\mathbf{p}}$, ${\mathbf{q}} \in s_{n-1}$. we relaxed the optimaltransportation by using entropy, which was introduced by cuturi. the optimalsolution was called the entropy-relaxed stochastic transportation plan. theentropy-relaxed optimal cost $c({\mathbf{p}}, {\mathbf{q}})$ wascomputationally much less demanding than the original wasserstein distance butdoes not define a distance because it is not minimized at${\mathbf{p}}={\mathbf{q}}$. to define a proper divergence while retaining thecomputational advantage, we first introduced a divergence function in themanifold $s_{n-1} \times s_{n-1}$ of optimal transportation plans. we fullyexplored the information geometry of the manifold of the optimal transportationplans and subsequently constructed a new one-parameter family of divergences in$s_{n-1}$ that are related to both the wasserstein distance and thekl-divergence.