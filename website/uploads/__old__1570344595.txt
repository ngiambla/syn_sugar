See	discussions,	stats,	and	author	profiles	for	this	publication	at:	https://www.researchgate.net/publication/317724119

Keyword	and	Keyphrase	Extraction	using
Newton's	Law	of	Universal	Gravitation

Conference	Paper	·	April	2017

DOI:	10.1109/CCECE.2017.7946724

CITATION
1

2	authors,	including:

Nicholas	Giamblanco
University	of	Toronto

1	PUBLICATION			1	CITATION			

SEE	PROFILE

READS
9

All	content	following	this	page	was	uploaded	by	Nicholas	Giamblanco	on	15	September	2017.

The	user	has	requested	enhancement	of	the	downloaded	file.

2017 IEEE 30th Canadian Conference on Electrical and Computer Engineering (CCECE)

Keyword and Keyphrase Extraction using Newton’s Law of Universal

Gravitation

Nicholas Giamblanco1 and Prathap Siddavaatam2

Abstract— In current times, there has been a surge in the
amount of collected data from computational systems. The vast
amount of data can be useful in many applications and ﬁelds,
particularly so in Big Data Analytics. However with a large
collection of data there is a difﬁculty discovering important
information. Automatic Document Summarization (ADS) sys-
tems are suitable for the task of outlining useful data. The ADS
system model takes a text document as input, and outputs a
semantically-relevant summary of this information. This infor-
mation can be further separated and outlined as keywords, or
keyphrases. This paper proposes a novel unsupervised approach
for automatic keyword and keyphrase generation system using
Newton’s Law of Universal Gravitation. This approach allows
for a complete capture of meaningful text, incorporating the
physical structure of a document and discovered relationships
between highly related words. Our model uses a new weighting
method that combines both the character length of a word, and
frequency of a word within a document to simulate a mass. Our
model then computes the force of attraction and ranks the word-
pair-force as a means of keyword and keyphrase extraction.
Experimental results on several text documents demonstrated
that the proposed approach improves on the state-of-the-art
models.

Index Terms— Data and Text Mining; Machine Learning; Big
Data; Natural Language Processing; Document Summarization;
Keyword Extraction; Document Classiﬁcation; Newtons Law
Of Universal Gravitation; Gravitational Fields; Newtonian
Gravity;

I. INTRODUCTION

A keyword can be deﬁned as a word which can relate
to the majority of information with a larger set of words,
namely a document. That
is, meaningful keywords and
keyphrases reﬂect the topics of a document [12]. Therefore
it is wise to investigate an automatic keyword and keyphrase
extraction model. The aim of an automatic keyword and
keyphrase extraction model is to capture meaningful infor-
mation within the document, extract this information, and
automatically generate a list of meaningful keywords which
relates to the document’s meaning. The elements of the list
can be a single keyword, or a combination of words (a
keyphrase). With the amount of collected data on the rise,
keywords and keyphrases have much importance in the ﬁeld
of natural language processing, information retrieval, data-
mining, text summarization, and text classiﬁcation. [7]

This article aims to improve the task of identifying and
generating keywords and keyphrases from documents in

1N. Giamblanco is a Researcher in Computer Engineering, Ryerson

University, 245 Church Street, Toronto, ON

2P. Siddavaatam is a Ph.D student in Electrical Engineering, Ryerson

University, 245 Church Street, Toronto, ON

order to aid data-mining initiatives. In general,
the use
of keywords and keyphrases enable individual
to query
information from these lists. The objective of this research
is to capture and extract the most relevant keywords and
keyphrases from a textual document. The task of keyword
extraction generally employs statistical models in order to
derive and arrive at a meaningful solution. In this paper, we
apply a non-probabilistic, unsupervised method of extracting
keywords, and related keywords which we will refer to as
keyphrases.

II. RELATED WORK

It is important to note that the evaluation of a Keyword
and Keyphrase extraction system is troublesome, as it is sub-
jective in nature. However, several Keyword and Keyphrase
Extraction Systems have been proposed in order to capture
the major topics within a single document using only a few
words or phrases (two or more words). Early systems utilized
word scoring functions that operated with word-document
frequency relationships, or from a TF-IDF scoring system.
Recently, an unsupervised method named Rapid Auto-
matic Keyword Extraction (RAKE) [12] has proven to be
effective in extracting meaningful keywords. The system
applies a stop-word ﬁlter to the corpus, in order to reduce
the effect of noise in their calculations. This system identiﬁes
keywords and keyphrases by formulating candidate phrases
and words. To extract meaningful data from this candi-
date list, their co-occurrences are calculated from document
statistics. After going through a ranking process, several
candidates are combined to form a keyword and keyphrase
set which describes the document.

TextRank is also a recent unsupervised keyword extraction
system, where the system’s foundation reﬂects the PageRank
algorithm. This keyword extraction system automatically
discovers a graphical representation of text, where singu-
lar words are symbolized as nodes, and the relationships
between nodes are generated by an indicated co-occurence
window [8].

Our proposed model is an unsupervised, single document,
domain-independent, keyword and keyphrase extraction sys-
tem. By using a Newtonian approach to linguistics, we
are able to cluster words following gravitational theories,
where words will be symbolically represented as the physical
quantity of mass; the interaction between masses will be
modeled utilizing Newton’s law of Universal Gravitation.
We will refer to our model as Key-LUG. In the following
sections, we will describe Key-LUG in more detail.

978-1-5090-5538-8/17/$31.00 ©2017 IEEE

2017 IEEE 30th Canadian Conference on Electrical and Computer Engineering (CCECE)

III. THE PROPOSED MODEL

In this section, we present our model, and describe the
contents of our extraction system behaviour. Our keyword
and keyphrase extraction system is categorized into four
processes from a high level view: (A) Noise Filtering, (B)
Word-Mass Assignment, (C) Word Attraction Computation,
(D) Keyword and Keyphrase Ranking.

A. Noise Filtering

Within any piece of English literature, it is imperative
to utilize pronouns, adverbs, or articles for proper sentence
construction. Although important in a grammatic context,
these words do not provide any meaning to the topics
of a document. Our model utilizes word frequency in the
calculation of the word-mass relationship. Therefore, these
frequented words can be viewed as noise from the system. To
combat noise, we apply a ﬁlter from a linguistic perspective,
where these stop words, will be omitted by our system
calculation. We supply the ﬁlter with a list of irrelevant words
selected by the authors of this paper. [10]

B. Word-Mass Assignment

Our proposed method (Algorithm 1) assigns a mass to all
words in the document. This means that we are considering
the entire document in our system, since much of the se-
mantic meaning is held in various locations [14]. Generally,
the importance of a word is reﬂected by its frequency of
appearance. Key-LUG operates under the assumption that
character length of a word impacts the importance of its
meaning. Therefore, the effect on word weight is propor-
tional to character count.

To apply the proposed model in this paper, the per word
mass assignment is required. We have assumed that a word
with a longer set of characters has more relevance to the
information in the document. Therefore, one can utilize the
character count as a factor in word-mass assignment. It is
also known that word frequency relates to the document’s
meaning [5]. The entire procedure for word-mass assignment
is equivalent as the product of one less than the frequency and
character count of the word. This results two effects, (1) we
remove words that have only occurred once, as their relative
mass would be zero, (2) a large number of characters within
a word do not necessarily reﬂect the semantic contents of
a document, ergo this is reﬂected by its frequency. During
this procedure, the relative location of each word is also
recorded, since gravitational ﬁelds affect objects based of
their distance r, and mass m. Refer to Algorithm 1 for a
detailed description of word-mass assignment.

C. Word Attraction Computation

Our proposed model assumes that within a document D,
a word wi may be related to some other word wj where
wi, wj ∈ D and i (cid:54)= j. We also assume that if the intensity of
a relation between words wi and wj is large, the keyphrase or
keyword pair has more relevance for extraction. Our model
forms a network of words, which is a complete weighted
graph. Each node ni of this network contains a word wi

Algorithm 1: Procedure for Word Mass Assignment
Result: Mw:= Word-Mass List

Input

Lw:= Word-Location List
: D := {w0, ..., wp}: Document of length p
S := {s0, ..., sk}: Stop Words of length k

1 position ← 1
2 foreach word in D do

Increment position by 1
/* Parsing words in the Document D

and filtering noise

44

5

6

7
8
9
10
11
12
13
14 end

else

end

end

*/

*/

*/

if word NOT in S then
/* Add Word to Location List
L[position] ⇐ word
/* Calculate Word Mass
if word NOT in M then

M [word] ⇐ 0
strlen ⇐ length(word)
M [word] ⇐ M [word] + strlen

and a mass mwi. An edge e connecting two nodes ni and
nj represents the distance between these two nodes |i − j|.
Hence,
links
closely to Issac Newton’s Law of Universal Gravitation:

this network based view of the our model

Faij = −G

mimj
|rij|2 ˆrij

(1)

Where two masses mi and mj, and their effective dis-
placement at positions pi and pj produce a relationship
in which the two masses are attracted to each other. This
attraction is represented in terms of force, namely Faij and
is inversely proportional to the square of pj−pi = rij. When
dealing with a physical environment, a constant G (Newton’s
Constant) is used for normalization. [3]

Our model employs this relationship, replacing mi and
mj with the mass of wi and wj, respectively. However,
original parameters may not relate to the ﬁeld of linguistics.
To provide a relationship with Newton’s Law of Universal
Gravitation we come to the following deﬁnitions:

D = {w0, w1, ..., wn}
wi = {c0, c1, ..., cq}
wi = {l0, l1, ..., lx}

(2)

Where each document D is deﬁned of n words, the ith word
wi in D consists of q characters c, and wi may occur in
different locations from l0 to lx in the document structure.
These parameters allow for the use of a modiﬁed universal
gravitational law. However, as n becomes large (n >> i),
the relationship between wi and wn−1 will be meaningless
(according to the inverse square law) [2] with the use of
equation (1). Most meaning can be captured within a subset

2017 IEEE 30th Canadian Conference on Electrical and Computer Engineering (CCECE)

of words wω, and at each word wi in the document, wω is
computed at the relative position of wi. Therefore x deﬁnes
a proportion of meaningful relationships between wi and wx.
We can deﬁne this in the following way:

x = λn where 0 < λ < 1

(3)

λ is the proportionality parameter which selects a portion of
words in the document. It should be intuitive that if we are
in some location l in the document, and 0 < l < n, then the
subset of words wω can be computed from all words thats
begin at l − x and end at l + x. Therefore, for each word wi
in the document D we deﬁne the following relationship:

Fwij = C

mwi mwj
|rij|2

,

where j ∈ {i − x, ..., i + x} and i ≤ n

(4)

Where C is a constant, mi and mj are the word-masses
such that i (cid:54)= j, and i ≤ n. It is important to note that C has
a normalizing effect with this equation. These formulae are
key for the proposed algorithm (Algorithm 2). At every word
location in the document, the attraction between two words
is computed and stored. The weighting process is detailed
as:

Algorithm 2: Word-Attraction Computation
Result: RankedKeyPhraseList: List of Ranked

Input

Keyphrases based on Force
: D := {w0, ..., wp}: Document of length p
λ: Proportionality Parameter

1 position ← 1
2 foreach word1 in D do
44
66
7
8

position ← location(word)
slidingW indow ← {position − λ : position + λ}
foreach word2 in SlidingWindow do

if word1 (cid:54)= word2 then

/* Computing Gravitational

Force between Masses
(words)

keys ← computeGF orce(word1, word2)

*/

*/

9

/* Appending to temp
keyphrase list

append(keyList, keys)

10
11
12
13 end

end

end

/* Ranking keyphrase list and removing
*/

non uniqueness from list

14 RankedKeyP hraseList ← rankF orce(keyList)

D. Keyword and Keyphrase Ranking

Once the document have been analyzed, the set of com-
puted values are sorted with respect to the force of attraction,
which we will refer to as the set K. A subset of the keyword
pairs (ordered pairs) S are then selected, such that S ⊂ K,
where S is of a speciﬁed length q, where q is a tunable

parameter. The elements within S are chosen by selecting
the keyword pairs of largest magnitude of force. These pairs
can be considered as candidates. However, candidate pairs
can only be added to the set S such that they do not contain
words already present in this set. During the initial phase of
our computation, we develop the set K. A sample of K is
listed in Table 1.

TABLE I

FORCE OF ATTRACTION BETWEEN KEYWORDS

Ranking

1
2
3
4
5
6
7
8
9
10

Keyword 1
equivalence
elicitation
algorithms

lindahl
allocation
manifest

communication

demand
classes
auctions

Keyword 2

queries

preference
learning
prices
optimal
valuations
polynomial

query

representation
combinatorial

Force [kN]

17532.2
16643.4
10243.3
4873.5
3188.4
2728.4
1760.6
975.2
708.5
577.6

Our method is easily understood and implementable.

IV. EXPERIMENTS

In order to test the effectiveness of this keyword and
keyphrase extraction system, we tested it against the fol-
lowing systems: RAKE [12], and TF-IDF scores [11] (im-
plemented in Python 2.7).

A. Experimental Settings

Our test method used a dataset (SemEval) from the
University of Waikato [6], which was employed in [12].
This particular dataset provided the expected outputs for a
keyphrase extraction system. This dataset contained 100 .txt
ﬁles for system evaluation. Utilizing this dataset allowed
for a comprehensive comparison between the two extraction
systems and our model, identifying the margins of optimal
operation. Our system operated with λ = 0.01 providing a
proportionality factor of 1% for all words in the document.
We also set the constant C to a value of 1. The majority of
our experiments suggest all words within 1% of the current
document position have relevance to the keywords. However,
this can be tuned to user-speciﬁc cases.

The systems we compared against had the following
operational parameters: (1) RAKE was conﬁgured to select
only words containing two or more characters, the maximum
allowable keyphrase length was two (since the Key-LUG sys-
tem only provides keyphrases constructed from two words),
and the term frequency was allowed to be at least one, since
Key-LUG permits single occurrence terms, (2) TF-IDF has
no conﬁguration parameters since this scoring system only
ranks keywords by term frequency.

Our system performs extractions on a document-by-
document basis,
is a compilation of in-
dividual articles and the subjects we tested against are
also single-document analysis systems. To provide complete
transparency in our experiments, we permitted keyphrases to

the test dataset

2017 IEEE 30th Canadian Conference on Electrical and Computer Engineering (CCECE)

be summarized into a list of single keywords. This elimi-
nates issues regarding complete matching [4]. The issue that
exists within matching is that a keyphrase generated from
a keyphrase extraction system may not exactly match the
human-provided keyphrase, differing by a grammatical basis.
Hence, we avoid this issue by summarizing all keyphrases
into a list of keywords.

B. Results

To evaluate Key-LUG against the following models, we
utilize the following measures which are common to use
within keyword and keyphrase extraction systems: (1) Pre-
cision, (2) Recall, (3) F1 Score [1]. These metrics are
considered a standard for text processing as by [15]. We
can mathematically describe these metrics as:

Precision =

Relevant Keywords ∩ Retrieved Keywords

Retrieved Keywords

Recall =

F1 = 2 ·

Relevant Keywords ∩ Retrieved Keywords

Relevant Keywords

1
recall +

1

1

precision

= 2 · precision · recall

precision + recall

(5)

(6)

(7)

and this provides a relationship between the recall and
precision of the keyword and keyphrase extractions, outlining
accuracy of the test. As deﬁned by [9], the measures of
Precision and Recall are equally important. However, since
our system is a single document keyword and keyphrase
extraction system, the measures Recall, Precision, and F1
Score are not computed for the entire corpus. Therefore, we
compute the geometric average of these measures across the
documents in the given corpus. We also split the corpus into
two sub-corpora, classiﬁed as (1) Training, and (2) Testing.
The procedure of splitting the corpus into a training and
test set was undertaken to be consistent with the compared
systems.

Tables II and III outline the difference in performance
by using Key-LUG in contrast to RAKE, and TF-IDF. As
observed from our results, it is clear Key-LUG outperforms
RAKE, and TF-IDF, with the chosen dataset [13]. As ob-
served from Table I,
the F1-Score of Key-LUG has an
increase of approximately 10% in comparison to TF-IDF.
Also, we observed a signiﬁcant increase of approximately
31% in contrast with RAKE’s F1-Score. Although, RAKE
outperforms Key-LUG in terms of Recall, this is insigniﬁcant
in terms of keyword and keyphrase extraction as recall
does not indicate precision. The observed metric of preci-
sion in Table I provides a clear difference in performance,
where Key-LUG outperforms both TF-IDF (approx. 1%) and
RAKE (approx. 35%). Similarily in Table II, we observe a
similar trend, where Key-LUG again dominates in terms of
F1-Score, and Precision for the testing data set. Again, it
was observed that the Recall metric from RAKE was greater
than Key-LUG’s metric.

View publication stats
View publication stats

TABLE II

SEMEVAL TRAINING SET RESULTS: MEAN SCORES

Method
TF-IDF
Key-LUG

RAKE

Precision % Recall % F1 Score %

34.53
36.65
1.58

34.53
58.75
72.58

34.53
44.32
3.08

TABLE III

SEMEVAL TEST SET RESULTS: MEAN SCORES

Method
TF-IDF
Key-LUG

RAKE

Precision % Recall % F1 Score %

35.78
35.71
1.55

35.78
58.92
74.05

35.78
44.42
3.03

V. CONCLUSIONS AND FUTURE WORK

In this article, we deﬁne a novel Keyword and Keyphrase
extraction system, which we name Key-LUG (Key-Law of
Univeral Gravitation), which is domain-independent, and
unsupervised, and formulated from the concept of Newtonian
Gravity, and his law of universal gravitation. As observed
from Section IV, our model outperforms two standards of
keyword and keyphrase extraction systems [12] and [11],
based of the metrics of Recall, Precision, and F1 Score [1].

REFERENCES

[1] Michael Buckland and Fredric Gey. The relationship between recall
and precision. Journal of the American society for information science,
45(1):12, 1994.

[2] Ofer Gal. Inverse square law. 2002.
[3] Øyvind Grøn. Newtons law of universal gravitation. In Lecture Notes

on the General Theory of Relativity, pages 1–16. Springer, 2009.

[4] Kazi Saidul Hasan and Vincent Ng. Automatic keyphrase extraction:
A survey of the state of the art. In ACL (1), pages 1262–1273, 2014.
[5] Adam Kilgarriff. Using word frequency lists to measure corpus
homogeneity and similarity between corpora. In Proceedings of ACL-
SIGDAT Workshop on very large corpora, pages 231–245, 1997.

[6] Olena Medelyan and Ian H Witten. Thesaurus based automatic
In Proceedings of the 6th ACM/IEEE-CS joint

keyphrase indexing.
conference on Digital libraries, pages 296–297. ACM, 2006.

[7] Z. A. Merrouni, B. Frikh, and B. Ouhbi. Automatic keyphrase
In 2016 4th IEEE
extraction: An overview of the state of the art.
International Colloquium on Information Science and Technology
(CiSt), pages 306–313, Oct 2016.

[8] Rada Mihalcea and Paul Tarau. Textrank: Bringing order into texts.

Association for Computational Linguistics, 2004.

[9] T. Pay.

Totally automated keyword extraction.

In 2016 IEEE
International Conference on Big Data (Big Data), pages 3859–3863,
Dec 2016.

[10] S. Popova, L. Kovriguina, D. Mouromtsev, and I. Khodyrev. Stop-
words in keyphrase extraction problem. In 14th Conference of Open
Innovation Association FRUCT, pages 113–121, Nov 2013.

[11] Juan Ramos et al. Using tf-idf to determine word relevance in
document queries. In Proceedings of the ﬁrst instructional conference
on machine learning, 2003.

[12] Stuart Rose, Dave Engel, Nick Cramer, and Wendy Cowley. Automatic
keyword extraction from individual documents. Text Mining, pages 1–
20, 2010.

[13] Alexander Thorsten Schutz. Keyphrase extraction from single docu-
ments in the open domain exploiting linguistic and statistical methods,
2008.

[14] Terry Winograd. Understanding natural language. Cognitive psychol-

ogy, 3(1):1–191, 1972.

[15] Yiming Yang and Xin Liu. A re-examination of text categorization
methods. In Proceedings of the 22nd annual international ACM SIGIR
conference on Research and development in information retrieval,
pages 42–49. ACM, 1999.

