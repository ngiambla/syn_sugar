620

M. W. P. STRAND BERG

and

that

between

positive

so-called

distribution

the noise-power

since it means

acts as least-count
it otherwise,

hP as 7 ff approaches —0. This is intuitively
spontaneous-emission

must be absorbed
of net absorption
must complete with the spontaneous-emission
Thus, as T,ff is lowered,
for the same signal-to-noise
di6'erence
temperatures
case,
zero as T,tt approaches +0. However,

to achieve one photon
(or emitted)
(or emission). These net photons
photons.
fewer photons need be handled
ratio. Here the tremendous
negative
(our T,tt) is apparent. For the absorption
p„(T) approaches
it approaches
satis-
noise
fying,
noise in a net emission
actually
if we have e photons per
system. To put
the
frequency interval per second from the amplifier,
the spon-
least count
noise. We are dealing here with phase-
taneous-emission
ratio
coherent photons, however,
instead of as the square
is as the reciprocal band width,
root of the reciprocal band width, which is the case
are counted.
when
solved the problem of the sta-
We have
tistical noise for a linear system with coherent particles.
inaccu-
noise, we
that have grown out of
the noise is
(shot effect) becomes

rate,
o8er the following suggestions
our work. At high effective temperatures,
high,

For those who like a simple, appealing,
of quantum-mechanical

is one photon and this is just

since the least-count

so the signal-to-noise

(or particles)

explanation

incoherent

essentially

photons

effect

albeit

absorption. As the effective temperature

large, because the net emission is small on account of
is
interfering
can be
lowered,
i.e.,
the
linearly
The
same
limit as T,ff approaches 0 will always be photon shot
noise.

the number of photons
and the same
lowered

to be amplified
least count,

can be maintained.

signal-to-noise

ratio,

details

ingenuity

sensitivity

and positive

of quantum-mechanical

then, many practical
the realm of engineering

Neglecting,
solely within
which we hold high regard), we have shown that
limiting
is given in a readily achievable
quantum-mechanical
power density is given parametrically
temperature. The essential
tween negative
strated by this function,
this function approaches
+0, this function

that are
(for
the
amplihers
limit by the eGective
noise power density. This noise
by an effective
and drastic difference be-
is demon-
in that, as T approaches —0,
(—hv) and, as T approaches
the region where hv(kT. .. the noise 6gure can be
in

approaches 0. This means

as the ratio of the quantum-
and the source temperature.

represented,
mechanical
With the equality
sign reversed,
comes large. For 1-cm radiation,
at 1.5'K. At any frequency, we may say that
limiting
mechanical

the noise 6gure be-
is
this turning point
the
quantum-

sensitivity
is, essentially,

temperature
ampli6er

essentially,
temperature

temperatures

for
hv/k.

that

a

P H YS I CAI

R EVI EW

VOI UME 106, NUMBER 4

MAY i5, &95 y

Infolrxiation Theory and Statistical Mechanics

Department

of Physics, Stanford University, Stanford, California

(Received September 4, 1956; revised manuscript

received March 4, 1957)

E. T. JAvxzs

is

the

least

regard

estimate.

on the basis of partial

theory provides a constructive
distributions

Information
up probability
to a type of statistical
and leads
It
maximum-entropy
possible on the given information;
mittal with
statistical mechanics as a form of statistical
as a physical
rules,
In the resulting "subjective statistical mechanics, "the usual
are an immediate

criterion for setting
knowledge,
inference which is called the
estimate
noncom-
considers
than
the usual
computational
of the partition function,
principle.
rules
and
verification; whether

biased
i.e., it is maximally
If one

consequence of the maximum-entropy

starting with the determination

justified independently

it is found that

are thus
in particular

of any physical

inference rather

of experimental

independently

to missing

information.

argument,

theory,

or not
best estimates
information

available.

the results agree with experiment,

the
could have been made on the basis of the

they still represent

that

assumptions

for its validity

theory dependent

It is concluded that statistical mechanics need not be regarded
on the truth of
in the laws of mechanics
equal a priori probabilities,
a sharp distinction
aspects. The former consists
of the states of a system and
of

as a physical
not contained
additional
(such as ergodicity, metric transitivity,
etc.). Furthermore,
between its physical
only of the correct enumeration
their
statistical

it is possible to maintain
and statistical

is a straightforward

properties;

inference.

example

latter

the

1. INTRODUCTION

HE recent appearance
survey' of past attempts

of a very comprehensive
to justify the methods
classical
of statistical mechanics
or quantum, has helped greatly, and at a very opportune
in this field.
time,

the unsolved problems

in terms of mechanics,

to emphasize

' D. ter Haar, Revs. Modern Phys. 27, 289 (1955).

years, we

in the sense that

still do not have

theory,
proceeding

Although
many
satisfactory
of argument
mechanics
regarded
Such an argument
mathematical

for
the subject has been under development
a complete
and
there is no line
from the laws of microscopic
is generally
respects.
should (a) be free from objection on
arbi-

to macroscopic phenomena,
as convincing
by physicists

(b) involve no additional

that
in all

grounds,

INFORMATION THEORY AND STATI STI CAL MECHANI CS

621

and (c) automatically

include

an
irre-
as those of conventional

conditions

and

of nonequilibrium

as well

trary assumptions,
explanation
versible
thermodynamics,
merely an ideal

processes

(b)

the

that

appear

condition

First was

theory will

It might

since equilibrium thermodynamics

that a physical
assumptions, whose consequences

is
limiting case of the behavior of matter.
is too severe,
since we expect
involve
certain unproved
are
deduced and compared with experiment. For example,
in the statistical mechanics of Gibbs'
there were several
difliculties which could not be understood
in terms of
and before the models which he
classical mechanics,
constructed could be made to correspond to the observed
facts, it was necessary to incorporate
into them addi-
tional restrictions not contained in the laws of classical
"freezing up" of certain
mechanics.
degrees of freedom, which caused the specific heat of
to be only —,' of the expected value.
diatomic
the entropy of com-
Secondly,
bined systems, which was resolved only by adoption of
the generic instead of the speci6c definition of phase,
to justify in
an assumption which
terms of classical notions. ' Thirdly,
in order to account
for the actual values of vapor pressures and equilibrium
about a natural
constants,
unit of volume
space was needed.
However, with the development
of quantum mechanics
the originally
are now seen as
assumptions
of the laws of physics. This
necessary
that we have now reached a
suggests
state where statistical mechanics
on physical
example of statistical

is no longer dependent
an

gases
the paradox regarding

arbitrary
consequences
the possibility

but may become merely

(l'ss~) of phase

an additional

hypotheses,

assumption

impossible

inference.

seems

time

That

involving

these questions

of specific phenomena

the present may be an opportune

to
is due to two recent de-
re-examine
velopments. Statistical methods are being applied to a
irreversible
variety
processes, and the mathematical methods which have
proven successful have not yet been incorporated
into
the basic apparatus of statistical mechanics. In addition,
theory' has been felt
the development
by many people to be of great significance for statistical
the exact way in which it should
mechanics,
be applied has remained obscure. In this connection it

of information

although

Principles

J. W. Gibbs, Elementary

in Statistical Mechanics
(Longmans Green and Company, New York, 1928), Vol. II of
collected works.
' We may note here that although Gibbs
(reference 2, Chap.
XV) started his discussion of this question by saying that
the
statistical method, "he concluded it with, "The perfect similarity
"seems in accordance with the spirit of the
generic definition
of several particles of a system will not in the least interfere with
in one case with a
the identification
particular particle in another. The question is one to be decided
the discussion of the problems with which we are engaged. "
of practical convenience
in accordance with the requirements
in
4 C. E. Shannon, Bell System Tech. J. 27, 379, 623 (1948);
and %. Weaver,
of

in C. E. Shannon
of Commlnication

these papers are reprinted
The Mathematical
Theory
Illinois Press, Urbana, 1949).

of a particular

(University

particle

entropy

from which

thermodynamic

and establishes

any connection

in itself establish

Section 2 defines

of equilibrium thermodynamic

to note the following. The mere fact

that
is essential
expression —P p; logp; occurs
the same mathematical
both in statistical mechanics and in information
theory
does not
between
these fields. This can be done only by 6nding new
viewpoints
and
entropy appear as the same concept.
information-theory
In this paper we suggest a reinterpretation
of statistical
this, so that information
mechanics which accomplishes
theory can be applied to the problem of justification of
statistical mechanics. We shall be concerned with the
prediction
properties,
treatment which involves only the
by an elementary
assigned to stationary states. Refinements
probabilities
obtainable by use of the density matrix and discussion
of irreversible processes will be taken up in later papers.
some of the ele-
mentary properties of maximum-entropy
inference, and
in Secs. 3 and 4 the application to statistical mechanics
facts concerning maxi-
is discussed. The mathematical
as given in Sec. 2, were pointed
mization of entropy,
out
these
properties were given the status of side remarks not
to the theory and not providing in themselves
essential
any justihcation
of statistical me-
chanics. The feature which was missing
has been
in the demon-
supplied
stration that
the expression for entropy has a deeper
This
meaning,
makes possible a reversal of the usual
line of reasoning in
statistical mechanics.
a
one constructed
theory based on the equations of motion, supplemented
of ergodicity, metric transi-
by additional
tivity, or equal a priori probabilities,
and the identifi-
cation of entropy was made only at the end, by com-
the laws of
parison
thermodynamics. Now, however, we
phenomenological
can take entropy as our starting concept, and the fact
that a probability
the entropy
subject to certain constraints becomes the essential fact
which justifies use of that distribution

long ago by Gibbs. In the past, however,

distribution maximizes

of thermodynamics.

quite independent

equations with

by Shannon'

only recently

for inference.

the resulting

the methods

Previously,

hypotheses

for

of

which

The most

results.

apparent

important

dependence

consequence

In freeing

the conceptual

is not, however,

simplification
from its

of this reversal of
and mathe-
viewpoint
matical
the
theory
on physical
of the above type, we make it possible to
hypotheses
see statistical mechanics
light.
Its principles
become
available for treatment of many new physical problems.
Two examples are provided by the derivation of Siegert's
"pressure ensemble" and treatment of a nuclear polari-
zation eGect, in Sec. 5.

in a much more general

and mathematical

methods

2. MAXIMUM-ENTROPY ESTIMATES

The quantity

x is capable of assuming

values x; (i =1,2 ~,rl). We are not given the corre-
sponding probabilities p;; all we know is the expectation

the discrete

622

value of the function f(oc):

E. T. JAYNES

(2-1)

On the basis of this information, what
value of the function g(x) '? At first glance,
seems insoluble because the given information
cient to determine
and the normalization

is the expectation
the problem
is insufli-
the probabilities p, . Equation (2-1)

condition
g p;=1
would have to be supplemented
tions before (g(x)) could be found.

(2-2)
by (tt—2) more condi-

the

events

theory

except

element

variable

"equally

paradoxes

of symmetry

random quantities,

"Principle
to supply

there is an evident
renders

is available,
Laplace's
an attempt

This problem of specie. cation of probabilities

in cases
is as old as
where little or no information
of
of probability.
the
Insuf6cient Reason" was
a
in which one said that
two events
criterion of choice,
if there is no
equal probabilities
are to be assigned
in cases
reason to think otherwise. However,
that
possible, " this
where
clearly
just as arbitrary as any other
assumption may appear
it has been very
that might be made. Furthermore,
in the case of continu'-
fertile in generating
'
intuitive
ously
notions of "equally possible" are altered by a change of
this way of
variables. ~ Since the time of Laplace,
abandoned,
largely
formulating
owing to the lack of any constructive
principle which
would give us a reason for preferring
one probability
in cases where both agree
distribution
equally well with the available information.
this problem,

one must
recognize the fact that probability theory has developed
in two very diferent directions as regards fundamental
notions. The "objective" school of thought''
regards
the probability of an event as an objective property of
of empirical
that event,
ratios in a
measurement
distri-
random experiment.
is making
bution

capable
always
by observation

In calculating a probability

in principle
of frequency

the objectivist

over another

For further

has been

discussion

problems

believes

since

that

he

of

any

intensity

assignment

the pressure,

of probabilities

chemical potentials,
statistical mechanics

~ Yet this is precisely the problem confronting

us in statistical
mechanics; on the basis of information which is grossly inadequate
to individual
to determine
quantum states, we are asked to estimate
specific
etc., of a
of magnetization,
heat,
is amaz-
macroscopic system. Furthermore,
in providing accurate estimates of these quantities.
ingly successful
that go
Evidently there must be other
of the problem as
beyond a mere correct statistical
stated above.

' The problems associated with the continuous

case are funda-
mentally more complicated than those encountered with discrete
random variables; only the discrete case will be considered here.
Inc., ¹w York, 1944l,
see E. P. Xorthrop, EiddLes in Mathe-

' For several examples,

(D. Van Nostrand Company,

for this success,

treatment

reasons

rnatscs
Chap. 8.

the

hand,

veri6able

the other

probabilities

"subjective"

predictions which are in principle
in every
just as are those of classical mechanics. The
detail,
test of a good objective probability distribution p(x) is:
does it correctly represent
the observable Quctuations
of XP
thought" " regards
On

of
of
the probability of an event
is merely
human ignorance;
a formal expression of our expectation that
the event
will or did occur, based on whatever
is
available. To the subjectivist,
the purpose of proba-
bility theory is to help us in forming plausible
conclu-
sions in cases where
information
thus detailed
available
verihcation is not expected. The test of a good subjec-
is does it correctly repre-
distribution
tive probability
as to the value of xP
sent our state of knowledge

to lead to certain conclusions;

school
expressions

there is not enough

information

as

identical,

controversy

has resulted

of subjective

to uphold one or the other

the theories
and objective
Although
are mathematically
the concepts
probability
refuse to be united. In the various statistical
themselves
to us by physics, both viewpoints
problems presented
are required. Needless
from
in all cases. The
attempts
subjective view is evidently the broader one, since it is
in this
always possible
the subjectivist will admit as legiti-
way; furthermore,
mate objects of
the
inquiry many
considers meaningless. The problem posed
objectivist
at
type, and
it we are necessarily adopting
therefore in considering
the subjective point of view.

section is of this

questions which

the beginning

to interpret

frequency

of this

ratios

which

avoids

bias, while

of sampling

of some method

there is a unique,

is given. The great

information
by information

Just as in applied statistics the crux of a problem is
that
often the devising
avoids bias, our problem is that of 6nding a probability
agreeing with
assignment
advance
whatever
theory lies in the discovery
provided
that
criterion for the
"amount
by a discrete
probability distribution, which agrees with our intuitive
represents more
notions
and
uncertainty
satisfies all other conditions which make it reasonable. 4
In Appendix A we sketch Shannon's
the
proof
is positive, which
increases with
quantity
and is additive for independent
increasing uncertainty,
sources of uncertainty,
is

distribution
does a sharply

of uncertainty"

unambiguous

represented

a broad

peaked

which

than

that

that

one,

&(Pt" P.)= ZP; P; lnP, , — (2-3)
where J is a positive constant. Since this is just
the
expression for entropy as found in statistical mechanics,
it will be called the entropy of the probability
distri-
the
terms
bution
"entropy" and "uncertainty"
I J. M. Keynes, 2 Treatise on Probabitity (MacMillan Company,
London, 1921).
"H. Jeffreys, Theory'of ProbabiLity

(Oxford University Press,

p, ; henceforth

as synonymous.

consider

we will

University Press, Princeton, 1946).

SH. Cramer,
' W. Feller, An Introduction

3IathematicaL Methods

of Statistics

(Princeton

to ProbabiLity

Theory

and its

APplications

(John Wiley and Sons, Inc., New York, 1950).

London, 1939).

I NFORM ATION THEORY AN 0 STAT I ST I CAL M E CHAN I CS

623

to whatever

It is now evident how to solve our problem;

in making
inferences on the basis of partial
information we must
use that probability
distribution which has maximum
is known. This is the only
entropy subject
to use any other
unbiased
of information
would amount
which by hypothesis we do not have. To maximize
(2-1) and (2-2), one
(2-3) subject
), p,
in the usual
introduces Lagrangian multipliers
way, and obtains the result

assignment we can make;

to the constraints

to arbitrary

assumption

p =& &. vf —(*~—)'

(2-&)

The constants X, p are determined
into
(2-1) and (2-2). The result may be written in the form

by substituting

where

(2-5)

(2-6)

(2-7)

will be called the partition function.

This may be generalized

f(x): given the averages

to any number of functions

form the partition function

Then the maximum-entropy
given by

probability

distribution

is

p, =exp( —[Xp+Xrft(xg)+. +X f (x,)j),

in which the constants are determined

from

&o= lnZ.

(2-10)

(2-11)

(2-12)

The entropy of the distribution

(2-10) then reduces to

S,„=Xp+Xt(ft(x))+
the constant E in (2-3) has been set equal

+X„(f„(x)),

(2-13)

where
unity. The variance of the distribution
to be

to
of f„(x) is found

(2-14)

In addition to its dependence on x, the function f„may
~ ~, and it is easily
contain other parameters
the
shown
derivatives

n&, n2,
the maximum-entropy

are given by

estimates

that

of

~

it

is uniquely

in case no information
of the possibilities
difference. The maximum-entropy

The principle of maximum entropy may be regarded
reason
is given
x,), with the

as an extension of the principle of insufhcient
(to which it reduces
except enumeration
essential
following
distribution may be asserted for the positive
reason
that
is
noncommittal with regard to missing infor-
maximally
there was no
mation,
reason to think otherwise. Thus the concept of entropy
the missing criterion of choice which Laplace
supplies
to remove
of the
needed
reason, and in addition it shows
principle of insufficient
is to be modified
in case
precisely how this principle
there are reasons for "thinking otherwise. "

instead of the negative one that

as the one which

the apparent

arbitrariness

determined

Mathematically,

the maximum-entropy

distribution

property

in eGect

properties

of Shannon's

and used in statistical

that
important
it assigns positive weight

the
is
has
no possibility
to every situation
ignored;
that is not absolutely excluded by the given information.
to an ergodic property.
This is quite similar
j.n this connection it is interesting
to note that prior to
the work of Shannon other
information measures had
been proposed""
inference,
in a diRerent way than in the present paper.
although
the quantity —g pP has many of the
In particular,
information meas-
qualitative
ure, and in many cases leads to substantially
the same
to apply in
results. However,
of —p p,s cannot be
practice. Conditional maxima
found by a stationary
involving Lagrangian
because the distribution which makes this
multipliers,
to prescribed averages does
quantity stationary subject
not in general satisfy the condition p;~&0. A much more
the Shannon measure
important
is that it is the only one which satis6es the condition of
consistency represented
law (Ap-
pendix A). Therefore one expects that deductions made
if carried far
from any other
enough, will eventually

it is much more dificult

lead to contradictions.

by the composition.

information measure,

reason for preferring

property

3. APPLICATION TO STATISTICAL MECHANICS
It will be apparent
ceding section that
form with the
is identical
inference
rules of calculation provided by statistical mechanics.
Specifically,

the theory of maximum-entropy
in mathematical

let the energy levels of a system be

from the equations

in the pre-

strain

tensor

the external

E(nt ns,
parameters
applied

),
e; may include
the
electric or magnetic
etc. Then if we know

where
volume,
fields, gravitational
only the average
probabilities of the levels E; are given by a special case
of (2-10), which we recognize as the Boltzmann distri-
bution. This observation really completes our derivation

(E), the maximum-entropy

potential,

energy

lnZ.

(2-15)

"R.A. Fisher, Proc. Cambridge Phil. Soc. 22, 700 (1925),
"J.L. Doob, Trans. Arn. Math. Soc. 39, 410 (1936),

E. T. JAYNES

of the conventional
example of statistical
temperature,
manner,

inference;
' with results summarized

rules of statistical mechanics as an
of
in a familiar

the identification

free energy, etc., proceeds

as

X,= (1/kT),

U TS=—F(T,n, ,ns,

)= kT—lnZ(T, crt, ns,

~ ),

(3-2)

(3-1)

S=— = —kp p;lnp;,

BF

tplT

i

p;=kT

8

BQy

lnZ.

(3-3)

(3-4)

entropy

is

identical with

The thermodynamic
information-theory
bution

stant. "The "forces" p; include pressure,

except

for

entropy
the presence

of the probability

of Boltzmann's

electric or magnetic moment,
(3-3), (3-4) then give a complete
thermodynamic
forces are given by special
estimates
maximum-entropy
(BE;/Bns) .

properties of the system,

cases of

of

the

etc., and Eqs.

description

the
distri-
con-
stress tensor,
(3-2),
of the
in which the
(2-15); i.e., as
derivatives

In the above relations we have assumed the number of
of each type to be 6xed. Nowlete~be the
molecules
number of molecules of type 1, e2 the number of type
then a possible "state"
2, etc. If the e, are not known,
of the system requires a specification of all the n, as well
). If we
as a particular
are given the expectation values

energy level E;(nins

l rsitis

(Ns),

(ui),

(E)
to make maximum-entropy
to (2-9),

according

then in order
we need to form,
function

inferences,
the partition

Z(aicrs

lib,ihs, P) = P P exp( —P.ili+Xsris
S/A/~ AD+" +PE'(~sl&) j)

(3-5)

and the corresponding maximum-entropy
that
(2-10)
is
ensemble;"
canonical
are recognized
stants,
the chemical potentials

distribution
grand
(2-11) fixing the con-
the Eqs.
as giving the relation between

"quantum-mechanical

the

of

pi = ~T~i)

(3-6)

(Cambridge

Statistical

Thermodynamics

'4 E. Schrodinger,
'~ Boltzmann's

University Press, Cambridge, 1948).

constant may be regarded as a correction factor
necessitated by our custom of measuring
in arbitrary
temperature
units derived from the freezing and boiling points of water. Since
the product TS must have the dimensions of energy,
the units in
which entropy is measured
depend on those chosen for tempera-
ture. It would be convenient
to de6ne an
"absolute
that Boltzmann's
to unity. Then entropy would become
constant
(as the considerations of Sec. 2 indicate it should be),
dhnensionless
and the temperature would be equal
to twice the average energy
per degree of freedom;
O~ of
Gibbs.

in general arguments

it is, of course,

the "modulus"

is made equal

cgs unit"

temperature

just

such

of

and the (rs;):

(e;)= BF/Bp, ,

(3-7)
where the free-energy function Ii = —hap, and Xo ——lnZ
is called the "grand potential. '"' Writing out
(2-13)
for
the usual
expression
F(T &1&2 '

rearranging,

case and

we have

' Pips'

' ')

this

'

=(E)—TS+pi(ei)+ps(ns)+

.

(3-8)

about

It is interesting

important

the entropy.

is known about

the laws of physics,

concept. Conventional

this can be recognized

the constants of the motion,

fact: theke is eothieg ie the ger&eral

to note the ease with which these
rules of calculation are set up when we make entropy
the primitive
arguments, which
exploit all that
in
lead to exactly
particular
that one obtains directly from
the same predictions
In the light of information
maximizing
as telling us a simple
theory,
but
turns
of motion that can provide us with any additional
infor
eke state of a system beyorid what we have
matiors
This re. fers to interpretation
obtairsed from measurement
of the state of a system at time t on the basis of meas-
carried out at time t. For predicting the course
urements
of time-dependent
knowledge of the equa-
tions of motion is of course needed. By restricting our
attention to the prediction of equilibrium properties as
in the present paper, we are in eRect deciding at the
outset
allowed
will be values of quantities which are observed to be
constant
these
quantities
(within macroscopic
laws of
experimental
in
physics,
assigning probabilities.

in time. Any prior
constant

the only type of initial

the
help us

in consequence

of
cannot

information

phenomena,

knowledge

redundant

is then

error)

would

that

that

and

be

ascribed

were
of

nonexistence

to discover

the motion,

and the assumed

This principle has interesting

of the motion in conventional

In view of the importance

consequences. Suppose
that a super-mathematician
a new
class of uniform integrals
hitherto
to
unsuspected.
sta-
uniform integrals
tistical mechanics,
of
that our equations would
new ones, one might expect
be completely changed by this development. This would
not be the case, however, unless we also supplemented
data
our prediction
as to the
which provided
likely values of these new constants. Even if we had a
clear proof that a system is not metrically
tramsitive, we
would still have no ratiorial basis for excluding arsy region
of Phase sPace skat is allowed by the information
available
to Ns. In its eRect on our ultimate predictions,
this fact
is equivalent
quite independ-
systems are in fact ergodic.
ently of whether physical

problem with new experimental
us with some information

to an ergodic hypothesis,

This shows

the great practical

convenience

subjective
establish

point of view.
the probabilities

If we were attempting
of di6erent

states

of the
to
in the

~6 D. ter Haar, Elements of Statistical Mechanics

Company, New York, 1954), Chap. 7.

(Rinehart and

I NFOR M ATION THEORY AN 0 STAT I ST I CAL ME CHAN I CS

625

is irrelevant. Nevertheless,

objective sense, questions of metric transitivity would
be crucial, and unless it could be shown that
the system
was metrically transitive, we would not be able to find
any solution at all. If we are content with the more
aim of 6nding subjective probabilities, metric
modest
transitivity
the subjective
theory leads to exactly the same predictions
that one
to justify in the objective sense. The
has attempted
only place where subjective statistical mechanics makes
contact with the laws of physics
is in the enumeration
of the diferent possible, mutually
states in
which the system might be. Unless a new advance in
alter
knowledge
the equations which we use for inference.

enumeration,

it cannot

exclusive

aGects

this

of

rather

expect

systems,

systems.

it alone

situations

conditions

the amount

of individual

is so minute

the behavior

the statistical

are so uniformly

su%ce for making

information
that

part of the argument

If the subject were dropped at this point, however,
it would remain very dificult
to understand why the
above rules of calculation
successful
In
in predicting
to its
stripping
it
bare essentials, we have revealed how little content
really has;
available
in
practical
could
reliable predictions. Without
never
arising from the physical nature of
further
macroscopic
such great
such as pressure
uncertainty
that we would have no definite theory which could be
compared with experiments. It might also be questioned
than the
whether
average, value over the maximum-entropy
distribution
that
since the
average might be the average of two peaks and itself
correspond to an impossible value.

should be compared with experiment,

in prediction of quantities

the most probable,

one would

it is not

the answer

the probability

It is well known that

to both of these
lies in the fact that for systems of very large
questions
distri-
of degrees of freedom,
number
butions of the usual macroscopic quantities determined
above, possess a single extremely
from the equations
sharp peak which includes practically all the "mass" of
the distribution. Thus for all practical purposes average,
most probable, median, or any other
type of estimate
It is instructive
are one and the'same.
in
given, maxi-
spite of the small amount of information
estimates of certain functions g(x) can
mum-entropy
because of the way the
approach
possible values of x are distributed. We illustrate
this
in which the possible values x; are defined
by a model
let e be a non-negative
and e a
as follows:
small positive number. Then we take

to see how,

certainty

practical

integer,

i = 1i 2)

x;+g—x,= e/x;",

(3-9)

xg"+'= e,

the x; increase without

.
According to this law,
limit as
i—+~, but become closer together at a rate determined
by e. By choosing e sufficiently
small we can make the
density of points x; in the neighborhood
of any partic-
ular value of x as high as we please, and therefore for a
a sum as
continuous
taken over a corre-
closely as we please by an integral

function f(x) we can approximate

sponding range of values of x,

Zf(x')
i

~t/(x) p(x)Cx,

where,

from (3-9), we have

p(x) = x"/e.
is not at

all

essential,

but

it

approximation

This
simplifies

the mathematics.

Now consider

the problem:

x'. Using our general
we first obtain the partition function

rules, as developed

(A) Given (x), estimate
in Sec. II,

Z(X) =

Jo

p(x)e-" Cx=

eX"+'

with X determined

from (2-11),

(x)= ——lnZ=

8
N

++1

Then we find, for the maximum-entropy

estimate of x',

(x'){(x))=Z-'

J,

x'p(x)e-"*Cx=

++2

(x)'.

(3-10)

Next we invert
x. The solution is

the problem:

(8) Given (x'), estimate

Z(X) =

p(x) exp( —Xx')Cx

(x') = ——lnZ=

8

N.

2"+'(e/2)! el% &&"+'&

++1

2X

(x){(x'))=Z '

40

px(x) exp( —Xx')Cx

(m+1) &

(-',e)!

2 ) (-', (m+1)]!

(x')*.

(3-»)

are plotted in Fig. 1 for the case v=1.
The solutions
The upper "regression line" represents Eq. (3-10), and
the lower one Eq. (3-11). For other values of I, the
lines are plotted in Fig. 2. As
slopes of the regression
e—&~, both regression
lines approach the line at 45',
and thus for large e, there is for all practical purposes
between (x) and (x'),
a de6nite functional
of which one is considered "given, " and
independently
as e increases
which
the distributions
in problem (A) we
find for the variance of x,

one "estimated. " Furthermore,

become sharper;

relationship

(x')—(x)'= (x)'/(I+1).

(3-12)

E. T. JAYNES

FIG. 1. Regression
of x and x2 for state
increasing
density
linearly with x. To
the maximum-
find
of
entropy
either quantity given
val-
the expectation
ue
other,
follow the arrows.

estimate

the

of

of

any

Similar

results hold in this model
estimate

for the maximum-
sufficiently mell-behaved
entropy
function g(x). If g(x) can be expanded in a power series
in a sufficiently wide region about
the point x=&x), we
the
obtain, using the distribution
and
following
variance of g:

of problem A above,
value

the expectation

expressions

for

(g(x)) =g(&x))+g"(&x))

(1 i
+oI —I

&x)'

2(n+ 1)

(3-13)

= Lg'(& ))&'

&x)'

n+ 1

|'1 )
+oI —I

Enm)

(3-14)

condition
of &g(x))

Conversely,

a sufficient
by knowledge
smooth monotonic

lack of symmetry,

determined
suSciently
parent
to g does not
the fact
that
been speci6ed in terms of x rather

in that
require monotonicity
the distribution

for x to be well
is that x be a
function of g. The ap-
from &x)
is due to
of possible values has

reasoning
of g(x),

As e increases,

the relative standard deviations of all
functions go down like e '; it

laws of thermodynamics,

sufficiently well-behaved
is in this way that definite
of the type of information given,
essentially independent
that at first appears
emerge from a statistical
incapable of giving reliable predictions. The parameter
e is to be compared with the number
of degrees of
freedom of a macroscopic system.

treatment

than g.

4. SUBJECTIVE AND OBJECTIVE

STATISTICAL MECHANICS

of

two diferent

interpretations.

the distribution

Many of the propositions of statistical mechanics are
The Max-
of velocities in a gas is, on the one
that
in the
for a given total energy; on
fact.
such as the density of a gas
on the one
on the other
physical phenomenon. Entropy as a con-

capable
wellian distribution
hand,
greatest number of ways
the other hand,
Fluctuations
or the voltage
hand the uncertainty
a measurable

it is a well-veri6ed

of our predictions,

across a resistor

can be realized

in quantities

experimental

represent

cept may be regarded
ignorance
hand, for equilibrium conditions
measurable
were 6rst found empirically.
that
the subjective interpretation

as a measure of our degree of
as to the state of a system; on the other
it is an experimentally
properties
It is this last circumstance
against

is most often advanced

quantity, whose most

as an argument

of entropy.

important

involved

recognize

and that

The relation

information

the probabilities

between maximum-entropy
inference
facts may be clari6ed as follows. We
and experimental
that
frankly
in
can have only
prediction based on partial
a subjective significance,
the situation cannot
a fictitious
be altered
to give
the
ensemble,
probabilities
One might
could be in any way
then ask how such probabilities
relevant
systems. A
remark that
good answer
probability
sense
reduced to calculation. " If we have little or no infor-

to this is Laplace's
theory

by the device of
even though

inventing
us
interpretation.

to the behavior of actual physical

famous
but

a frequency

is nothing

common

enables

this

2.0

l,8

LIJ0
O
(g) l,4

l.2

Fxo. 2. Slope of
lines as a

regression
function of e.

0

2

4

n~
6

8

I
IO

l4

that

common

no strong

conclusions

either way

the favored

to a certain question,

the appearance of a broad probability

sense
are
in statistical
distri-
the verdict, "no de6nite conclusion. "

mation relevant
tells us
justified. The same thing must happen
inference,
bution signifying
On the other hand, whenever
is sufficient
entropy inference gives sharp probability
indicating
makes
only when, and to the extent
bled ops.
When

the available information
to justify fairly strong opinions, maximum-
distributions
theory
as to experimental
behavior
that, it teads to sharp distri

to speak of experimental
decreases

the predictions
become inde6nite and it becomes less and less meaning-
veri6cation. As the avail-
ful
to zero, maximum-entropy
able information
inference (as well as common sense) shades continuously
becomes useless. Never-
into nonsense
and eventually
theless, at each stage it still
that
the best
represents
could have been done with the given information.

our distributions

alternative.

predictions

broaden,

definite

Thus,

the

Phenomena

in which

are well veri6ed experimentally

mechanics
those iri which our probability
macroscopic quantities
mously

sharp peaks. But

the predictions

of statistical
are always
for the
actually measured& have enor-
the process of maximum-

distributions,

I NFORMATION THEORY AN 0 STAT I ST I CAL M E CHAN I CS

627

the

over

have

distribution

distributions

for macroscopic

could have no experimental

entropy inference is one in which we choose the broadest
the microscopic
possible probability
states, compatible with the initial data. Evidently,
such
can
quantities
sharp
emerge only if it is true that for each of the overwhelm-
ing majority of those states to which appreciable weight
is assigned, we would
sabre macroscopic
behavior. We regard this, not merely as an interesting
fact without which
side remark, but as the essential
va-
statistical mechanics
lidity, and indeed without which matter would have no
definite macroscopic
physics would
"macroscopic uniformity" which provides
content of the calculations, not the probabilities
Because of it,
large extent
of the probability distributions
if we choose at random
over microstates. For example,
one out of each 10""of the possible states and arbi-
this
trarily assign zero probability
eGect on the
would in most cases have no discernible
macroscopic predictions.

of
the objective
per se.
of the theory are to a

It is this principle

the predictions

be impossible.

experimental

the others,

independent

properties,

to all

and

the

the

that

states

Consider

now the

the part

and their

the initial

predictions

information

case where

The f.ailures

case is that

at all. The most

cannot be explained

enumeration
of

reasonable
of
the

to lead to the correct prediction;

theory makes
and they are not borne out by
definite
experiment. This situation
away
was not
by concluding
if that were
sufhcient
a sharp
the case the theory would not have given
distribution
conclusion in
the different
this
(i.e.,
theory which
possible
of the laws of physics) was not
involves our knowledge
proof that a definite
correctly given. Thus, experirnenta/
prediction is incorrect gives evidence of the existence of new
statistical
laws of physics
by quantum theory,
mechanics,
provide several examples of this phenomenon.
Although the principle of maximum-entropy

inference
the prediction
appears
it is to be noted that
problems of statistical mechanics,
of statistical
prediction
is the problem of inter-
mechanics. Equally important
pretation; given certain observed behavior of a system,
can we draw as to the microscopic
what
causes of that behavior? To treat
this problem and
theory; which we may call
others
objective statistical mechanics,
is needed. Considerable
confusion has resulted from failure to distin-
semantic
prob-
guish between the prediction and interpretation
to make a single formalism do
lems, and attempting
for both.

capable of handling most of

is only one of the functions

like it, a diGerent

of classical

conclusions

resolution

of diGerent

the probabilities

In the problem of interpretation,

one will, of course,
in the
consider
of state e is the
objective sense;
in state e.
fraction of the time that
It is readily seen that one can never deduce the ob-
states from macro-
jective probabilities
of individual
There will be a great number of
scopic measurements.

i.e., the probability

the system spends

states

that

are

assignments

very severe unknown

probability
experimentally;

indistin-
diGerent
con-
guishable
straints on the possible states could exist. We see that,
it is now a relevant
question, metric transi-
although
tivity is far from necessary,
the
either
rules of calculation used in prediction, or for interpreting
observed behavior. Bohm and Schutzer" have come to
diGerent
similar
arguments.

on the basis of entirely

for justifying

conclusions

5. GENERALIZED STATISTICAL MECHANICS

the

this

that

inference

same basis,

in subjective

the principles

In conventional

statistical mechanics

are independent

and in the interaction

it appears
all measurable
subject
equivalence,

energy
quantities
plays a preferred role among all dynamical
because it is conserved both in the time development
of diGerent
of isolated systems
of maximum-
systems. Since, however,
of any physical
entropy
statistical
properties,
quantities may be treated on
mechanics
precautions. To
to certain
the
to the general
return
exhibit
we
inference of Sec. 2, and
problem of maximum-entropy
the eGect of a small change in the problem.
consider
fj, (x) whose expectation
Suppose we vary the functions
in an arbitrary way; Sf'(x;) may be
values are given,
for each value of k and i. In
specified independently
addition we change the expectation values of the fI, in
a manner
is no
5(fq) and (bfI,). We thus pass from
relation between
to a
one maximum-entropy
slightly different one,
bp;
and in the Lagrangian rnultipliers
from the 8(f&) and 8f1,(x;) by the relations of Sec. 2.
the entropy? The change in the
How does this affect
partition function (2-9) is given by

distribution
in probabilities

probability
the variations

5A, ~ being determined

independent

the bf~,

i.e.,

there

of

Q, p ——8 lnZ= —QI [Ag(fl)+RA(8 fI)],

(5-1)

and therefore, using (2-13),

bS= g, X,[5(f,)—(bf,)]

= Qa &abgI

The quantity

be.=b(f'&-(V')

(5-3)

supplied

to the system,

in the ordinary sense. We see that

of the notion of infinitesimal
and might be called the

provides a generalization
heat
"heat of the hth type. " If f& is the energy, BQ& is the
the Lagrangian
heat
factor for the kth type
multiplier XI, is the integrating
of heat, and therefore it is possible to speak of the kth
to ) & as
refer
type of temperature. However, we shall
to fl„and use
the quantity
only in their
the
the theory
is
conventional
to all quantities fI,
completely symmetrical with respect
'7 D. Bohm and W. Schutzer, Nuovo cimento, Suppl. II, 1004

conjugate"
"heat" and "temperature"
sense. Up to this point,

"statistically

terms

(1955).

628

E. T. JAYNES

equations

discussions,

the results of measurements

statistical mechanics. However,

the idea has been
the (fi) on which we base our probability
of
represent
If the energy is included among the
are identical with those of
in practice
of energy is rarely part of the initial
is

In all
the foregoing
that
implicit
distributions
various quantities.
fi„ the resulting
conventional
a measurement
information
to treat
easily measurable.
from the present point of
measurement
view, it is necessary to consider not only the system 0&
system 0.2. We
under
introduce several definitions:

In order
of temperature

is the temperature

but also another

the experimental

investigation,

available;

that

it

A heat bath is a system 0-2 such that
(a) The separation

than any macroscopically measurable

smaller
diGerence,
the macroscopic point of view, a continuum.

of energy levels of o2 is much
energy
the possible energies E2; form, from

so that

(b) The entropy S2 of the maximum-entropy

bility distribution
function
parameters" which can be varied independently
energy.

proba-
for given (E~) is a definite monotonic
no "mechanical
of its

i.e., o.2 contains

(E2);

of

(c) o~ can be placed in interaction with

another
system 0~ in such a way that only energy can be trans-
etc.),
ferred between them (i.e., no mass, momentum,
and in the total energy E=E&+E2+E», the interaction
to either E~ or E~. This
term 8~2 is small compared
state of interaction will be called thermal contact.

A thermometer

is a heat-bath
pointer which reads its average
however,
the temperature,

02 equipped with a
energy. The scale is,
calibrated so as to give a number T, called

de6ned by

1/T= dS2/d(E2). —

(5-4)

of

we place

in thermal

probability

temperature,

In a measurement

inferences, we must

the
contact with the system 0.~ of
thermometer
interest. We are now uncertain not only of the state of
the system 0.& but also of the state of the thermometer
find the
a2, and so in making
of the total
maximum-entropy
system 2= o.i+o.2, subject
to the available information.
a
A state of Z is de6ned by specifying simultaneously
state i of 0.& and a state j of 0.2
to which we assign a
probability p;, . Now however we have an additional
of a type not previously
con-
piece of information,
sidered; we know that
the interaction of 0.& and cr2 may
to take place between states (ij) and
allow transitions
(mu)

distribution

if the total energy is conserved:
Ei;+E2r=Ei +Ex

knowledge

for these transitions

In the absence of detailed
of the matrix
elements of Ei2 responsible
(which
in practice is never available), we have no rational basis
for excluding
of this
a given total
type. Therefore
the probability
energy must be considered equivalent;
p;; in its dependence
only

on energy may contain

all states of Z having

of any transition

the possibility

(E»+E»), not E» and E» separately ".Therefore,
of energy,

maximum-entropy
knowledge
associated with the partition function

of (E2) and the conservation

the
based on
is

distribution,

probability

Z (X) =P exp t

—X (Ei,+E2;)j=Zi(X)Zg (X),
which factors into separate partition functions
two systems

(5-5)

for the

Zi (X)=P, exp( —XEi;), Z2P ) =P; exp( —XE2;), (5-6)
with ) determined

as before by

(E2)= ——lnZ2(lw, );

BX

(5-7)

or, solving
quantity
reciprocal

statistically
temperature:

for X by use of (2-13), we find that

conjugate

to the energy

the
is the

(5-g)

X= dSg/d(E2) = 1/T.
this factorization
available

More generally,
is always possible if
consists of certain properties
the information
of 0& by itself and certain properties of 0-2 by itself.
The probability
into two
independent

distributions

distribution

factors

then

p' =p'(1)PJ(2)
and the total entropy is additive:

(5-9)

S(Z) =Si+S2.
the function of the thermometer

it

distribution

We conclude that

the probability

is no longer necessary

is
merely to tell us what value of the parameter
P should
be used in specifying
of
system 0&. Given this value and the above factorization
the
property,
properties of the thermometer
in detail when incorpo-
into our probability
rating temperature measurements
the mathematical
distributions;
processes
in
based on energy or
setting up probability
are exactly the same but
temperature measurements
only interpreted
diGerently.

to consider

distributions

used

It is clear

in such a way that

that any quantity which

changed between two systems
total amount
energy in arguments
mental
quantities

can be inter-
the
is conserved, may be used in place of
of the above type, and the funda-
to such
is preserved. Thus, we may dehne a "volume
bath, " "particle bath, " "momentum bath, " etc., and

of the theory with respect

symmetry

the probability
unbiased representation
of a system is obtained
procedure whether
of a measurement
quantity XA,.

distribution

gives

which

of our knowledge

the most
of the state
by the same mathematical
consists
conjugate

information

the available
of (f&) or its statistically

' This argument

admittedly

only by consideration
various
however,

lacks rigor, which can be supplied
of phase coherence properties between the
states by means of the density matrix formalism. This,
leads to the result given.

INFORMATION THEOR Y AN D STATISTICAL MECHANICS

629

Wt.. now give two elementary

ment of problems
tistical mechanics.

The pressure

ensemble
levels E,(V) dependent
macroscopic measurements
volume (V), the appropriate

using this generalized

examples of the treat-
form of sta-
.—Consider a gas with energy
on the volume. If we are given
of the energy (E) and the
partition function is

Z(X,y) =

~00

Jp

dVQ;expL —XE;(V)—pVj,

where ), p are Lagrangian multipliers. A short calcu-
lation shows that

the pressure is given by
P= (BE,(—V)/BV)= p/X,

the quantity

statistically

conjugate

to the

so that
volume is

p=XP=P/kT.

consists of either
Thus, when the available information
(T,(E)), plus either of the quantities
of the quantities
(P/T, (V)), the probability distribution which describes
is
this
proportional

information, without

assuming

anything

else,

to

E;(V)+XV

exp

(5-11)

and

to its

coupled

environment,

is conserved;

This is the distribution

A nuclear polarization egect

since all states of 0.& have

of the "pressure ensemble" of

they
angular momentum in such a way that

Lewis and Siegert."
Conside.—r amacroscopic
system which consistsof o& (a nucleus with spin I), and
o2 (the rest of the system) ~ The nuclear
spin is very
can
loosely
the
exchange
thus o-~ is an angular mo-
total amount
mentum bath. On the other hand they cannot exchange
the same energy.
energy,
and in addition
Suppose we are given the temperature,
the system 02 is rotating about a certain
are told that
axis, which we choose as the s axis, with a macroscopi-
angular velocity ~. Does that provide
cally measured
spin I is
any evidence
polarized along the same axis) Let m2 be the angular
of 02, and denote by e
momentum quantum number
to specify a
necessary
all other
quantum numbers
state of 02. Then we form the partition function
Z2(P, X)= P expL —PE2(u, m2) —Xm2j

for expecting that

the nuclear

(5-12)

where P=1/kT, and X is determined
(m2)= ——lnZ2 ——

8

by

Bco

A

(5-13)

where 8 is the moment of inertia of 0.2. Then, our most
the rotation of the molecular
unbiased

is that

guess

» M. B.Lewis and A. J.F. Siegert, Phys. Rev. lol, j.227 (19S6).

surroundings
polarization(m')=(I.

should produce on the average a nuclear
to the Brillouin function

), equal
8

(mg)= ——inZg(X),

BX

(5-14)

(5-15)

(5-16)

where

Z (g) —P e—xm

m—l
In the case I= —,', the polarization

(mg)= ——,' tanh(-,'X).
If the angular velocity &o is small,
proximated by a power series in ):

reduces to

(5-12) may be ap-

Z2(p, X) =Z2(p, O)L1—X(m2)0+~X(mm')0+

j,
in the
state. In the absence of a magnetic Geld,

for an expectation

)~ stands

value

(

where
nonrotating
(m2)p=O, A'(m22)0=k',

(5-13) reduces to

so that
X= —A"/kT.

(5-17)

the predicted polarization

Thus,
produced by a magnetic field of such strength that
Larmor
described by a "dragging coeKcient"

is just what would be
the
frequency ~~=~. If ~X~«1, the result may be

(mg)=

"'I(I+1)

3kTB

(mg).

(5-18)

it

involve

no reference

is closely related

the energy of the spin system,
to the mechanism causing

There is every reason to believe that
this ef'feet actually
to the Einstein-de Haas
exists;
effect. It is especially interesting that it can be predicted
in some detail by a form of statistical mechanics which
does not
and
the
makes
if a sample of
polarization. As a numerical
this should polarize the
water
to the same extent as would a magnetic Geld
protons
of about 1/7 gauss. This should be accessible to experi-
extension of these calculations
ment. A straightforward
by nuclear
would
spin values.
quadrupole

is modiGed
in the case of higher

is rotated at 36 000 rpm,

how the eGect

example,

coupling,

reveal

6. CONCLUSION

represented

very literally,

as a measure

of uncertainty

The essential point

in the arguments presented above

the von-Neumann —Shannon expres-

thus entropy becomes the primitive

is that we accept
of the
sion for entropy,
by a probability
amount
con-
distribution;
even than
cept with which we work, more fundamental
the prediction
energy.
in the subjective sense,
problem of statistical mechanics
in a very elementary
we can derive the usual
of ensembles or appeal
way without
ergodicity or equal
to the usual arguments
concerning
a priori probabilities. The principles
and mathematical
methods of statistical mechanics are seen to be of much

If in addition we reinterpret

any consideration

relations

630

E. T. JAYNES

more general applicability than conventional
arguments
would lead one to suppose. In the problem of prediction,
of entropy is not an application of a
the maximization
law of physics, but merely a method of reasoning which
ensures
assumptions
have been introduced.

no unconscious

arbitrary

that

APPENDIX A. ENTROPY OF A PROBABILITY

DISTRIBUTION

(pi,

probabilities

the discrete

The variable

x can assume

corresponding

the value of x can be represented

x ). Our partial understanding

values
of the processes
by
,p„). We
it is possible to find any
in a unique way
p„) which measures
by this proba-
It might at first seem very diflicult
for such a measure which would
to say nothing
fact
of consistency,
really to only one composition law, already
a con-
the function H(pi

(xi,
which determine
assigning
ask, with Shannon, 4 whether
quantity H(pi
the amount of uncertainty
bility distribution.
to specify conditions
ensure both uniqueness
of usefulness. Accordingly
that
elementary
amounting
determines
stant factor. The three conditions are:

it is a very remarkable

p„) to within

and consistency,

represented

conditions

the most

(1) H is a continuous
(2) If
are
is a monotonic

p;
,1/e)

function of the p,.
equal,

increasing

the

all

quantity

A (I)
function

=H(1/e, .
of s.

the

total

Instead

of giving

possibilities

the amount of uncertainty

composition
of the events

(3) The
the
law.
(xi. . x„) directly, we might
probabilities
group the 6rst k of them together as a single event, and
give its probability wi ——(pi+
+p&); then the next
are
probability
assigned
w2 ——(pi+i+
+p~ ), etc. When this much has been
as to the composite
specified,
is H(wi. w„). Then we give the conditional
events
events
probabilities
event had
(xi
for the second
occurred,
at
composite
the same state of knowledge
p„) had
therefore if our information measure
been given directly,
is to be consistent, we must obtain.
the same ultimate
how the choices were broken
uncertainty

,pi,/wi) of the ultimate
the first

and so on. We arrive ultimately

the conditional
event,

as if the (pi

xi), given

probabilities

no matter

composite

(pi/wi,

that

H(i'll'

down in this way. Thus, we must have
H(pl'

'ii'2)+ii'1H(pl/~1)

'pn)

'

'

'

'

+~2H(pi+i/~~,

,pip~/w2)+

' )pi/wi)
~ ~ .

(A-1)

factor x~ appears

The weighting
because the additional
is encountered
H(1/2, 1/3, 1/6) =H(1/2, 1/2)+-,'H(2/3, 1/3).

in the second term
.,p&/wi)
only with probability m». For example,

uncertainty H(p&/wi,

(1), it is sufhcient

to determine H

From condition

for all rational values

with n, integers. But then condition (3) implies
that H
is determined
quantities
2 (n). For we can regard a choice of one of the alter-
x„) as a first step in the choice of one of
natives

already from the symmetrical

(xi

equally likely alternatives,
also a choice between e; equally
likely alternatives.
As an example, with x=3, we might choose (ei,e~,N3)
= (3,4,2). For this case the composition law becomes

the second step of which is

H ~

3

|'342'
—,—,— ~+—A (3)+—A (4)+—A (2) = A (9).
(999)

9

9

4

9

2

In general,

it could be written

H(pi

p~)+Q; pQ (e;) =A (Q; n,).

(A-2)

In particular, we could choose all e; equal
upon (A-2) reduces to

to ns, where-

A (m) +A (e) =A (me).

(A-3)

(A-4)

Evidently this is solved by setting
A(N) =E inc,
(2), E)0. For a proof that

where, by condition
is the only solution of (A-3), we refer
Shannon's
have the desired result,

paper. 4 Substituting

(A-4)

(A-4)
to
into (A-2), we

the reader

H(p,

.p„)=K ln(g e;) Eg p;inn, —

= —KQ; p;lnp, .

