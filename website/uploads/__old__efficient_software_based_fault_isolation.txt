Efficient Software—Based Fault Isolation

Robert VVahbe

Steven Lucco

Thomas E. Anderson

Susan L. Graham

Computer Science Division
University of California

Berkeley, CA 94720

Abstract

One way to provide fault isolation among cooperating
software modules is to place each in its own address
space. However, for tightly—coupled modules, this so—
lution incurs prohibitive context switch overhead. In
this paper, we present a software approach to imple—
menting fault isolation within a single address space.
Our approach has two parts. First, we load the code
and data for a distrusted module into its own fault do-
main, a logically separate portion of the application’s
address space. Second, we modify the object code of a
distrusted module to prevent it from writing or jump-
ing to an address outside its fault domain. Both these
software operations are portable and programming lan—
guage independent.

Our approach poses a tradeoff relative to hardware
fault isolation: substantially faster communication be—
tween fault domains, at a cost of slightly increased
execution time for distrusted modules We demon—
strate that for frequently communicating modules, im~
plementing fault isolation in software rather than hard-
ware can substantially improve end-to—end application
performance.

This work was supported in part by the National Sci—
ence Foundation (CDA-8722788), Defense Advanced Research
Projects Agency (DARPA) under grant MDA972—92-J-1028 and
contracts DABT63-92'C-0026 and N00600-93—C—2481, the Digi-
tal Equipment Corporation (the Systems Research Center and
the External Research Program), and the AT&T Foundation.
Anderson was also supported by a National Science Foundation
Young Investigator Award. The content of the paper does not
necessarily reﬂect the position or the policy of the Government
and no ofﬁcial endorsement should be inferred.

Email: {rwahbe , lucco, tea, graham}@cs .berkeley.edu
Permission to copy Without fee all or part of (his material IS
granted provided that Hie cvpies are "0‘ made 0! distributed for
direct commercial advantage. the ACM copyright notice and the
mile of the publicaiion and MS data appear, and notice IS given
that copying is by permissmn of (he Assomalion for Computing
Machinery. To copy otherWise. or to republish, requires a fee
and/or specnfic permissron.

SIGOPS '93/12/93/N.C., USA
31993 ACM 0-83791-632-8/93/0012...$L50

1 Introduction

Application programs often achieve extensibility by
incorporating independently developed software mod—
ules. However, faults in extension code can render a
software system unreliable, or even dangerous, since
such faults could corrupt permanent data. To in—
crease the reliability of these applications, an operat—
ing system can provide services that prevent faults in
distrusted modules from corrupting application data.
Such fault isolation services also facilitate software de-
velopment by helping to identify sources of system fail—
ure.

For example, the POSTGRES database manager in—
cludes an extensible type system [St087]. Using this
facility, POSTGRES queries can refer to general—purpose
code that deﬁnes constructors, destructors, and pred—
icates for user—deﬁned data types such as geometric
objects. Without fault isolation, any query that uses
extension code could interfere with an unrelated query
or corrupt the database.

Similarly, recent operating system research has fo—
cused on making it easier for third party vendors
to enhance parts of the operating system. An ex
ample is micro-kernel design; parts of the operat—
ing system are implemented as user—level servers that
can be easily modiﬁed or replaced. More gener—
ally, several systems have added extension code into
the operating system, for example, the BSD network
packet ﬁlter [MRA87, MJQ3]7 application—speciﬁc vir-
tual memory management [HC92]. and Active Mes—
sages [VCGSQQ]. Among industry systems, Microsoft’s
Object Linking and Embedding system [Cla92] can
link together independently developed software mod—
tiles. Also, the Quark Xprese desktop publishing sys-
tem [Dy592] is structured to support incorporation of
general—purpose third party code. As with POSTGRES,

faults in extension modules can render any of these
systems unreliable. .

One way to provide fault isolation among cooperat—
ing software modules is to place each in its own address
space. Using Remote Procedure Call (RFC) [BN84],
modules in separate address spaces can call into each
other through a normal procedure call interface. Hard-
ware page tables prevent the code in one address space
from corrupting the contents of another.

Unfortunately, there is a high performance cost
to providing fault isolation through separate address
spaces. Transferring control across protection bound—
aries is expensive, and does not necessarily scale
with improvements in a processor’s integer perforv
mance [ALBL91]. A cross—address-space RPC requires
at least: a trap into the operating system kernel, copy—
ing each argument from the caller to the callee, sav~
ing and restoring registers, switching hardware ad—
dress spaces (on many machines, ﬂushing the transla—
tion lookaside buffer), and a trap back to user level.
These operations must be repeated upon RPC re—
turn. The execution time overhead of an RPC, even
with a highly optimized implementation, will often
be two to three orders of magnitude greater than
the execution time overhead of a normal procedure
call [BALL90, ALBL91].

The goal of our work is to make fault isolation cheap
enough that system developers can ignore its perfor—
mance effect in choosing which modules to place in
separate fault domains. In many cases where fault iso
lation would be useful, cross-domain procedure calls
are frequent yet involve only a moderate amount of
computation per call. In this situation it is imprac-
tical to isolate each logically separate module within
its own address space, because of the cost of crossing
hardware protection boundaries.

We propose a. software approach to implementing
fault isolation within a single address space. Our ap—
proach has two parts. First, we load the code and data
for a. distrusted module into its own fault domain, a
logically separate portion of the application’s address
space. A fault domain, in addition to comprising a cori—
tiguous region of memory within an address space, has
a unique identiﬁer which is used to control its access to
process resources such as ﬁle descriptors. Second, we
modify the object code of a distrusted module to pre—
vent it from writing or jumping to an address outside
its fault domain. Program modules isolated in sepa—
rate software—enforced fault domains can not modify
each other’s data or execute each other’s code except
through an explicit cross-fault-domain RPC interface.

We have identiﬁed several programming-language-
independent transformation strategies that can render
object code unable to escape its own code and data
segments. In this paper, we concentrate on a sim—

204

ple transformation technique, called sandboxing, that
only slightly increases the execution time of the mod-
iﬁed object code. We also investigate techniques that
provide more debugging information but which incur
greater execution time overhead.

Our approach poses a tradeoff relative to hardware—
based fault isolation. Because we eliminate the need to
cross hardware boundaries, we can offer substantially
lower-cost RPC between fault domains. A safe RPC in
our prototype implementation takes roughly 1.1 us on a
DECstation 5000/240 and roughly 0.8,us on a DEC Al-
pha 400, more than an order of magnitude faster than
any existing RFC system. This reduction in RFC time
comes at a cost of slightly increased distrusted module
execution time. On a test suite including the the C
SPE092 benchmarks, sandboxing incurs an average of
4% execution time overhead on both the DECstation
and the Alpha.

Software—enforced fault isolation may seem to be
counter-intuitive: we are slowing down the common
case (normal execution) to speed up the uncommon
case (crossrdomain communication). But for fre-
quently communicating fault domains, our approach
can offer substantially better end—to—end performance.
To demonstrate this, we applied software—enforced
fault isolation to the POSTGRES database system run-
ning the Sequoia 2000 benchmark. The benchmark
makes use of the POSTGRES extensible data. type sys—
tem to deﬁne geometric operators. For this bench—
mark, the software approach reduced fault isolation
overhead by more than a factor of three on a DECsta—
tion 5000/240.

A software approach also provides a tradeoif be
tween performance and level of distrust. If some mod—
ules in a. program are trusted while others are dis-
trusted (as may be the ease with extension code), only
the distrusted modules incur any execution time over-
head. Code in trusted domains can run at full speed.
Similarly, it is possible to use our techniques to im-
plement full security, preventing distrusted code from
even reading data outside of its domain, at a cost of
higher execution time overhead. We quantify this ef»
fect in Section 5.

The remainder of the paper is organized as follows.
Section 2 provides some examples of systems that re-
quire frequent communication between fault domains.
Section 3 outlines how we modify object code to pre—
vent it from generating illegal addresses. Section 4
describes how we implement low latency cross—faultv
domain RPC. Section 5 presents performance results
for our prototype, and ﬁnally Section 6 discusses some
related work.

2 Background

In this section, we characterize in more detail the
type of application that can beneﬁt from software—
enforced fault isolation. We defer further description
of the POSTGRES extensible type system until Section
5, which gives performance measurements for this ap—
plication.

The operating systems community has focused con-
siderable attention on supporting kernel extensibil-
ity. For example, the UNIX vnode interface is de-
signed to make it easy to add a new ﬁle system into
UNIX [Kle86]. Unfortunately, it is too expensive to
forward every ﬁle system operation to user level, so
typically new ﬁle system implementations are added
directly into the kernel. (The Andrew ﬁle system is
largely implemented at user level, but it maintains a
kernel cache for performance [HKM'l'BSH Epoch’s ter—
tiary storage ﬁle system [Web93] is one example of op—
erating system kernel code developed by a third party
vendor.

Another example is user—programmable high perfor—
mance I/O systems. If data is arriving on an I/O
channel at a high enough rate, performance will be
degraded substantially if control has to be transferred
to user level to manipulate the incoming data [FP93].
Similarly, Active Messages provide high performance
message handling in distributed—memory multiproces-
sors [VCG8921. Typically, the message handlers are
application-speciﬁc, but unless the network controller
can be accessed from user level [Thi92], the message
handlers must be compiled into the kernel for reason—
able performance.

A user-level example is the Quark Xpress desktop
publishing system. One can purchase third party soft-
ware that will extend this system to perform func~
tions unforeseen by its original designers [DysQQ]. At
the same time, this extensibility has caused Quark a
number of problems. Because of the lack of efﬁcient
fault domains on the personal computers where Quark
Xpress runs, extension modules can corrupt Quark’s
internal data structures Hence, bugs in third party
code can make the Quark system appear unreliable,
because end—users do not distinguish among sources of
system failure.

All these examples share two characteristics. First,
using hardware fault isolation would result in a signif-
icant portion of the overall execution time being spent
in operating system context switch code. Second, only
a small amount of code is distrusted; most of the exe-
cution time is spent in trusted code. In this situation,
software fault isolation is likely to be more efﬁcient
than hardware fault isolation because it sharply re—
duces the time spent crossing fault domain boundaries,
while only slightly increasing the time spent executing

205

the distrusted part of the application. Section 5 quan-
tiﬁes this trade-off between domain—crossing overhead
and application execution time overhead, and demon
strates that even if domain—crossing overhead repre—
sents a modest proportion of the total application ex—
ecution time, software—enforced fault isolation is cost
effective.

3 Software-Enforced Fault Iso-
lation

In this section, we outline several software encapsula—
tion techniques for transforming a distrusted module
so that it can not escape its fault domain. We ﬁrst
describe a technique that allows users to pinpoint the
location of faults within a software module. Next, we
introduce a technique, called sandboxing, that can iso-
late a distrusted module while only slightly increasing
its execution time. Section 5 provides a performance
analysis of this techinique. Finally, we present a soft-
ware encapsulation technique that allows cooperating
fault domains to share memory. The remainder of
this discussion assumes we are operating on a RISC
load /storc architecture, although our techniques could
be extended to handle CISCs. Section 4 describes
how we implement safe and efficient cross—fault—domain
RPC.

We divide an application’s virtual address space into
segments, aligned so that all virtual addresses within
a segment share a unique pattern of upper bits, called
the segment identiﬁer. A fault domain consists of two
segments, one for a distrusted module’s code, the other
for its static data, heap and stack. The speciﬁc seg-
ment addresses are determined at load time.

Software encapsulation transforms a distrusted
module‘s object code so that it can jump only to tar-
gets in its code segment, and write only to addresses
within its data segment. Hence, all legal jump tar—
gets in the distrusted module have the same upper bit
pattern (segment identiﬁer); similarly, all legal data
addresses generated by the distrusted module share
the same segment identiﬁer. Separate code and data
segments are necessary to prevent a module from mod—
ifying its code segmentl. It is possible for an address
with the correct segment identiﬁer to be illegal, for in-
stance if it refers to an unmapped page. This is caught
by the normal operating system page fault mechanism.

3.1

An unsafe mstmctzan is any instruction that jumps to
or stores to an address that can not be statically ver—

Segment Matching

10111" system supports dynamic linking through a special
interface.

iﬁed to be within the correct segment. Most control
transfer instructions, such as program‘counter‘relative
branches, can be statically veriﬁed. Stores to static
variables often use an immediate addressing mode and
can be statically veriﬁed. However, jumps through reg—
isters, most commonly used to implement procedure
returns, and stores that use a register to hold their
target address, can not be statically veriﬁed.

A straightforward approach to preventing the use of
illegal addresses is to insert checking code before eve
ery unsafe instruction. The checking code determines
whether the unsafe instruction’s target address has the
correct segment identiﬁer. If the check fails, the in-
serted code will trap to a system error routine outside
the distrusted module’s fault domain. We call this
software encapsulation technique segment matching.

On typical RISC architectures, segment matching
requires four instructions. Figure 1 lists a pseudo—code
fragment for segment matching. The ﬁrst instruction
in this fragment moves the store target address into
a dedzcated register. Dedicated registers are used only
by inserted code and are never modiﬁed by code in
the distrusted module. They are necessary because
code elsewhere in the distrusted module may arrange
to jump directly to the unsafe store instruction, by-
passing the inserted check. Hence, we transform all
unsafe store and jump instructions to use a dedicated
register.

All the software encapsulation techniques presented
in this paper require dedicated registersz. Segment
matching requires four dedicated registers: one to hold
addresses in the code segment, one to hold addresses
in the data segment, one to hold the segment shift
amount, and one to hold the segment identiﬁer.

Using dedicated registers may have an impact on
the execution time of the distrusted module. However,
since most modern RISC architectures, including the
MIPS and Alpha, have at least 32 registers, we can
retarget the compiler to use a smaller register set with
minimal performance impact. For example7 Section 5
shows that, on the DECstation 5000/240, reducing by
ﬁve registers the register set available to a C compiler
(gee) did not have a signiﬁcant effect on the average
execution time of the SPEC92 benchmarks.

3.2 Address Sandboxing

The segment matching technique has the advantage
that it can pinpoint the offending instruction. This
capability is useful during software development. We
can reduce runtime overhead still further, at the cost
of providing no information about the source of faults.

2 For architectures with lenitccl register sets, such as the
80386 [Int86], it is possible to encapsulate a module using no re-
served registers by restricting control ﬂow within a fault domain.

206

dedicated—reg <2 target address
lilove target address into dedicated register.
scratch-reg <= (dedicated—reg>>shift~reg)
Right—shift address to get segment identiﬁer.
scratch—reg is not a dedicated register.
shift-reg is a dedicated register.
compare scratch—reg and segment—reg
segment-reg is a dedicated register.
trap if not equal
Trap if store address is outside of segment.
store instruction uses dedicated-reg

Figure 1: Assembly pseudo code for segment matching.

dedicated—reg c target-reghand—mask—reg
Use dedicated register and—mask-reg
to clear segment identiﬁer bits.
dedicated—reg <2 dedicated-regl segment—reg
Use dedicated register segment-reg
to set segment identiﬁer bits.
store instruction uses dedicated-reg

Figure 2: Assembly pseudo code to sandbox address
in target—reg.

Before each unsafe instruction we simply insert code
that sets the upper bits of the target address to the
correct segment identifier. We call this sandborzn g the
address. Sandboxing does not catch illegal addresses;
it merely prevents them from affecting any fault do—
main other than the one generating the address.

Address sandboxing requires insertion of two arith-
metic instructions before each unsafe store or jump
instruction. The ﬁrst inserted instruction clears the
segment identifier bits and stores the result in a ded—
icated register. The second instruction sets the seg—
ment identiﬁer to the correct value. Figure 2 lists the
pseudo‘code to perform this operation. As with seg-
ment matching, we modify the unsafe store or jump
instruction to use the dedicated register. Since we are
using a dedicated register, the distrusted module code
can not produce an illegal address even by jumping
to the second instruction in the sandboxing sequence;
since the upper bits of the dedicated register will al—
ready contain the correct segment identiﬁer, this sec-
ond instruction will have no effect. Section 3.6 presents
a simple algorithm that can verify that an object code
module has been correctly sandboxed.

Address sandboxing requires ﬁve dedicated registers.
One register is used to hold the segment mask, two
registers are used to hold the code and data segment

 

 

<——reg+oﬂ'sel j
«— reg

 

Guard Zones S eg ment

 

 

 

 

 

Figure 3: A segment with guard zones. The size of
the guard zones covers the range of possible immediate
offsets in register—plus-offset addressing modes.

identiﬁers, and two are used to hold the sandboxed
code and data addresses.

3.3 Optimizations

The overhead of software encapsulation can be re-
duced by using conventional compiler optimizations.
Our current prototype applies loop invariant code mo—
tion and instruction scheduling optimizations [ASU86,
ACD74]. In addition to these conventional techniques,
we employ a number of optimizations specialized to
software encapsulation.

We can reduce the overhead of software encapsulae
tion mechanisms by avoiding arithmetic that computes
target addresses. For example, many RISC architec—
tures include a register-plus—oﬁset instruction mode,
where the offset is an immediate constant in some lim—
ited range. On the MIPS architecture such offsets are
limited to the range -64K to +64K. Consider the
store instruction store value,oﬁset(reg), whose
address offset (reg) uses the register—plus—olfsct ad~
dressing mode. Sandboxing this instruction requires
three inserted instructions: one to sum reg+oﬁset
into the dedicated register, and two sandboxing in—
structions to set the segment identiﬁer of the dedicated
register.

Our prototype optimizes this case by sandboxing
only the register reg, rather than the actual target ad—
dress reg+oﬁset, thereby saving an instruction. To
support this optimization, the prototype establishes
guard zones at the top and bottom of each segment.
To create the guard zones, virtual memory pages ad—

jacent to the segment are unmapped (see Figure 3),
We also reduce runtime overhead by treating the

MIPS stack pointer as a dedicated register. We avoid
sandboxing the uses of the stack pointer by sandboxing

207

this register whenever it is set. Since uses of the stack
pointer to form addresses are much more plentiful than
changes to it, this optimization signiﬁcantly improves
performance.

Further, we can avoid sandboxing the stack pointer
after it is modiﬁed by a small constant offset as long as
the modiﬁed stack pointer is used as part of a load or
store address before the next control transfer instruc»
tion. If the modiﬁed stack pointer has moved into a
guard zone, the load or store instruction using it will
cause a hardware address fault. On the DEC Alpha
processor, we apply these optimizations to both the
frame pointer and the stack pointer.

There are a number of further optimizations that
could reduce sandboxing overhead. For example,
the transformation tool could remove sandboxing se—
quences from loops, in cases where a store target ad-
dress changes by only a small constant oifset during
each loop iteration. Our prototype does not yet imple—
ment these optimizations.

3.4 Process Resources

Because multiple fault domains share the same virtual
address space, the fault domain implementation must
prevent distrusted modules from corrupting resources
that are allocated on a per—addressspace basis. For
example, if a fault domain is allowed to make system
calls, it can close or delete ﬁles needed by other code
executing in the address space, potentially causing the
application as a whole to crash.

One solution is to modify the operating system to
know about fault domains. On a system call or page
fault, the kernel can use the program counter to deter-
mine the currently executing fault domain, and restrict
resources accordingly.

To keep our prototype portable, we implemented
an alternative approach. In addition to placing each
distrusted module in a separate fault domain, we re—
quire distrusted modules to access system resources
only through cross-fault-domain RPC. We reserve a
fault domain to hold trusted arbitration code that de—
termines whether a particular system call performed
by some other fault domain is safe. If a distrusted
module’s object code performs a direct system call, we
transform this call into the appropriate RPC call. In
the case of an extensible application, the trusted por-
tion of the. application can make system calls directly
and shares a fault domain with the arbitration code.

3.5 Data Sharing

Hardware fault isolation mechanisms can support data
sharing among virtual address spaces by manipulate
ing page table entries. Fault domains share an ad—

dress space, and hence a set of page table entries,
so they can not use a standard shared memory im—
plementation. Read-only sharing is straightforward;
since our software encapsulation techniques do not al-
ter load instructions, fault domains can read any mem—
ory mapped in the application’s address space 3.

If the object code in a particular distrusted mod—
ule has been sandboxed, then it can share read-write
memory with other fault domains through a technique
we call lazy pointer swizzling. Lazy pointer swizzling
provides a mechanism for fault domains to share ar—
bitrarily many read‘write memory regions with no ad-
ditional runtirne overhead. To support this technique,
we modify the hardware page tables to map the shared
memory region into every address space segment that
needs access; the region is mapped at the same offset
in each segment. In other words, we alias the shared
region into multiple locations in the virtual address
space, but each aliased location has exactly the same
low order address bits. As with hardware shared mem-
ory schemes, each shared region must have a different
segment offset.

To avoid incorrect shared pointer comparisons in
sandboxed code, the shared memory creation inter—
face must ensure that each shared object is given a
unique address. As the distrusted object code ac-
cesses shared memory, the sandboxing code automati-
cally translates shared addresses into the correspond
ing addresses within the fault domain’s data segment.
This translation works exactly like hardware transla~
tion; the low bits of the address remain the same, and
the high bits are set to the data segment identiﬁer.

Under operating systems that do not allow virtual
address aliasing, we can implement shared regions by
introducing a new software encapsulation technique:
shared segment matching. To implement sharing, we
use a dedicated register to hold a bitmap. The bitmap
indicates which segments the fault domain can access.
For each unsafe instruction checked, shared segment
matching requires one more. instruction than segment
matching.

3.6 Implementation and Veriﬁcation

We have identiﬁed two strategies for implementing
software encapsulation. One approach uses a compiler
to emit encapsulated object code for a distrusted mod-
ule; the integrity of this code is then veriﬁed when the
module is loaded into a fault domain. Alternatively,
the system can encapsulate the distrusted module by
directly modifying its object code at load time.

a\«Ve have implemented versions of these techniques that per
form general protection by encapsulating load instructions as
well as store and jump instructions. We discuss the performance
of these variants in Section 5.

Our current prototype uses the first approach. We
modiﬁed a version of the gcc compiler to perform soft—
ware encapsulation. Note that While our current imple-
mentation is language dependent, our techniques are
language independent.

We built a veriﬁer for the MIPS instruction set
that works for both sandboxing and segment match-
ing. The main challenge in veriﬁcation is that, in the
presence of indirect jumps, execution may begin on
any instruction in the code segment. To address this
situation, the veriﬁer uses a property of our software
encapsulation techniques: all unsafe stores and jumps
use a dedicated register to form their target address.
The veriﬁer divides the program into sequences of in—
structions called unsafe regions. An unsafe store re-
gion begins with any modiﬁcation to a dedicated store
register. An unsafe jump region begins with any mod-
iﬁcation to a dedicated jump register. If the ﬁrst in—
struction in a unsafe store or jump region is executed,
all subsequent instructions are guaranteed to be exe-
cuted. An unsafe store region ends when one of the
following hold: the next instruction is a store which
uses a dedicated register to form its target address,
the next instruction is a control transfer instruction,
the next instruction is not guaranteed to be executed,
or there are no more instructions in the code segment.
A similar deﬁnition is used for unsafe jump regions.

The veriﬁer analyzes each unsafe store or jump re:
gion to insure that any dedicated register modiﬁed in
the region is valid upon exit of the region. For ex—
ample, a load to a dedicated register begins an unsafe
region. If the region appropriately sandboxes the ded—
icated register, the unsafe region is deemed safe. if an
unsafe region can not be veriﬁed, the code is rejected.

By incorporating software encapsulation into an ex—
isting compiler, we are able to take advantage of com—
piler infrastructure for code optimization. However,
this approach has two disadvantages. First, most mod-
iﬁed compilers will support only one programming lan—
guage (gcc supports C, C++, and Pascal). Second, the
compiler and veriﬁer must be synchronized with re—
spect to the particular encapsulation technique being
employed.

An alternative, called bmary patchzng, alleviates
these problems. When the fault domain is loaded, the

system can encapsulate the module by directly modi-
fying the object code. Unfortunately, practical and r07
bust binary patching, resulting in efﬁcient code, is not
currently possible [LB92]. Tools which translate one
binary format to another have been built, but these
tools rely on compiler—speciﬁc idioms to distinguish
code from data and use processor emulation to han-
dle unknown indirect jumps[SCK”93]. For software
encapsulation, the main challenge is to transform the
code so that it uses a subset of the registers, leav—

208

 

Trusted
Caller Domain

Unlru sted
Calico Domain

call Add

 

Jump Table

 

 

 

Figure 4: Major components of a crossefault—domain
RFC.

ing registers available for dedicated use. To solve this
problem, we are working on a binary patching proto-
type that uses simple extensions to current object ﬁle
formats. The extensions store control ﬂow and register
usage information that is sufﬁcient to support software
encapsulation.

4 Low Latency Cross Fault Do—
main Communication

The purpose of this work is to reduce the cost of fault
isolation for cooperating but distrustful software mod—
ules. In the last section, we presented one half of our
solution: efficient software encapsulation. In this sec-
tion, we describe the other half: fast communication
across fault domains.

Figure 4 illustrates the major components ofa cross—
fault~domain RFC between a trusted and distrusted
fault domain. This section concentrates on three as—
pects of fault domain crossing. First, we describe
a simple mechanism which allows a fault domain to
safely call a trusted stub routine outside its domain;
that stub routine then safely calls into the destination
domain. Second, we discuss how arguments are effi—
ciently passed among fault domains. Third, we detail
how registers and other machine state are managed on
cross—fault—domain RPCs to insure fault isolation. The
protocol for exporting and naming procedures among
fault domains is independent of our techniques.

The only way for control to escape a. fault domain
is via a jump table. Each jump table entry is a con—
trol transfer instruction whose target address is a legal
entry point outside the domain. By using instructions
whose target address is an immediate encoded in the
instruction, the jump table does not rely on the use of
a dedicated register. Because the table is kept in the

(readvonly) code segment, it can only be modified by
a trusted module.

For each pair of fault domains a customized call and
return stub is created for each exported procedure.
Currently, the stubs are generated by hand rather than
using a stub generator [JRTSS]. The stubs run unpro—
tected outside of both the caller and callee domain.
The stubs are responsible for copying cross-domain
arguments between domains and managing machine
state.

Because the stubs are trusted, we are able to copy
call arguments directly to the target domain. Tra—
ditional RPC implementations across address spaces
typically perform three copies to transfer data. The
arguments are marshalled into a message, the kernel
copies the message to the target address space, and
ﬁnally the callee must de-marshall the arguments. By
having the caller and callee communicate via a shared
buffer, LRPC also uses only a single copy to pass data
between domains [BALLQI].

The stubs are also responsible for managing machine
state. On each cross—domain call any registers that are
both used in the future by the caller and potentially
modiﬁed by the callee must be protected. Only regis—
ters that are designated by architectural convention to
bc preserved across procedure calls are saved. As an
optimization, if the callee domain contains no instruc—
tions that modify a preserved register we can avoid
saving it. Karger uses a trusted linker to perform this
kind of optimization between address spaces [KarSQ].
In addition to saving and restoring registers, the stubs
must switch the execution stack, establish the correct
register context for the software encapsulation tech-
nique being used, and validate all dedicated registers.

Our system must also be robust in the presence of
fatal errors, for example, an addressing violation7 while
executing in a fault domain. Our current implementa—
tion uses the UNIX signal facility to catch these errors;
it then terminates the outstanding call and notiﬁes the
caller’s fault domain. If the application uses the same
operating system thread for all fault domains, there
must be a way to terminate a call that is taking too
long, for example, because of an inﬁnite loop. Trusted
modules may use a timer facility to interrupt execu—
tion periodically and determine if a call needs to be
terminated.

5 Performance Results

To evaluate the performance of software-enforced fault.
domains, we implemented and measured a prototype
of our system on a 40MHz DECstation 5000/240 (DEC—
MIPS) and a lﬁONIliz Alpha 400 (DEC—ALPHA).

We consider three questions. First, how much over

209

head does software encapsulation incur? Second, how
fast is a crossrfault—domain RFC? Third, what is the
performance impact of using software enforced fault
isolation on an end-user application? We discuss each
of these questions in turn.

5.1 Encapsulation Overhead

We measured the execution time overhead of sand-
boxing a wide range of C programs, including the C
SPE092 benchmarks and several of the Splash bench-
marks [AssQl, SWGQl]. We treated each benchmark
as if it were a distrusted module, sandboxing all of
its code. Column 1 of Table 1 reports overhead on
the DEC—MIPS, column 6 reports overhead on the DEC—
ALPHA. Columns 2 and 7 report the overhead of using
our technique to provide general protection by sand«
boxing load instructions as well as store and jump
instructions“. As detailed in Section 3, sandboxing
requires 5 dedicated registers. Column 3 reports the
overhead of removing these registers from possible use
by the compiler. All overheads are computed as the
additional execution time divided by the original pro~
gram‘s execution time.

On the DECeMiPS, we used the program measure—
ment tools pixie and qpt to calculate the number
of additional instructions executed due to sandbox~
ing [Dig, BL92]. Column 4 of Table 1 reports this
data as a percentage of original program instruction
counts.

The data in Table 1 appears to contain a num—
ber of anomalies For some. of the benchmark pro-
grams, for example, 056.ear 011 the DECAMIPS and
026 . compress on the DEC—ALPHA, sandboxing reduced
execution time. in a number of cases the overhead is
surprisingly low.

To identify the source of these variations we de~
veloped an analytical model for execution overhead.
The model predicts overhead based on the number
of additional instructions executed due to sandbox
ing (s—znstructzons), and the number of saved ﬂoat~
ing point interlock cycles (interlocks). Sandboxing in»
creases the available instructionlevel parallelism, aL
lowing the number of ﬂoating—point interlocks to be
substantially reduced The integer pipeline does not
provide interlocking; instead, delay slots are explicitly
ﬁlled with nop instructions by the compiler or assem~
bler. Hence, scheduling ell'ects among integer instruc~
tions will be accurately reﬂected by the count of in~
structions added (s—mstructzons). The expected overs
head is computed as:

(s—msz‘mchons — interlacksﬂcycles—per—sccond

 

original-erecutwn- lune-seconds

4Loads in the libraries, such as the standard C library, were
not sandboxed.

The model provides an effective way to separate known
sources of overhead from second order effects. Col-
umn 5 of Table 1 are the predicted overheads.

As can be seen from Table 1, the model is, on aver
age, eﬁective at predicting sandboxing overhead. The
differences between measured and expected overheads
are normally distributed with mean 0.7% and standard
deviation of 2.6%. The difference between the means
Ofthe measured and expected overheads is not statisti-
cally signiﬁcant. This experiment demonstrates that,
by combining instruction count overhead and ﬂoating
point interlock measurements, we can accurately pres
dict average execution time overhead. If we assume
that the model is also accurate at predicting the over—
head of individual benchmarks, we can conclude that
there is a second order effect creating the observed
anomalies in measured overhead,

We can discount eﬁective instruction cache size and
virtual memory paging as sources for the observed ex~
ecution time variance. Because sandboxing adds in-
structions, the effective size of the instruction cache is
reduced. While this might account for measured over-
heads higher than predicted, it does not account for
the opposite effect. Because all of our benchmarks are
compute bound, it is unlikely that the variations are
due to virtual memory paging.

The DEC<MIPS has a physically indexed, physically
tagged, direct mapped data cache. In our experiments
sandboxing did not affect the size, contents, or starting
Virtual address of the data segment. For both original
and sandboxed versions of the benchmark programs,
successive runs showed insigniﬁcant variation. Though
difﬁcult to quantify, we do not believe that data cache
alignment was an important source of variation in our
experiments.

\Ve conjecture that the observed variations are
caused by instruction cache mappzng conﬂicts. Soft—
ware encapsulation changes the mapping of instruc~
tions to cache lines, hence changing the number of in—
struction cache conﬂicts. A number of researchers have
investigated minimizing instruction cache conﬂicts to
reduce execution time [McF89, PHQO, Sam88]. One
researcher reported a 20% performance gain by sim—
ply ehanging the order in which the object ﬁles were.
linked [PHQO]. Samples and Hilﬁnger report signif—
icantly improved instruction cache miss rates by re—
arranging only 3% to 8% of an application’s basic
blocks [SarnSS].

Beyond this effect, there were statistically signiﬁcant
differences among programs. On average, programs
which contained a signiﬁcant percentage of ﬂoating
point operations incurred less overhead. On the DEC—
MIPS the mean overhead for ﬂoating point intensive
benchmarks is 2.5%, compared to a mean of 5.6% for
the remaining benchmarks. All of our benchmarks are

210

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

DEC-MIPS DEC-ALPHA
Fault Protection Reserved Instruction Fault Fault Protection
Benchmark Isolation Overhead Register Count Isolation Isolation Overhead
Overhead Overhead Overhead Overhead Overhead
(predicted)

052. alvinn FP 1.4% 33.4% —0.3% 19.4% 0.2% 8.1% 35.5%
bps FP 5.6% 15.5% -0.1% 8.9% 5.7% 4.7% 20.3%
cholesky FP 0.0% 22.7% 0.5% 6.5% 4.5% 0.0% 9.3%
026 . compress INT 3.3% 13.3% 0.0% 10.9% 4.4% 4.3% 0.0%
056.ear FP —1.2% 19.1% 0.2% 12.4% 2.2% 3.7% 18.3%
023 . eqntott INT 2.9% 34.4% 1.0% 2.7% 2.2% 2.3% 17.4%
008 . espresso INT 12.4% 27.0% —1.6% 11.8% 10.5% 13.3% 33.6%
001 .gcc1.35 INT 3.1% 18.7% -9.4% 17.0% 8.9% NA NA

022.11 INT 5.1% 23.4% 0.3% 14.9% 11.4% 5.4% 16.2%
locus INT 8.7% 30.4% 4.3% 10.3% 8.6% 4.3% 8.7%
mp3d FP 10.7% 10.7% 0.0% 13.3% 8.7% 0.0% 6.7%
psgrind INT 10.4% 19.5% 1.3% 12.1% 9.9% 8.0% 36.0%
ch PF 05% 27.0% 2.0% 8.8% 1.2% -0.8% 12.1%
072 . sc INT 5.6% 11.2% 7.0% 8.0% 3.8% NA NA

tracker INT -0.8% 10.5% 0.4% 3.9% 2.1% 10.9% 19.9%
water FP 0.7% 7.4% 0.3% 6.7% 1.5% 4.3% 12.3%

| Average I 4.3% | 21.8% | 0.4% | 10.5% | 5.0% I 4.3% | 17.6% ‘I

 

Table 1: Sandboxing overheads for DEC—MIPS and DEC—ALPHA platforms. The benchmarks 001.gcc1.35 and
072.sc are dependent on a pointer size of 32 bits and do not compile on the DEC-ALPHA. The predicted fault
isolation overhead for cholesky is negative due to conservative interlocking on the MIPS ﬂoatingvpoint unit.

compute intensive. Programs that perform signiﬁcant
amounts of I/O will incur less overhead.

5.2 Fault Domain Crossing

We now turn to the cost of cross—fault—domain RPC.
Our RPC mechanism spends most of its time saving
and restoring registers. As detailed in Section 4, only
registers that are designated by the architecture to be
preserved across procedure calls need to be saved. In
addition, if no instructions in the callee fault domain
modify a preserved register then it does not need to be
saved. Table 2 reports the times for three versions of
a NULL cross—fault—domain RPC. Column 1 lists the
crossing times when all data registers are caller saved.
Column 2 lists the crossing times when the preserved
integer registers are saved. Finally, the times listed in
Column 3 include saving all preserved ﬂoating point
registers. In many cases crossing times could be further
reduced by statically partitioning the registers between
domains.

For comparison, we measured two other calling
mechanisms. First, we measured the time to perform a
C procedure call that takes no arguments and returns
no value. Second, we sent a single byte between two
address spaces using the pipe abstraction provided by

211

the native operating system and measured the round-
trip time. These times are reported in the last two
columns of Table 2. On these platforms, the cost
of cross—address—space calls is roughly three orders of
magnitude more expensive than local procedure calls.

Operating systems with highly optimized RPC im—
plementations have reduced the cost of cross-address-
space RPC to within roughly two orders of magni—
tude of local procedure calls. On Mach 3.0, cross—
address-space RPC on a 25Mhz DECstation 5000/200
is 314 times more expensive than a local procedure
call [BerQBl. The Spring operating system, running on
a 40Mhz SPARCstationQ, delivers cross—address—space
RPC that is 73 times more expensive than a local leaf
procedure call [HK93]. Software enforced fault isola«
tion is able to reduce the relative cost of cross-fault-
domain RPC by an order of magnitude over these sys-
tems.

5.3 Using Fault Domains in POSTGRES

To capture the effect of our system on application
performance, we added software enforced fault do
mains to the POSTGRES database management system,
and measured POSTGRES running the Sequoia 2000
benchmark [SFGMQ3]. The Sequoia '2000 benchmark

 

 

 

 

 

 

 

 

Cross FaultADomain RFC
Platform Caller Save Save C Pipes
Save Integer Integer+Float Procedure
Registers Registers Registers Call
DEC~MIPS 1.11ps 1.81ps 2.83m 0.10/4s 204.72ns
DEC—ALPHA 0175/15 1.35/5 lSOns 0.06ps 227.88ps

 

 

 

 

 

Table ‘2: Cross-faultrdomain crossing times.

 

 

 

 

 

 

 

 

 

Sequoia 2000 Untrusted Software—Enforced Number DEC—MIPS—PIPE
Query Function Manager Fault Isolation Cross—Domain Overhead
Overhead Overhead Calls (predicted)
Query 6 1.4% 1.7% 60989 18.6%
Query 7 5.0% 1.8% 121986 386%
Query 8 9.0% 2.7% 121978 312%
Query 10 9.6% 5.7% 1427024 31.9%

 

 

 

 

Table 3: Fault isolation overhead for POSTGRES running Sequoia 2000 benchmark.

contains queries typical of those used by earth scien—
tists in studying the climate. To support these kinds
of non~traditional queries, POSTGRES provides a. user
extensible type system. Currently, userrdeﬁned types
are written in conventional programming languages,
such as C, and dynamically loaded into the database
manager. This has long been recognized to be a serious
safety problem[St088].

Four of the eleven queries in the Sequoia 2000 bench-
mark make use of user—deﬁned polygon data types. We
measured these four queries using both unprotected
dynamic linking and software—enforced fault isolation.
Since the POSTGRES code is trusted, we only sand—
boxed the dynamically loaded user code. For this
experiment, our cross-fault—domain RFC mechanism
saved the preserved integer registers (the variant cor-
responding to Column 2 in Table 2). In addition, we
instrumented the code to count the number of cross-
fault-domain RFCs so that we could estimate the per
formance of fault isolation based on separate address
spaces.

Table 3 presents the results, Untrusted user—deﬁned
functions in POSTGRES use a separate calling mecha-
nism from built—in functions. Column 1 lists the over—
head of the untrustcd function manager Without soft-
ware enforced fault domains. All reported overheads in
Table 3 are relative to original POSTGRES using the un—
trusted function manager. Column 2 reports the mea~
sured overhead of software enforced fault domains. Us—
ing the number of cross—domain calls listed in Column 3
and tho DEC*MIPS—I‘IPE time reported in Table 2, Col—
umn 4 lists the estimated overhead using conventional
hardware address spaces.

212

5.4 Analysis

For the POSTGRES experiment software encapsulation
provided substantial savings over using native operat-
ing system services and hardware address spaces. In
general, the savings provided by our techniques over
hardware—based mechanisms is a function of the per—
centage of time spent in distrusted code (Q), the per-
centage of time spent crossing among fault domains
(2‘6), the overhead of encapsulation (h), and the ratio,
r, of our fault domain crossing time to the crossing
time of the competing hardware-based RPC mecha—
nism.

savings = (1 — 7°)t‘C -— htd

Figure 5 graphically depicts these trade—offs. The X
axis gives the percentage of time an application spends
crossing among fault domains. The Y axis reports the
relative cost of software enforced fault-domain cross—
ing over hardware address spaces. Assuming that the
execution time overhead of encapsulated code is 4.3%,
the shaded region illustrates when software enforced
fault isolation is the better performance alternative.

Softwarevenforccd fault isolation becomes increas—
ingly attractive as applications achieve higher degrees
of fault isolation (see Figure 5). For example, if an ap-
plication spends 30% of its time crossing fault domains,
our RPC mechanism need only perform 10% better
than its competitor, Applications that currently spend
as little as 10% of their time crossing require only a
39% improvement in fault domain crossing time As
reported in Section 52, our crossing time for the DEC-
MIPS is Hons and for the DEC—ALPHA UTE/is. Hence,

 

 

Crossing Time Relative to
Existing RFC

 

:9 HP :9 e9
ementage of Execution Time Spent Crossing

 

 

 

Figure 5: The shaded region represents when soft~
ware enforced fault isolation provides the better per—
formance alternative. The X axis represents per
centage of time spent crossing among fault domains
(16). The Y axis represents the relative RPC crossing
speed (7‘). The curve represents the break even point:
(1—7')t,; = htd. In this graph, h = 0.043 (encapsulation
overhead on the DEC~MIPS and DEC-ALPHA).

for this latter example, a hardware address space cross—
ing time of 1.80m on the DEC—MIPS and 1.23/15 on the
DEC~ALPHA would provide better performance than
software fault domains. As far as we know, no pro—
duction or experimental system currently provides this
level of performance.

Further, Figure 5 assumes that the entire applica-
tion was encapsulated. For many applications, such as
POSTGRES, this assumption is conservative. Figure 6
transforms the previous ﬁgure, assuming that 50% of
total execution is spent in distrusted extension code.

Figures 5 and 6 illustrate that software enforced
fault isolation is the best choice whenever crossing
overhead is a significant proportion of an applica-
tion’s execution time. Figure 7 demonstrates that
overhead due to software enforced fault isolation re—
mains small regardless of application behavior. Fig—
ure 7 plots overhead as a function of crossing behavior
and crossing cost. Crossing times typical of vendor-
supplied and highly optimized hardware—based RPC
mechanisms are shown. The graph illustrates the rel—
ative performance stability of the software solution.
This stability" allows system developers to ignore the
performance effect of fault isolation in choosing which
modules to place in separate fault domains.

6 Related Work

Many systems have considered ways of optimizing
RPC performance [vaT88, TASS, Bla90. SB90, HK93,
BALL90, BALL91]. Traditional RFC systems based

 

100%
90%
80%
70%
60%

40%

Crossing Time Relative 10
Existing RPC
u.
§

 

Percentage of Execution Time Spent Crossing

 

 

Figure 6: The shaded region represents when soft~
ware enforced fault isolation provides the better per-
formance alternative. The X axis represents per-
centage of time spent crossing among fault domains
(136). The Y axis represents the relative RPC crossing
speed ('r'). The curve represents the break even point:
(l—r)tc = htd. In this graph, h = 0.043 (encapsulation
overhead on the DEC—MIPS and DEC—ALPHA).

 

 

 

 

 

 

 

100% .
a" Ultrix 4.2 Context Switch
8
d3 80% — _
E
a.
U}
0
g 60% e _
'E—<
E
g 40% e —
é DECstation 5000
3 Hardware Minimum
00
20% — _
t
*- Software
a?
0% l
0 1O 20
# Crossings/Millcsecond

 

Figure 7: Percentage of time spent in crossing code
versus number of fault domain crossings per millisec-
ond on the DECeMIPS. The hardware minimum cross—
ing number is taken from a crossvarchitectural study
of context switch times [ALBL91]. The Ultrix 4.2 con-
text switch time is as reported in the last column of
Table 2.

213

 

on hardware fault isolation are ultimately limited by
the minimal hardware cost of taking two kernel traps
and two hardware context switches. LRPC was one
of the ﬁrst RPC systems to approach this limit, and
our prototype uses a number of the techniques found
in LRPC and later systems: the same thread runs in
both the caller and the callee domain, the stubs are
kept as simple as possible, and the crossing code jumps
directly to the called procedure, avoiding a dispatch
in the callee domain. Unlike these systems, software—
based fault isolation avoids hardware context switches,
substantially reducing crossing costs.

Address space identiﬁer tags can be used to reduce
hardware context switch times. Tags allow more than
one address space to share the TLB; otherwise the
TLB must be ﬂushed on each context switch. It was
estimated that 25% of the cost of an LRPC on the
Fireﬂy (which does not have tags) was due to TLB
misses[BALL90]. Address space tags do not, however,
reduce the cost of register management or system calls,
operations which are not scaling with integer perfor-
mance[ALBL91]. An important advantage of software—
based Jfault isolation is that it does not rely on specialv
ized architectural features such as address space tags.

Restrictive programming languages can also be used
to provide fault isolation. Pilot requires all kernel,
user, and library code to be written in Mesa, 3 strongly
typed language; all code then shares a single address
space [RDII+80]. The main disadvantage of relying on
strong typing is that it severely restricts the choice
of programming languages, ruling out conventional
languages like C, C++, and assembly. Even with
strongly—typed languages such as Ada and Modula—3,
programmers often find they need to use loopholes in
the type system, undercutting fault isolation. In con—
trast, our techniques are language independent.

Deutsch and Grant built a system that allowed
user—deﬁned measurement modules to be dynamically
loaded into the operating system and executed directly
on the processor [DG71]. The module format was a
stylized native object code designed to make it easier
to statically verify that the code did not violate pro—
tection boundaries.

An interpreter can also provide failure isolation. For
example. the BSD UNIX network packet ﬁlter utility
deﬁnes a language which is interpreted by the operat-
ing system network driver. The interpreter insulates
the operating system from possible faults in the cus—
tomization code. Our approach allows code written in
any programming language to be safely encapsulated
(or rejected if it is not safe), and then executed at near
full speed by the operating system.

Anonymous RFC exploits 64-bit address spaces to
provide low latency RFC and probabilistic fault iso—
lation [YBA93]. Logically independent domains are

214

placed at random locations in the same hardware ad»
dress spacer Calls between domains are anonymous,
that is, they do not reveal the location of the caller
or the callee to either side. This provides probabilis—
tic protection , it is unlikely that any domain will
be able to discover the location of any other domain
by malicious or accidental memory probes. To pre»
serve anonymity, a cross domain call must trap to pro-
tected code in the kernel; however, no hardware con~
text switch is needed.

7 Summary

We have described a software-based mechanism for
portable, programming language independent fault
isolation among cooperating software modules. By
providing fault isolation within a single address space,
this approach delivers crossefaultrdomain communica
tion that is more than an order of magnitude faster
than any RPC mechanism to date.

To prevent distrusted modules from escaping their
own fault domain, we use a software encapsulation
technique, called sandboxing, that incurs about 4%
Despite this overhead in
executing distrusted code, software—based fault isola-
tion Will often yield the best overall application per-
formance. Extensive kernel optimizations can reduce
the overhead of hardware-based RPC to within a fac-
tor of ten over our software—based alternative. Even
in this situation, software—based fault isolation will be
the better performance choice whenever the overhead
of using hardware—based RPC is greater than 5%.

execution time overhead.

8 Acknowledgements

We thank Brian Bershad, Mike Burrows, John Hen-
nessy, Peter Kessler, Butler Lampson, Ed Lazowska,
Dave Patterson, John Ousterhout, Oliver Sharp,
Richard Sites, Alan Smith and Mike Stonebraker for
their helpful comments on the paper. Jim Larus pro-
vided us with the proﬁling tool qpt. We also thank
Mike Olson and Paul Aoki for helping us with POST—
GRES.

References

[ACD74] TL. Adam, KM. Chandy, and JR. Dickson.
A comparison of list schedules for parallel pro-
cessing systems. Communications of the ACM,

17(12):685—690, December 197/1.

[ALBUM] Thomas Anderson, Henry Levy, Brian Ber—
shad, and Edward Lazowska. The Interaction

of Architecture and Operating System Design.

[A5591]

[ASUSG]

[BALLQO]

[BALL91]

[Ber93]

[BL92]

[BlaQO]

[1m 84]

[Cla92]

[DG71]

[Dis]

[Dys92]

[FP93]

[H092]

111 Proceedings of the 4th International Confer-
ence on Architectural Supportfor Programming
Languages and Operating Systems, pages 108—
120, April 1991.

Administrator: National Computer Graphics
Association. SPEC Newsletter, 3(4), December
1991.

Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ull-
man. Compilers, Principles, Techniques, and
Tools. Addison—Wesley Publishing Company,
1986.

Brian Bershad, Thomas Anderson, Edward La-
zowska, and Henry Levy. Lightweight Remote
Procedure Call. ACM Transactions on Com-
puter Systems, 8(1), February 1990.

Brian Bershad, Thomas Anderson, Edward La~
zowska, and Henry Levy. User-Level Interpre-
cess Communication for Shared~Memory Mul-
tiprocessors. ACM Transactions on Computer
Systems, 9(2), May 1991.

Brian Bershad, August 1993. Private Commu—
nication.

Thomas Ball and James R. Larus. Optimally
proﬁling and tracing. In Proceedings of the
Conference on Principles of Programming Lan-
guages, pages 59‘70, 1992.

David Black. Scheduling Support for ConcuI~
rency and Parallelism in the Mach Operating
System. IEEE Computer, 23(5):35 43, May
1990.

Andrew Birrell and Bruce Nelson. Implement-
ing Remote Procedure Calls. ACM Transac-
tions on Computer Systems, 2(1):?19‘59, Febru‘
ary 1984.

.1.D. Clark. lVindow Programmer’ Guide To
OLE/DUE, Prentice—Hall, 1992.

L. P. Deutsch and C. A. Grant. A ﬂexible mea~
surement tool for software systems. In IFIP
Congress, 1971.

Digital Equipment Corporation. Ultriz 114.2

Pixie Manual Page.

Peter Dyson. Xtensions for Xpress: Modular
Software for Custom Systems. Seybold Report
on Desktop Publishing, 6(10):1—‘.’.1, June 1992.

Kevin Fall and Joseph Pasquale. Exploiting in—
kernel data paths to improve I/O throughput
and CPU 3. vailability. In Proceedings of the
1993 Winter USENIX Conference, pages 327—
333, January 1993.

Keiran Harty and
David Cheriton. Application—controlled physi-
cal memory using external page—cache manage—
ment. In Proceedings of the 5th International
Conference on Architectural Support for Pro-
gramming Languages and Operating Systems,
October 1992.

215

[11K93]

[HKM+88]

[Int86]

[JRTSS]

[K ar89]

[K1886]

[LB92]

[McF89]

[MJ93]

[M RA87]

[P1190]

[RDH+ 80]

Graham Hamilton and Panos Kougiouris. The
Spring nucleus: A microkernel for objects. In
Proceedings of the Summer USENIX Confer-
cncc, pages 1477159, June 1993.

J. Howard, M. Kazar, S. Menees, D. Nichols,
M. Satyanarayanan, R. Sidebotham, and
M. West. Scale and Performance in 3. Dis-
tributed File System. ACM Transactions on
Computer Systems, 6(1):51—82, February 1988.

Intel Corporation, California.
Intel 80386 Programmer’s Reference Manual,
1986.

Michael B. Jones, Richard F. Rashid, and
Mary R. Thompson. Matchmaker: An in-
terface speciﬁcation language for distributed
processing. In Proceedings of the 12th ACM
SIGACT-SIGPLAN Symposium on Principles
of Programming Languages, pages 225435,
January 1985.

Santa Clara,

Paul A. Karger. Using Registers to Optimize
Cross—Domain Call Performance. In Proceed-
ings of the 3rd International Conference on
Architectural Support for Programming Lan-
guages and Operating Systems, pages 1947204.
April 3~6 1989.

Steven R. Kleiman. Vnodes: An Architecture
for Multiple File System Types in SUN UNIX.
In Proceedings of the 1986 Summer USENIX
Conference, pages 238—247, 1986.

James R. Larus and Thomas Ball. Rewrit-
ing executable ﬁles to measure program be—
havior. Technical Report 1083, University of
Wisconsin-Madison, March 1992.

Scott McFarling. Program optimization for
instruction caches. In Proceedings of the In:
ternational Conference on Architectural Sup—
port for Programming Languages and Operat-
ing Systems, pages 183—191, April 1989.

Steven McCanne and Van lacobsen. The
BSD Packet Filter: A New Architecture for
User—Level Packet Capture. In Proceedings of
the 1993 Winter USENIX Conference, January
1993.

l. C. Mogul, R. F. Rashid, and M. J. Ac-
cetta. The packet ﬁlter: An cﬂicient mecha—
nism for user—level network code. In Proceed-
ings of the Symposium on Operating System
Principles, pages 39—51, November 1987.

Karl Pettis and Robert C. Hansen. Proﬁle
guided code positioning. In Proceedings of
the Conference on Programming Language De-
sign and Implementation, pages 16—27, White
Plains, New York, June 1990. Appeared as
SIGPLAN NOTICES 25(6).

David D. Redell, Yogen K. Dalal, Thomas R.
Horsley, Hugh C. Lauer, William C. Lynch,

[Sam88]

[5390]

[501693]

[SFGMQS]

[St087]

[St088]

[SWG91]

[TAss]

[Thiﬁz]

[VCGSQZ]

[VVSTSB]

[Web93]

[YBA93]

Paul R. McJones, Hal G. Murray, and
Stephen C. Purcell. Pilot: An Operating Sys-
tem for a Personal Computer. Communications
of the A01”, 23(2):81~92, February 1980.

A. Dain Samples. Code reorganization for in
struction caches. Technical Report UCB/CSD
88/447. University of California, Berkeley, 0C,
tober 1988.

Michael Schroeder and Michael Burrows. Per-
formance of Fireﬂy RPC. ACM I‘mnsac»
tions on Computer Systems, 8(1):1—17, Febru-
ary 1990.

Richard L. Sites, Anton Chernoff, Matthew B.
Kirk, Maurice P. Marks, and Scott G. Robin-
son. Binary translation. Communications of
the ACM, 36(2):69—81, February 1993.

M. Stonebral-zer, J. Frew, K. Gardels, and
.I. Meridith. The Sequoia 2000 Benchmark.
In Proceedings of the ACM SIGMOD Inter-
national Conference on Management of Data,
May 1993.

Michael Stonebraker. Extensibility in POST~
GRES. IEEE Database Engineering, Septem-
ber 1987.

Michael Stonebraker. Inclusion of new types in
relational data base systems. In Michael Stone-
braker, editor, Readings in Database Systems,
pages 480—487. Morgan Kaufmann Publishers,
Inc., 1988.

J. P. Singh, W. Weber, and A. Gupta.
Splash: Stanford parallel applications for
shared—memory. Technical Report CSL—TR—Sl—
469, Stanford, 1991.

Shin—Yuan Tzou and David P. Anderson. A
Performance Evaluation of the DASH Message-
Passing System. Technical Report UCB/CSD
88/452, Computer Science Division, University
of California, Berkeley, October 1988.

Thinking Machines Corporation. CM—5 Net-
work Interface Programmer’s Guide, 1992.

T. von Eicken, I). Culler, S. Goldstein, and
K. Schauser. Active Messages: A Mechanism
for Integrated Communication and Computa—
tion. In Proceedings of the 19th Annual Sym-
posium on Computer Architecture, 1992.

Robbert van Renesse, Hans van Staveren, and
Andrew S. Tanenbaum. Performance of the
World’s Fastest Distributed Operating System.
Operating Systemic Review, 22(1):25734, Octo—
ber 1988.

Neil Webber. Operating System Support for
Portable Filesystem Extensions. In Proceed-
ings of the 1993 Winter USENIX Conference,
January 1993.

Curtis Yarvin. Richard Bnkowski, and Thomas
Anderson. Anonymous RFC: LOW Latency

216

Protection in a 64—Bit Address Space. In Pro-
ceedings of the Summer USENIX Conference,
June 1993.
