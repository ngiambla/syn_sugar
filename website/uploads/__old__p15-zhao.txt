Accelerating Binarized Convolutional Neural Networks

with Software-Programmable FPGAs

Ritchie Zhao1,, Weinan Song2, Wentao Zhang2, Tianwei Xing3, Jeng-Hau Lin4,

Mani Srivastava3, Rajesh Gupta4, Zhiru Zhang1,

1School of Electrical and Computer Engineering, Cornell University, USA

2School of Electronics Engineering and Computer Science, Peking University, China

3Department of Electrical Engineering, University of California Los Angeles, USA

4Department of Computer Science and Engineering, University of California San Diego, USA

{rz252, zhiruz}@cornell.edu

Abstract
Convolutional neural networks (CNN) are the current state-
of-the-art for many computer vision tasks. CNNs outperform
older methods in accuracy, but require vast amounts of com-
putation and memory. As a result, existing CNN applications
are typically run on clusters of CPUs or GPUs. Research
on FPGA acceleration of CNN workloads has achieved re-
ductions in power and energy consumption. However, large
GPUs outperform modern FPGAs in throughput, and the ex-
istence of compatible deep learning frameworks give GPUs
a signicant advantage in programmability.

Recent work in machine learning demonstrates the po-
tential of very low precision CNNs  i.e., CNNs with bi-
narized weights and activations. Such binarized neural net-
works (BNNs) appear well suited for FPGA implementation,
as their dominant computations are bitwise logic operations
and their memory requirements are greatly reduced. A com-
bination of low-precision networks and high-level design
methodology may help address the performance and pro-
ductivity gap between FPGAs and GPUs. In this paper, we
present the design of a BNN accelerator that is synthesized
from C++ to FPGA-targeted Verilog. The accelerator out-
performs existing FPGA-based CNN accelerators in GOPS
as well as energy and resource efciency.

Introduction

1.
Deep convolutional neural networks (CNNs) have become
an important class of machine learning algorithms widely
used in computer vision and articial intelligence. While
CNNs have been known to researchers for decades, they
were popularized after demonstrating high accuracy at the

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for prot or commercial advantage and that copies bear this notice and the full citation
on the rst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specic permission and/or a
fee. Request permissions from Permissions@acm.org.
FPGA 17, February 22-24, 2017, Monterey, CA, USA
c(cid:13) 2017 ACM. 978-1-4503-4354-1/17/02. . . $15.00
http://dx.doi.org/10.1145/3020078.3021741

2012 ImageNet recognition challenge [15]. Subsequently,
CNNs have become the state-of-the-art for image classica-
tion, detection, and localization tasks. Research in CNNs and
other areas of deep learning continues at a rapid pace, with
hundreds of new papers published each year introducing new
models and techniques. One indicator of this rate of progress
is the improvement in the top-5 accuracy of the ImageNet
competition winner over the years: 84.7% in 2012 [15] to
96.4% in 2015 [10].

One challenge to the widespread deployment of CNNs is
their signicant demands for computation and storage ca-
pacity. The VGG-19 network, for instance, contains over
140 million oating-point (FP) parameters and performs
over 15 billion FP operations to classify one image [22].
Consequently, the training and inference of modern CNNs
is almost exclusively done on large clusters of CPUs and
GPUs [4]. One additional benet of such platforms is the
availability of compatible deep learning frameworks such
as Caffe [12], Theano [24], or TensorFlow [9], which allow
users to make use of the latest models or to train a custom
network with little engineering effort.

While CPU and GPU clusters are currently the go-to plat-
forms for CNN and many other machine learning appli-
cations, a customized hardware solution on FPGA can of-
fer signicant improvements in energy efciency and power
dissipation. These factors may be critical in enabling the
increased use of CNNs in low-power settings such as un-
manned drones or embedded computing. Recent work by
Microsoft has even explored cost-effective acceleration of
deep learning on FPGAs at datacenter scale [18]. There are
also efforts in the academic community on FPGA-based
CNN accelerators [27, 19] as well as tools for generating
them automatically [23, 26]. Yet, there remains a sizable
gap between GPU and FPGA platforms in both CNN perfor-
mance and design effort. The latter is especially distressing
given the rate of algorithmic innovation in deep learning 
an FPGA-based CNN accelerator (or CNN design compiler)
is unlikely to support the most up-to-date models, putting
them at a severe competitive disadvantage.

15We observe two trends which may help overcome these
obstacles. The rst is a series of recent papers in the machine
learning community regarding very-low-precision CNNs.
Networks with binary weights [6], or binary weights and ac-
tivations [7, 21] have in certain cases demonstrated accuracy
comparable to full precision nets. Such binarized neural net-
works (BNNs) may be the key to efcient deep learning on
FPGA. Binarization reduces storage and memory bandwidth
requirements, and replace FP operations with binary opera-
tions which can be very efciently performed on the LUT-
based FPGA fabric.

Concerning the cost and effort of FPGA implementation,
we see a steady improvement in FPGA design automation
tools over the past decade. High-level synthesis (HLS) tools
such as Xilinx Vivado HLS [5] and LegUp [1] enable a user
to write code in a high-level programming language, then al-
gorithmically compile that code down to a register-transfer
level (RTL) design specication. More recent tools such as
Intel FPGA SDK for OpenCL [8] and Xilinx SDSoC [13] of-
fer further automation features for generating the hardware-
software interface and on-chip memory network. In the con-
text of deep learning, these tools have the potential to criti-
cally reduce time-to-market on new accelerator designs and
thus reduce the aforementioned innovation gap.

In this paper we present the design of a BNN accelerator
for FPGAs. In order to take full advantage of the binarized
values and operations, our design differs in multiple aspects
from CNN accelerators in literature. Our specic contribu-
tions are as follows:
 To our best knowledge, we are the rst to study FPGA
acceleration for very low precision CNNs. Compared to
their full-precision counterparts, such networks are po-
tentially a better t for the LUT-based fabric and limited
on-chip storage in modern FPGAs.
 We employ an HLS design methodology for productive
development of our FPGA-based BNN accelerator. Ex-
isting HLS work has examined loop ordering, unrolling,
and local buffering for CNNs [27]. Our HLS implemen-
tation leverages these optimizations, and further propose
novel BNN-specic hardware constructs to ensure full
throughput and hardware utilization across the different
input feature sizes.
 We implement our BNN classier on a low-cost FPGA
development board (ZedBoard) and show promising im-
provements over CPU and embedded GPU baselines as
well as existing FPGA accelerators. Our source code is
publicly available on the authors websites.

The rest of this paper is organized as follows: Section 2
gives a primer on CNNs and BNNs; Section 3 describes our
BNN accelerator design; Section 4 provides some details on
our HLS code; Section 5 reports our experimental ndings,
Section 6 reviews previous work on FPGA-based CNN ac-
celerators; and we conclude the paper in Section 7.

2. Preliminaries
In this section we briey review the basic principles and
terminology of CNNs, the differences between a CNN and
BNN, and the specic CIFAR-10 BNN model that our ac-
celerator will target.

2.1 Convolutional Neural Network Primer
A CNN is a machine learning classier that typically takes
in a multi-channel image and produces the probabilities of
that image belonging to each output class. A typical CNN
consists of a pipeline of connected layers. Each layer takes
as input a set of feature maps (fmaps), performs some com-
putation on them, and produces a new set of fmaps to be fed
into the next layer. The input fmaps of the rst layer are the
channels of the input image. Layers may require congura-
tion values known as parameters, which must rst be de-
termined by training the CNN ofine on pre-classied data.
Once the parameters are nalized, the CNN can be deployed
for inference  the classication of new data points. For
most practical machine learning applications, the rst-class
concerns are the accuracy and execution time of online clas-
sication. This paper will thus focus on accelerating the in-
ference task without compromising accuracy.

Figure 1: Comparison of CNNs and BNNs  Left: the or-
der of operations in a CNN for a conv and pool layer. Right:
the (modied) order of operations in the BinaryNet BNN [7].
Pooling is performed early and a batch normalization pre-
cedes the binarization to minimize information loss. Biases
have been removed from the BNN.

Below we describe three layer types which are found in

most CNNs, including our CIFAR-10 BNN model.

ConvolutionAdd BiasNon-linearityPoolMreal fmapsNreal fmapsNreal fmapsConvolutionBatch NormBinarizeMbinary fmaps2Ninteger fmapsNbinary fmaps2PoolCNNBNN16Convolutional (conv) layers convolve each input fmap
with a K  K weight lter. The conv results are summed,
added with a bias, and passed through a non-linearity func-
tion (such as ReLU or sigmoid) to produce a single output
fmap. In this paper, we assume conv layers pad the input
fmaps at the borders to produce output fmaps of the same
size. Equation (1) below shows the operation of a conv layer
with M input fmaps x1, ..., xM , N output fmaps y1, ..., yN ,
and non-linearity f.

M(cid:88)

m=1

M(cid:88)

yn = f (

xm  wn,m + bn)

(1)

The parameters of this conv layer are M  N  K  K
weights and N biases.
Pooling layers maps each input fmap to an output fmap
whose every pixel is the max/mean of a K  K window of
input pixels. Unlike conv layers the windows do not overlap,
and the output fmaps are K times smaller in each dimension.
Pooling layers are inserted throughout a CNN to gradually
reduce the size of the intermediate feature maps.
Dense or fully-connected (FC) layers take an input vec-
tor of 11 feature maps (pixels) and perform a dot prod-
uct with a weight vector. The result is added to a bias and
passed through a non-linearity to produce a single 11 out-
put. Equation (2) below shows the operation of a FC layer
with M input pixels, N output pixels, and non-linearity f.

yn = f (

xmwn,m + bn)

(2)

m=1

The parameters are M  N weights and N biases.
2.2 Binarized Neural Networks
A BNN is essentially a CNN whose weights and fmap pixels
are binarized to -1 or +1; they can be seen as an extreme
example of the quantized, reduced-precision CNN models
commonly used for hardware acceleration. In this paper we
focus on an architecture developed by Courbariaux et al.
in [6] and later rened in [7]. The rst paper binarizes only
the weights while the follow-up binarizes both weights and
fmaps. We focus on the latter version and refer to it as
the BinaryNet architecture/model. This architecture achieves
near state-of-the-art results on both CIFAR-10 and SVHN
datasets at time of publication. Other more recent work on
low precision networks promise accuracy close to state of
the arts on ImageNet [16, 28].

In the BinaryNet model, the weights and outputs of both
conv and FC layers are binarized using the Sign function
(i.e., positive weights are set to +1 and negatives to -1).
Figure 1 illustrates the ow of data through a conv and
pooling layer in both a CNN and a BNN. For the CNN, the
order of operations matches Equation (1) and the fmaps are
real-valued at all times. In the BNN, the feature maps go
from binary to integer (after convolution) until it is binarized

again. Biases have been removed (see Section 3.1). Pooling
in the BNN is always performed on the integer data.

The BNN also introduces a new layer type  Batch nor-
malization [11] layers reduce the information lost during
binarization by linearly shifting and scaling the input dis-
tribution to have zero mean and unit variance. This reduces
quantization error compared to an arbitrary input distribu-
tion. 1 The transformation is given in Equation (3) below,

y =

x  

2 + 

 + 

(3)

where x and y are input and output, respectively,  and 
are statistics collected over the training set,  and  are
trained parameters, and  is to avoid round-off problems.
During inference, all parameters are xed, so we need only
be concerned with efciently applying Equation (3) to each
input fmap pixel. Each output fmap require its own set of
batch norm parameters.

The primary advantages of BNNs over their higher preci-

sion counterparts are twofold:

1. The convolution operation in Equation (1) (which nom-
inally requires a K  K element multiply-accumulate)
can now be implemented as a bitwise XNOR between
two K  K bit vectors and a popcount. This is highly
relevant to FPGA design, as these operations can be im-
plemented very efciently in the logic fabric.

2. Assuming comparable numbers of feature maps and FC
layer units, binarizing weights and fmaps greatly re-
duces their memory size. This is again compelling for
FPGAs as existing FPGA accelerators are typically con-
strained in performance by a combination of on-chip stor-
age space and off-chip memory bandwidth.

2.3 CIFAR-10 BNN Model
The CIFAR-10 dataset [14] contains sixty thousand 3232
3-channel images consisting of photos taken of real world
vehicles and animals. The images come from 10 classes
(airplane, truck, cat, etc.) and are divided into a training set
of 50000 and a test set of 10000.
The BinaryNet architecture consists of six conv layers
followed by three FC layers. All conv layers use 33 l-
ters and edge padding, and all conv/FC layers apply batch
norm before binarization. There is a 22 max pooling layer
after the 2nd, 4th, and 6th conv layers. The rst conv layer
is different from the rest: its input is the image, which is
oating-point, not binary; its weights are still binary. The ar-
chitecture is summarized in Table 1; the size of the fmaps
gets smaller deeper into the network, and that the rst two
dense layers contain most of the weights.

1 Batch normalization can also speed up training and regularize the activa-
tions in full-precision CNNs, but this is beyond the scope of this paper.

17Layer

Conv1
Conv2
Pool
Conv3
Conv4
Pool
Conv5
Conv6
Pool
FC1
FC2
FC3
Total

Conv
FC

Input Output Output Output Weight
Bits
Fmaps
3456
3
128
144K
128
128
256
256
256
512
512
8192
1024
1024

Fmaps
128
128
128
256
256
256
512
512
512
1024
1024
10

Bits
128K
128K
32K
64K
64K
16K
32K
32K
8192
1024
1024
10

Dim
32
32
16
16
16
8
8
8
4
1
1
1

288K
576K

1.1M
2.3M

8.0M
1.0M
10K
13.4M
4.36M
9.01M

Table 1: Architecture of the BinaryNet CIFAR-10 BNN
 The weight bits exclude batch norm parameters, whose
total size after optimization (see Section 3.1) is 0.12M bits,
less than 1% of the size of the weights.

Training of the CIFAR-10 BNN model was done using
open-source Python code provided by Courbariaux et al. 2,
which uses the Theano and Lasagne deep learning frame-
works. We reached 11.58% test error out-of-the-box, in line
with their results. Their paper also presents more advanced
training techniques such as stochastic binarization, which
further reduce error rate. We did not use them in this work.
Different training schemes do not affect the inference pass
or the compatibility of our accelerator.

3. FPGA Accelerator Design
In this section, we rst outline how we optimize the Bina-
ryNet model for hardware, then describe the design of our
system and the specic compute units.

3.1 Hardware Optimized BNN Model
As with the design of conventional CNN accelerators, a key
optimization we made to the BNN model is parameter quan-
tization. While the weights are already binarized, the biases
and batch norm parameters are real numbers. During bias
quantization, we noticed that nearly every bias was much
smaller than 1. Given that the inputs have magnitude 1, we
tried setting the biases to zero and observed no effect on ac-
curacy. We then retrained the network with biases removed
from the model, and reached a test error of 11.32%. For the
rest of the paper we use this as the baseline error rate.

A second optimization involved noting that the batch
norm calculation (Equation (3)) is a linear transformation,
and can thus be formulated as y = kx + h, where:
h =   

k =

and

(4)


2 + 

2 + 

2 https://github.com/MatthieuCourbariaux/BinaryNet

This reduces the number of operations and cuts the num-
ber of stored parameters to two. Furthermore, the BNN al-
ways binarizes immediately after batch norm. Thus we do
not need the magnitude of y, only the sign, allowing us scale
k and h by any multiplicative constant. We exploit this prop-
erty during quantization by scaling each k and h to be within
the representable range of our xed-point implementation.
Empirical testing showed that k and h can be quantized to 16
bits with negligible accuracy loss while being a good t for
power-of-2 word sizes. We also quantized the oating point
BNN inputs to 20-bit xed point. Table 2 summarizes the
impact of each algorithmic modication on test error. The
HLS accelerator has the same accuracy as the C++ code.

3.2 Retraining for +1 Edge-Padding
One complication in the BinaryNet model is the interaction
between binarization and edge padding. The model binarizes
each activation to -1 or +1, but each input fmap is edge
padded with zeros, meaning that a convolution can see up
to 3 values: -1, 0, or +1. Thus the BinaryNet model actually
requires some 2-bit operators (though the fmap data can still
be stored in binary form). We managed to modify and retrain
the BinaryNet model to pad with +1, eliminating the zeros
and creating a truly binarized CNN. This +1 padded BNN
achieves a test error of 11.82% in Python and 12.27% in
C++/FPGA, only slightly worse than the original.

For our FPGA implementation we used the 0 padded
BNN as the resource savings of the +1 padded version was
not particularly relevant for the target device.

Source Model
From [7]
Python
Python
Python
C++
C++

-
Default
no-bias
no-bias
no-bias, xed-point
no-bias, xed-point

Padding Test Error

0
0
0
+1
0
+1

11.40%
11.58%
11.32%
11.82%
11.46%
12.27%

Table 2: Accuracy of the BNN with various changes 
no-bias refers to retraining after removing biases from all
layers and fixed-point refers to quantization of the inputs
and batch norm parameters.

3.3 System Architecture
Our system architecture, shown in Figure 2(a), consists of
three compute units, data and weight buffers, a direct mem-
ory access (DMA) system for off-chip memory transfer, and
an FSM controller. The three compute units work on differ-
ent types of layers: the FP-Conv unit for the (non-binary)
rst conv layer, the Bin-Conv unit for the ve binary conv
layers, and the Bin-FC unit for the three binary FC layers. Of
the three, the Bin-Conv and Bin-FC units must handle differ-
ent numbers of input and output fmaps, and (in the case of
Bin-Conv) different fmap sizes.

18(a)

(b)

Figure 2: Architectural diagrams of our BNN accelerator  (a) system-level block diagram showing the three compute
units, buffers, and how the accelerator is connected to the CPU and off-chip memory hierarchy; (b) architecture of the Bin-
Conv unit with input and output parallelization factors fin = 2 and fout = 3. The unit can stream in two words per cycle and
produce three output fmaps per invocation.

The storage of intermediate data in our accelerator differs
from most existing designs. In full-precision CNN accelera-
tors, the size of a single set of fmaps between two layers typ-
ically exceed the size of FPGA on-chip storage. This neces-
sitates the continuous transfer of fmaps to and from off-chip
RAM. However, as Table 1 shows, the size of the largest set
of fmaps in our BNN is only 128K bits, which easily ts on-
chip even in smaller FPGAs. Our design uses two in-out data
buffers A and B of equal size. One layer reads from A and
write its outputs to B; then (without any off-chip data trans-
fers) the next layer can read from B and write to A. Thus,
off-chip memory transfers are only needed for the input im-
age, output prediction, and loading each layers weights.

Unlike the fmaps, there is only enough memory on-chip
to store a portion of a layers weights. Multiple accelerator
invocations may be needed for a layer; in each invocation we
load in a new set of weights and produce a new set of fmaps.
The next invocation produces the next set of fmaps, and etc,
until all output fmaps have been generated and stored in the
on-chip data buffer. Invoking the accelerator requires pass-
ing it arguments such as pointers to the weights, the layer
type and size, the fmap size, and whether pooling should be
applied. Inside the accelerator, the controller decodes these
inputs and coordinates the other modules.

3.4 Compute Unit Architectures
In our accelerator, each compute unit must store binarized
data to the on-chip RAMs at the end of its execution. As
Figure 1 reveals, the rst operation of a conv or FC layer
transforms the binary inputs to integers; we make sure each
unit will also perform the subsequent batch-norm, pooling,
and binarization before writing data out to the buffers. One
of our design goals is to limit the amount of integer-valued
intermediate data buffered inside each compute unit,

FP-Conv  The xed-point conv unit utilizes the well-
known line buffer architecture for 2D convolutions. Because
this unit only targets a single layer, we hardwire it to handle

a 3-channel 3232 input. While the input pixels are 20-bit
xed-point, the weights are binarized, so we can replace
the multiplies in the conv operation with sign inversions.
We fully parallelize across the three input channels: each
cycle we stream in three input pixels, add them to three line
buffers, and compute a 333 convolution. The result is put
through batch norm and binarization to produce one output
bit per cycle. Greater parallelism in this unit is achievable,
but the rst conv layer takes up a very small portion of the
overall runtime, and we focused our efforts elsewhere.

Bin-Conv  The binary conv unit is the most critical
component of the accelerator, as it will be responsible for
the ve binary conv layers which take up the vast major-
ity of the runtime. The unit must maintain high throughput
and resource efciency while handling different input widths
at runtime; our design targets 8, 16, or 32, and can sup-
port larger power-of-two widths with minor changes. To ef-
ciently compute a convolution, multiple rows of input pix-
els need to be buffered for simultaneous access. However,
a standard line buffer (i.e., from video processing applica-
tions) is unsuitable for this task due to two reasons:
1. A line buffer must be sized for the largest input fmap; in
this case it must have a width of 32. This not only causes
buffer under-utilization when the fmap is 16 or 8 wide, it
also leads to loss of throughput, as we can only perform
as many convolutions per cycle as the input width.

2. A line buffer is designed to shift one pixel per cycle
to always store the most recent rows. However, with
binarized in puts we have access to not one but many lines
of new pixels each cycle (for instance, a 32-bit word can
hold 4 lines from an 88 fmap). This radically changes
how we should update the line buffer.

In full precision CNN accelerators, the size problem can be
addressed by tiling, where input fmaps are processed in tiles
which are always the same size. This is unsuitable in a BNN
since the fmaps are typically very small in terms of number

BAData BuffersParamBuffersCompute UnitsControllerDMAFP-ConvBin-ConvBin-FCOff-chip memCPUBitSelfoutConv WeightsfinConvolversVariable-width Line BufferBitSel+Integer buffer+Pooling, Bnorm, Binarizefoutoutput streams19Figure 3: Example usage of the variable-width line buffer  we show how a 32-bit input word is divided up by BitSel and
inserted into the VWLB. The line buffer has height 3 and width 32. Sliding a 33 conv lter across the VWLB produces 32
output pixels, ignoring edge effects. (a) for a 32-wide input fmap, each row of the VWLB stores one line and applying the conv
lter produces one 32-wide output line; (b) for an 8-wide input fmap, each row of the VWLB stores four lines and applying
the conv lter produces four consecutive 8-wide output lines given the mapping of input lines to banks shown.

of bits. One possible solution to the second problem is to
reorganize the data so each bit in an input word comes from
a different fmap, and assign each bit to a separate line buffer.
However, this requires an exorbitant number of line buffers
and is not area efcient.

To address the above issues, we introduce two new mod-
ules: the BitSel module and the variable-width line buffer
(VWLB). Figure 2(b) shows the basic structure of the Bin-
Conv unit, whose execution proceeds in two phases. In
the rst phase, input fmaps from the on-chip buffers are
streamed in on the left side, through the BitSel modules, and
into the VWLBs. The Convolver modules compute the par-
tial conv sums and accumulates them in the integer buffers.
The BitSel is responsible for reordering the input bits so that
the Convolver logic can be agnostic of the fmap width. In
Figure 2(b), fin is the input parallelization factor  the Bin-
Conv unit accepts fin input words per cycle and the data
buffers are partitioned to match this rate. fout is the output
parallelization factor  each Convolver applies fout 3  3
conv lters per cycle to the data in the VWLB and gener-
ates partial sums for fout different fmaps. The rst phase
ends when all input fmaps in the current layer have been
processed. At this point each integer buffer contains a n-
ished conv map. In the second phase we compute max pool-
ing, batch norm, and binarization to produce fout binary out-
put fmaps. Note that max-pooling and binarization are non-
linear operations, so we cannot apply them to partially n-
ished conv maps and accumulate afterwards.

Figure 3 explains the operation of the BitSel and VWLB
in greater detail. The diagram assumes we have a word size
of 32 bits and a 33 conv lter, which requires a VWLB
with three rows and 32 elements per row. We demonstrate
how the VWLB works for input fmap widths 32 and 8 and
ignore edge padding for the sake of simplicity.

1. For a 32-wide input, each word contains exactly one line.
Each cycle, the VWLB shifts up and the new 32-bit line

is written to the bottom row. We can then slide the 33
conv window across the VWLB to generate one 32-bit
line of conv outputs.

2. For an 8-wide input, each word contains four lines. We
split each VWLB row into four banks, and map each
input line to one or more VWLB banks. The mapping is
done in such a way that sliding the conv window across
the VWLB produces four consecutive 8-bit output lines.
Each cycle the VWLB shifts both up and to the left.

The BitSel is responsible for slicing the input word and
mapping the slices to the row banks. Because the smallest
input width is 8, each slice and VWLB bank is sized at 8 bits.
For a 32-wide input, BitSel maps four contiguous 8-bit slices
to the bottom row. For an 8-wide input, the mapping is more
complex, but still highly regular and can be computed in
hardware with just adds and shifts. Each pixel in the output
lines in Figure 3 is an integer conv sum, and each sum is
accumulated at a different location in the integer buffer.

The BitSel and VWLB provides three primary advan-
tages: (1) the VWLB achieves full hardware utilization re-
gardless of input width, (2) a new input word can be buffered
every cycle, and (3) the BitSel deals with various input
widths by itself, allowing the actual buffer and convolution
logic to be xed. Note that the VWLB used in our design dif-
fers from Figure 3 in a few details. First, we have neglected
edge padding. The actual VWLB contains two additional el-
ements per bank to hold horizontal pad bits; vertical padding
is handled by inserting lines of zeros. Second, because the
pad bits are 0 rather than +1 or -1, we must make each ele-
ment in the VWLB two bits instead of one. The conv oper-
ation is performed between the 2-bit data and 1-bit weights,
and can be implemented as sign inversion and accumulate.

Bin-FC  The binary FC unit is comparatively simple.
Each cycle we read in fin data words and an equal number
of weight words. fin here is the input parallelization factor

convline 3line 6line 4line 5line 4line 7line 5line 6line 5line 8line 6line 7Bank 0Bank 3Bank 1Bank 232 total elementsInput width 32line 5line 8line 6line 7(a)Input Wordline 1line 2line 3line 3(b)Input width 8zline 3line 6line 4line 5line 1Line BufferOutput LineconvBitSelBitSel32820just as in Bin-Conv. Because there is no edge padding in an
FC layer the computations can be truly binary. We perform
a dot product between the data and weight words by apply-
ing a bitwise XOR operation and then summing the resulting
bits with a popcount. Similar to the Bin-Conv unit, we accu-
mulate the sum in an integer buffer and apply binarization
after all inputs have been processed. Note that the FC layers
are typically bound by memory bandwidth of the off-chip
connection, rather than the throughput of the accelerator.

Data Buffers  To accommodate multiple reads per cy-
cle, the data buffers are partitioned into fin banks, and fea-
ture maps are interleaved across the different banks. Figure 4
shows an example with fin = 2 and four words per fmap.
The data words are read sequentially by address, so a com-
pute unit always accesses fin consecutive fmaps in parallel.

Figure 4: Example of data buffer banking  The compute
unit and memory system have fin = 2. Each fmap contains
four words which are laid out sequentially. The fmaps are
interleaved across banks, and both Bin-Conv and Bin-FC
benet from this banking.
4. HLS Accelerator Implementation
Figure 5 shows the HLS pseudocode for the front half of
the Bin-Con unit, and demonstrates a key difference between
BNN and CNN hardware design. For a CNN the code typ-
ically loops over an fmap processing one pixel at a time;
key design decisions include loop ordering and unroll fac-
tors (see [27] for a good example). In our BNN accelera-
tor, the basic atom of processing is not a pixel but a word.
The example code is designed to sustain one word per cy-
cle throughput over the entire input feature map set. Each
fmap consists of words per fmap words, a number which
differs between layers. As it processes the input set, the code
updates the weights on each new fmap and accumulates the
conv results in outbuf. We call BitSel and conv inside the
loop to instantiate the BitSel units and conv logic as shown
in Figure 2(b). To increase the number of input streams we
can tile the loop and unroll the inner loop body.

A key design decision here is the input word size, which
controls the level of parallelism across the pixels of an fmap.
To guarantee correctness, words per fmap must be an inte-
ger greater than zero; this constrains the word size to at most
the size of the smallest input fmap (8  8 = 64 bits in our
case). The word size restriction is not a signicant limiting

#pragma HLS pipeline

conv(c, linebuf, wts);

}

// update the weights each time we
// begin to process a new fmap
if (i % words_per_fmap == 0)

wts = weights[i / words_per_fmap];

// read input word, update linebuffer
WordType word = input_data[i];
BitSel(linebuf, word, input_width);

1 VariableLineBuffer linebuf;
2 ConvWeights wts;
3 IntegerBuffer outbuf;
4
5 for (i = 0; i < n_input_words; i++) {
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23 }
Figure 5: HLS pseudocode for part of the Bin-Conv unit
 the pseudocode implements a pipeline which reads and
performs convolution on one input word each cycle. Many
details are left out; the goal is to illustrate how our design
can be expressed in high-level code.

// perform conv across linebuffer
for (c = 0; c < LINE_BUF_COLS; c++) {
#pragma HLS unroll

outbuf[i % words_per_fmap][c] +=

factor in our design, as 64 is already a very large paralleliza-
tion factor (it means we perform 64 convolutions per cycle),
and there are other sources of parallelism to exploit in the
BNN. We chose a word size of 64 bits for the data buffers
and sized each data buffer A and B at 2048 words, which is
just enough to store the largest set of fmaps in the BNN.

We also explored different values for fin and fout in Bin-
Conv. It was observed that both have roughly similar effects
on execution time, but increasing fout has a more severe
effect on total area. fin controls the number of BitSels and
VWLBs while fout controls the number of pooling/batch
norm units and integer buffers. In terms of logic a BitSel
and a pooling/batch norm unit is similar, but each VWLB
contains 32  3 2-bit registers while each integer buffer
contains 32  32 12-bit registers. Thus all else being equal
it is better to increase fin. This result shows the importance
of minimizing the storage of intermediate values and only
committing binarized data to memory.

We use Xilinx SDSoC as the primary design tool for our
BNN application. SDSoC takes as input a software program
with certain functions marked as hardware. It invokes Vi-
vado HLS under the hood to synthesize the hardware por-
tion into RTL. In addition, it automatically generates the data
motion network and DMA necessary for memory transfer
between CPU and FPGA based on the specied software-
hardware partitioning. We selected a DMA engine built for
contiguous memory since it has the highest throughput, and

0123Bank 00123Bank 1fmap0fmap2fmap1fmap3Convolver0Convolver121a neural networks data and weights can be laid out contigu-
ously. We used directives to ensure that data is only trans-
ferred on the rst and last accelerator invocation; weights
are transferred on every invocation.

5. Experimental Results
We evaluate our design on a ZedBoard, which uses a low-
cost Xilinx Zynq-7000 SoC containing an XC7Z020 FPGA
alongside an ARM Cortex-A9 embedded processor. We
make use of Xilinx SDSoC 2016.1 as the primary design
tool, which leverages Vivado HLS and Vivado to perform
the actual HLS compilation and FPGA implementation.
We compared our design against two server-class comput-
ing platforms: an Intel Xeon E5-2640 multicore processor
(CPU) and an NVIDIA Tesla K40 GPU (GPU). We also
compared against an NVIDIA Jetson TK1 embedded GPU
board (mGPU). As BNNs are a recent development, our
baseline applications will not be as well optimized compared
to CNN baselines (where implementations can be found in
frameworks such as Caffe). The CPU and GPU baselines
are adapted from code provided in [7]. The code leverages
Theano, and calls OpenBLAS for CPU and CUDA for GPU.
However, it does not perform bitwise optimizations since
they are not natively supported in Theano, and instead uses
oating-point values binarized to -1 and +1. For the base-
lines we used the BNN model with no biases and k and h,
and on the GPU we always used the largest batch size.

Power measurement is obtained via a power monitor. We
measured 4.5W idle and 4.7W max power on the Zedboard
power suppy line when running our BNN. This indicates the
dynamic power consumption of the FPGA is very low.

Table 3: Comparison of different congurations  Last
row shows the resources available on the device; Runtime
is in milliseconds. * indicates our chosen conguration.

fin

1
2
4
8*
Dev.

LUT

25289
35291
38906
46900
53200

FF

BRAM DSP Runtime

28197
37125
36771
46134
106400

86
87
87
94
140

3
3
3
3
220

17.5
10.8
7.98
5.94

-

Table 3 shows the performance and resource utilization of
our accelerator using different values of fin for the Bin-Conv
and Bin-FC units. All numbers are post place and route.
In our experiments fout is set to 1 for reasons outlined in
Section 4. Performance scaling is clear, though the scaling is
less than unity due to memory transfer and other overheads.
We use fin = 8 for the rest of the experiments.

We compare the performance of our accelerator to the
various baselines in Table 4. As raw throughput depends
heavily on device size, we also show the power consump-
tion and the throughput per Watt. The FPGA design ob-

tains 15.1x better performance and 11.6x better throughput
per Watt over mGPU, which has a similar power envelope.
Against the x86 processor, it achieves a 2.5x speedup. While
the binary conv layers were faster, the FC layers were slower,
which is unsurprising as the FC layers are bound by external
memory bandwidth. Versus GPU, the FPGA is 8.1x worse
in performance. But as expected, it has much lower power
consumption and better throughput per Watt.

To show that the FC layers are indeed limited by memory
bandwidth, we created a design where the FC computations
are removed but the memory transfers are kept. The new
execution time of the FC layers is within 5% that of the
original, demonstrating that there is not much to gain by
further parallelizing them beyond the current design.

Table 4: Performance comparison  Conv1 is the rst FP
conv layer, Conv2-5 are the binary conv layers, FC1-3 are
the FC layers. A  indicates a value we could not measure.
Numbers with * are sourced from datasheets. The last row
shows power efciency in throughput per Watt.

Execution time per image (ms)
mGPU CPU GPU FPGA

Conv1
Conv2-5
FC1-3
Total
Speedup
Power (Watt)
imgs/sec/Watt




90
1.0x
3.6
3.09

0.68
13.2
0.92
14.8
6.1x
95*
0.71

0.01
0.68
0.04
0.73
123x
235*
5.83

1.13
2.68
2.13
5.94
15.1x
4.7
35.8

Table 5 compares our implementation against state-of-
the-art FPGA accelerators found in literature  all num-
bers are retrieved from the respective papers. Note that two
of the comparisons are against larger FPGAs while one
is against the same device. Throughput is shown in giga-
operations-per-second (GOPS), and we count adds and mul-
tiplies following [27]: each binary xor, negation, or addition
counts as one operation. Our BNN accelerator beats the best
known FPGA accelerators in pure throughput, and is also
much more resource and power efcient. BNNs save espe-
cially on the number of DSPs since multiplication/division
is only needed for batch norm and not for the compute-
intensive conv or FC calculations. The metrics of through-
put per kLUT and throughput per Watt are especially im-
portant  due to the relative novelty of the BNN, we were
only able to obtain a suitable network for CIFAR-10 while
previous work shows results for larger ImageNet networks.
However, our data provides evidence that the BNN is algo-
rithmically better suited for FPGA than CNN, enabling far
more efcient usage of resource and power. With a more ad-
vanced network and larger device, our design should scale up
and achieve similar gains. Very recent work on low-precision

22CNNs have also made great strides into achieving near state-
of-the-art accuracy on the ImageNet dataset [16, 28].
Table 5: Comparison of our work against state-of-the-art
FPGA accelerators  GOPS counts multiplies and adds
per second. * indicates values approximated from charts.

[23]

[19]

[25]

Ours

Stratix-V

GSD8

Zynq
7Z045

Zynq
7Z020

Zynq
7Z020

695

218.6

53.2

53.2

120
19.1
8-16b
136.5
117.8
120*
760*

0.98

6.17

150
9.6
16b
187.8
137.0
182.6
780

100
-
-
-

12.73
43.2
208

143
4.7
1-2b
318.9
207.8
46.9

3

0.75

0.29

4.43

14.3

7.27

44.2

Platform

Capacity
(kLUTs)
Clock(MHz)
Power(W)
Precision
GOPS (conv)
GOPS (all)
kLUTs
DSPs
GOPS/
kLUT
GOPS/
Watt

While it may not be completely fair to compare GOPS
between a binarized and conventional network, it is cur-
rently the (de facto) standard practice for hardware accel-
erator studies to compare reduced and full-precision imple-
mentations that use different data types.

6. Related Work
Our paper owes much to the groundbreaking work on BNNs
in the machine learning community [6, 7]. These papers
contain some discussion on the advantages of BNNs over
CNN for hardware, but to our best knowledge we are the
rst to present a working FPGA implementation.

There have been many studies on the design of CNN ac-
celerators for FPGA. Zhang et al. [27] describe how to opti-
mize an HLS design by reordering and tiling loops, inserting
the proper pragmas, and organizing external memory trans-
fers. Ensuing publications have mostly eschewed HLS in
favor of RTL designs. Qiu et al. [19] propose an architec-
ture that computes conv and FC layers on the same hard-
ware, as well as dynamic xed-point quantization. Their pa-
per demonstrates an area-efcient accelerator for AlexNet
on the Xilinx ZC706 board.

A related line of research focuses on creating CNN de-
sign compilers which can generate optimized hardware for
a family of models. These works typically use a set of RTL
modules combined with a design space exploration tool to

nd the optimal architectural parameters. Rahman et al. [20]
propose a scalable array-based CNN accelerator with heavy
input reuse. Motamedi [17] uses a rooine model for perfor-
mance to guide hardware generation. Wang [26] proposes
DeepBurning, which targets a variety of CNN architectures
and performs data layout optimization.

OpenCL frameworks for deep learning on FPGA have
also been proposed. Suda et al. [23] use parameterized
OpenCL alongside analytical models for performance and
resource, enabling a genetic algorithm to search for the opti-
mal conguration. Venieris and Bouganis [25] study the use
of synchronous dataow to capture CNN workloads and use
graph partitioning to control resource consumption.

There is also a great deal of research work on ASIC CNN
co-processors. Among the most well know is the DianNao
line of architectures [2]. The Eyeriss paper [3] contains a
comprehensive study of popular dataows for spatial archi-
tectures and derive an optimal one.

Our approach differs from existing work in two major
ways: (1) we are the rst to study BNNs for FPGA accel-
eration; (2) we make use of a C-based HLS methodology
and propose design constructs to maximize throughput on
different layers. Existing CNN accelerators on FPGA are not
well-equipped to handle BNNs due signicant differences in
compute and storage requirements, and layer organization.
Our nal design differs greatly from previous work.

7. Conclusions and Future Work
We are the rst to implement an accelerator for binarized
neural networks on FPGA. BNNs feature potentially re-
duced storage requirements and binary arithmetic opera-
tions, making them well suited to the FPGA fabric. However,
these characteristics also render CNN design constructs such
as input tiles and line buffers ineffective. We introduce new
design constructs such as a variable-width line buffer to ad-
dress these challenges, creating an accelerator radically dif-
ferent from existing work. We leverage modern HLS tools
to write our design in productive, high-level code, and our
accelerator outperforms existing work in raw throughput,
throughput per area, and throughput per Watt.

Future BNN work should focus both on algorithmic and
architectural improvements. On the algorithmic side we
would like to explore techniques to reduce model size. From
the architectural side one action item is to implement a
low-precision network for ImageNet, which would involve
a much larger and more complicated accelerator design.

Acknowledgements
This research was supported in part by DARPA Award
HR0011-16-C-0037, a DARPA Young Faculty Award, NSF
Awards #1337240, #1453378, #1512937, and a research gift
from Xilinx, Inc. The Tesla K40 GPU used for this research
was donated by the NVIDIA Corporation.

23References
[1] A. Canis, J. Choi, M. Aldham, V. Zhang, A. Kammoona,
T. Czajkowski, S. D. Brown, and J. H. Anderson. LegUp:
An Open-Source High-Level Synthesis Tool for FPGA-Based
Processor/Accelerator Systems. ACM Trans. on Embedded
Computing Systems (TECS), 13(2):24, 2013.

[2] T. Chen, Z. Du, N. Sun, J. Wang, C. Wu, Y. Chen, and
O. Temam. Diannao: A Small-Footprint High-Throughput
Intl Conf.
Accelerator for Ubiquitous Machine-earning.
on Architectural Support for Programming Languages and
Operating Systems (ASPLOS), Mar 2014.

[3] Y.-H. Chen, T. Krishna, J. Emer, and V. Sze. Eyeriss:
An Energy-Efcient Recongurable Accelerator for Deep
Intl Symp. on Computer
Convolutional Neural Networks.
Architecture (ISCA), Jun 2016.

[4] A. Coates, B. Huval, T. Wang, D. J. Wu, A. Y. Ng, and
B. Catanzaro. Deep Learning with COTS HPC Systems.
Intl Conf. on Machine Learning (ICML), pages 13371345,
Jun 2013.

[5] J. Cong, B. Liu, S. Neuendorffer, J. Noguera, K. Vissers, and
Z. Zhang. High-Level Synthesis for FPGAs: From Prototyping
to Deployment. IEEE Trans. on Computer-Aided Design of
Integrated Circuits and Systems (TCAD), Apr 2011.

[6] M. Courbariaux, Y. Bengio, and J.-P. David. BinaryConnect:
Training Deep Neural Networks with binary weights during
propagations. Advances in Neural Information Processing
Systems (NIPS), pages 31233131, 2015.

[7] M. Courbariaux, I. Hubara, D. Soudry, R. El-Yaniv, and

Y. Bengio. Binarized Neural Networks: Training Deep Neural
Networks with Weights and Activations Constrained to +1 or
-1. arXiv e-print, arXiv:1602.02830, Feb 2016.

[8] T. S. Czajkowski, U. Aydonat, D. Denisenko, J. Freeman,
M. Kinsner, D. Neto, J. Wong, P. Yiannacouras, and D. P.
Singh. From OpenCL to High-Performance Hardware on
Intl Conf. on Field Programmable Logic and
FPGAs.
Applications (FPL), pages 531534, Aug 2012.

[9] M. A. et al. TensorFlow: Large-Scale Machine Learning
on Heterogeneous Systems, 2015. Software available from
tensorow.org.

[10] K. He, X. Zhang, S. Ren, and J. Sun. Deep Residual Learning
for Image Recognition. arXiv e-print, arXiv:1512.0338, Dec
2015.

[11] S. Ioffe and C. Szegedy. Batch Normalization: Accelerating
Deep Network Training by Reducing Internal Covariate Shift.
arXiv e-print, arXiv:1502.03167, Mar 2015.

[12] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolutional
architecture for fast feature embedding. arXiv preprint,
arXiv:1408.5093, 2014.

[13] V. Kathail, J. Hwang, W. Sun, Y. Chobe, T. Shui, and

J. Carrillo. SDSoC: A Higher-level Programming Environment
for Zynq SoC and Ultrascale+ MPSoC. Intl Symp. on Field-
Programmable Gate Arrays (FPGA), pages 44, Feb 2016.

[14] A. Krizhevsky and G. Hinton. Learning Multiple Layers of
Features from Tiny Images, 2009. Masters Thesis. Department
of Coumputer Science, University of Toronto.

[15] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
Classication with Deep Convolutional Neural Networks.
Advances in Neural Information Processing Systems (NIPS),
pages 10971105, 2012.

[16] F. Li and B. Liu. Ternary Weight Networks. arXiv e-print,

arXiv:1605.04711, May 2016.

[17] M. Motamedi, P. Gysel, V. Akella, and S. Ghiasi. Design

Space Exploration of FPGA-Based Deep Convolutional Neural
Networks. Asia and South Pacic Design Automation Conf.
(ASP-DAC), pages 575580, Jan 2016.

[18] K. Ovtcharov, O. Ruwase, J.-Y. Kim, J. Fowers, K. Strauss,

and E. Chung. Accelerating Deep Convolutional Neural
Networks Using Specialized Hardware. Microsoft Research,
Feb 2015.

[19] J. Qiu, J. Wang, S. Yao, K. Guo, B. Li, E. Zhou, J. Yu, T. Tang,
N. Xu, S. Song, et al. Going Deeper with Embedded FPGA
Platform for Convolutional Neural Network. Intl Symp. on
Field-Programmable Gate Arrays (FPGA), pages 2635, Feb
2016.

[20] A. Rahman, J. Lee, and K. Choi. Efcient FPGA Acceleration
of Convolutional Neural Networks using Logical-3D Compute
Array. Design, Automation, and Test in Europe (DATE), pages
13931398, Apr 2016.

[21] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. XNOR-

Net: ImageNet Classication Using Binary Convolutional
Neural Networks. European Conference on Computer Vision
(ECCV), Oct 2016. arXiv:1603.05279.

[22] K. Simonyan and A. Zisserman. Very Deep Convolutional
Networks for Large-Scale Image Recognition. arXiv e-print,
arXiv:1409.15568, Apr 2015.

[23] N. Suda, V. Chandra, G. Dasika, A. Mohanty, Y. Ma, S. Vrud-

hula, J.-s. Seo, and Y. Cao. Throughput-Optimal OpenCL-
based FPGA Accelerator for Large-Scale Convolutional Neu-
ral Networks. Intl Symp. on Field-Programmable Gate Arrays
(FPGA), pages 1625, Feb 2016.

[24] Theano Development Team. Theano: A Python framework for
fast computation of mathematical expressions. arXiv e-print,
arXiv:1605.02688, May 2016.

[25] S. I. Venieris and C.-S. Bouganis. fpgaConvNet: A Frame-
work for Mapping Convolutional Neural Networks on FPGAs.
IEEE Symp. on Field Programmable Custom Computing Ma-
chines (FCCM), May 2016.

[26] Y. Wang, J. Xu, Y. Han, H. Li, and X. Li. DeepBurning:
Automatic Generation of FPGA-based Learning Accelerators
for the Neural Network Family. Design Automation Conf.
(DAC), page 110, Jun 2016.

[27] C. Zhang, P. Li, G. Sun, Y. Guan, B. Xiao, and J. Cong.

Optimizing FPGA-based Accelerator Design for Deep Convo-
lutional Neural Networks. Intl Symp. on Field-Programmable
Gate Arrays (FPGA), pages 161170, Feb 2015.

[28] S. Zhou, Y. Wu, Z. Ni, X. Zhou, H. Wen, and Y. Zou.

DoReFar-Net: Training Low Bitwidth Convolutional Neural
Networks with Low Bitwidth Gradients. arXiv e-print,
arXiv:1606.06160, Jul 2016.

24