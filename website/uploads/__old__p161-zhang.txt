Optimizing FPGA-based Accelerator Design for Deep

Convolutional Neural Networks

Chen Zhang1

Peng Li2

chen.ceca@pku.edu.cn

pengli@cs.ucla.edu

Guangyu Sun1,3
gsun@pku.edu.cn

guanyijin@pku.edu.cn

Yijin Guan1


Jason Cong 2,3,1,
cong@cs.ucla.edu
1Center for Energy-Efcient Computing and Applications, Peking University, China

xiao@cs.ucla.edu

Bingjun Xiao2

2Computer Science Department, University of California, Los Angeles, USA

3PKU/UCLA Joint Research Institute in Science and Engineering

ABSTRACT
Convolutional neural network (CNN) has been widely em-
ployed for image recognition because it can achieve high ac-
curacy by emulating behavior of optic nerves in living crea-
tures. Recently, rapid growth of modern applications based
on deep learning algorithms has further improved research
and implementations. Especially, various accelerators for
deep CNN have been proposed based on FPGA platform
because it has advantages of high performance, recongura-
bility, and fast development round, etc. Although current
FPGA accelerators have demonstrated better performance
over generic processors, the accelerator design space has not
been well exploited. One critical problem is that the com-
putation throughput may not well match the memory band-
width provided an FPGA platform. Consequently, existing
approaches cannot achieve best performance due to under-
utilization of either logic resource or memory bandwidth. At
the same time, the increasing complexity and scalability of
deep learning applications aggravate this problem. In order
to overcome this problem, we propose an analytical design
scheme using the rooine model. For any solution of a CNN
design, we quantitatively analyze its computing throughput
and required memory bandwidth using various optimization
techniques, such as loop tiling and transformation. Then,
with the help of rooine model, we can identify the solution
with best performance and lowest FPGA resource require-
ment. As a case study, we implement a CNN accelerator
on a VC707 FPGA board and compare it to previous ap-
proaches. Our implementation achieves a peak performance
of 61.62 GFLOPS under 100MHz working frequency, which
outperform previous approaches signicantly.


In addition to being a faculty member at UCLA, Jason Cong is also a
co-director of the PKU/UCLA Joint Research Institute and a visiting
chair professor of Peking University.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for prot or commercial advantage and that copies bear this notice and the full cita-
tion on the rst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior specic permission
and/or a fee. Request permissions from permissions@acm.org.
FPGA15, February 2224, 2015, Monterey, California, USA.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-3315-3/15/02 ...$15.00.
http://dx.doi.org/10.1145/2684746.2689060.

Categories and Subject Descriptors
C.3 [SPECIAL-PURPOSE AND APPLICATION-BASED
SYSTEMS]: Microprocessor/microcomputer applications

Keywords
FPGA; Rooine Model; Convolutional Neural Network; Ac-
celeration

1.

INTRODUCTION

Convolutional neural network (CNN), a well-known deep
learning architecture extended from articial neural network,
has been extensively adopted in various applications, which
include video surveillance, mobile robot vision, image search
engine in data centers, etc [6] [7] [8] [10] [14]. Inspired by the
behavior of optic nerves in living creatures, a CNN design
processes data with multiple layers of neuron connections to
achieve high accuracy in image recognition. Recently, rapid
growth of modern applications based on deep learning algo-
rithms has further improved research on deep convolutional
neural network.

Due to the specic computation pattern of CNN, general
purpose processors are not ecient for CNN implementation
and can hardly meet the performance requirement. Thus,
various accelerators based on FPGA, GPU, and even ASIC
design have been proposed recently to improve performance
of CNN designs [3] [4] [9]. Among these approaches, FPGA
based accelerators have attracted more and more attention
of researchers because they have advantages of good perfor-
mance, high energy eciency, fast development round, and
capability of reconguration [1] [2] [3] [6] [12] [14].

For any CNN algorithm implementation, there are a lot
of potential solutions that result in a huge design space for
exploration. In our experiments, we nd that there could
be as much as 90% performance dierence between two dif-
ferent solutions with the same logic resource utilization of
FPGA. It is not trivial to nd out the optimal solution,
especially when limitations on computation resource and
memory bandwidth of an FPGA platform are considered.
In fact, if an accelerator structure is not carefully designed,
its computing throughput cannot match the memory band-
width provided an FPGA platform. It means that the per-
formance is degraded due to under-utilization of either logic
resource or memory bandwidth.

161Unfortunately, both advances of FPGA technology and
deep learning algorithm aggravate this problem at the same
time. On one hand, the increasing logic resources and mem-
ory bandwidth provided by state-of-art FPGA platforms en-
large the design space.
In addition, when various FPGA
optimization techniques, such as loop tiling and transforma-
tion, are applied, the design space is further expanded. On
the other hand, the scale and complexity of deep learning al-
gorithms keep increasing to meet the requirement of modern
applications. Consequently, it is more dicult to nd out
the optimal solution in the design space. Thus, an ecient
method is urgently required for exploration of FPGA based
CNN design space.

To eciently explore the design space, we propose an an-
alytical design scheme in this work. Our work outperforms
previous approaches for two reasons. First, work [1] [2] [3]
[6] [14] mainly focused on computation engine optimization.
They either ignore external memory operation or connect
their accelerator directly to external memory. Our work,
however, takes buer management and bandwidth optimiza-
tion into consideration to make better utilization of FPGA
resource and achieve higher performance. Second, previous
study [12] accelerates CNN applications by reducing external
data access with delicate data reuse. However, this method
do not necessarily lead to best overall performance. More-
over, their method needs to recongure FPGA for dierent
layers of computation. This is not feasible in some scenarios.
Our accelerator is able to execute acceleration jobs across
dierent layers without reprogramming FPGA.

The main contributions of this work are summarized as

follows,

 We quantitatively analyze computing throughput and
required memory bandwidth of any potential solution
of a CNN design on an FPGA platform.

 Under the constraints of computation resource and
memory bandwidth, we identify all possible solutions
in the design space using a rooine model.
In addi-
tion, we discuss how to nd the optimal solution for
each layer in the design space.

 We propose a CNN accelerator design with uniform
loop unroll factors across dierent convolutional layers.
 As a case study, we implement a CNN accelerator that
achieves a performance of 61.62 GFLOPS. To the best
of our knowledge, this implementation has highest per-
formance and the highest performance density among
existing approaches.

The rest of this paper is organized as follows: Section 2
provides a background for CNN and rooine model. Sec-
tion 3 presents our analytical approach for optimizing accel-
erator design. Section 4 describes implementation details.
Section 5 shows our experiment result. Section 6 makes
comparison between our implementation and existing work
and Section 7 concludes the paper.

2. BACKGROUND
2.1 CNN Basics

Convolutional neural network (CNN) is rst inspired by
research in neuroscience. After over twenty years of evolu-
tion, CNN has been gaining more and more distinction in

research elds, such as computer vision, AI (e.g. [11] [9]).
As a classical supervised learning algorithm, CNN employs
a feedforward process for recognition and a backward path
for training.
In industrial practice, many application de-
signers train CNN o-line and use the o-line trained CNN
to perform time-sensitive jobs. So the speed of feedforward
computation is what matters.
In this work, we focus on
speeding up the feedforward computation with FPGA based
accelerator design.

A typical CNN is composed of two components: a feature
extractor and a classier. The feature extractor is used to
lter input images into feature maps that represent various
features of the image. These features may include corners,
lines, circular arch, etc., which are relatively invariant to po-
sition shifting or distortions. The output of the feature ex-
tractor is a low-dimensonal vector containing these features.
This vector is then fed into the classier, which is usually
based on traditional articial neural networks. The purpose
of this classier is to decide the likelihood of categories that
the input (e.g. image) might belong to.

A typical CNN is composed of multiple computation lay-
ers. For example, the feature extractor may consist of sev-
eral convolutional layers and optional sub-sampling layers.
Figure 1 illustrates the computation of a convolutional layer.
The convolutional layer receives N feature maps as input.
Each input feature map is convolved by a shifting window
with a K  K kernel to generate one pixel in one output fea-
ture map. The stride of the shifting window is S, which is
normally smaller than K. A total of M output feature maps
will form the set of input feature maps for the next convo-
lutional layer. The pseudo code of a convolutional layer can
be written as that in Code 1.

Figure 1: Graph of a convolutional layer

f o r ( row =0;

row<R ;

f o r ( c o l =0; c o l <C ;

f o r ( t o =0;

to<M;

f o r ( t i =0;
f o r ( i =0;

t i <N ;
i <K ;

f o r ( j =0;

j <K ;

row++) {
c o l ++) {
t o++) {
t i ++) {
i ++) {
j ++) {

L :

} } } } } }

o u t p u t f m [ t o ] [ row ] [ c o l ] +=

w e i g h t s [ t o ] [ t i ] [ i ] [ j ]
i n p u t f m [ t i ] [ Srow+i ] [ S c o l+j ] ;

Code 1: Pseudo code of a convolutional layer

In the feedforward computation perspective, a previous
study [5] proved that convolution operations will occupy over
90% of the computation time. So in this work, we will focus
on accelerating convolutional layers. An integration with
other optional layers, such as sub-sampling or max pooling
layers, will be studied in future work.

1622.2 A Real-Life CNN

Figure 2: A real-life CNN that won the ImageNet
2012 contest [9]

Figure 2 shows a real-life CNN application, taken from [9].
This CNN is composed of 8 layers. The rst 5 layers are con-
volutional layers and layers 6  8 form a fully connected arti-
cial neural network. The algorithm receives three 224x224
input images that are from an original 256x256 three-channel
RGB image. The output vector of 1000 elements represents
the likelihoods of 1000 categories. As is shown in Figure 2,
Layer1 recieves 3 input feature maps in 224x224 resolution
and 96 output feature maps in 55x55 resolution. The output
of layer1 is partitioned into two sets, each sized 48 feature
maps. Layer1s kernel size is 11x11 and the sliding window
shifts across feature maps in a stride of 4 pixels. The follow-
ing layers also have a similar structure. The sliding strides of
other layers convolution window are 1 pixel. Table 1 shows
this CNNs conguration.

Table 1: CNN congurations
Layer

3

input fm (N)
output fm (M)

fm row (R)
fm col. (C)
kernel (K)
stride (S)

set #

1
3
48
55
55
11
4
2

2
48
128
27
27
5
1
2

4

256
192
13
13
3
1
2

192
192
13
13
3
1
2

5

192
128
13
13
3
1
2

2.3 The Rooine Model

Computation and communication are two principal con-
straints in system throughput optimization. An implemen-
tation can be either computation-bounded or memory-bounded.
In [15], a rooine performance model is developed to relate
system performance to o-chip memory trac and the peak
performance provided by the hardware platform.

Eq. (1) formulates the attainable throughput of an appli-
cation on a specic hardware platform. Floating-point per-
formance (GFLOPS) is used as the metric of throughput.
The actual oating-point performance of an application ker-
nel can be no higher than the minimum value of two terms.
The rst term describes the peak oating-point throughput
provided by all available computation resources in the sys-
tem, or computational roof. Operations per DRAM trac,
or computation to communication (CTC) ratio, features the
DRAM trac needed by a kernel in a specic system imple-
mentation. The second term bounds the maximum oating-
point performance that the memory system can support for
a given computation to communication ratio.

(cid:26) Computational Roof

CT C Ratio  BW

Attainable P erf. = min

(1)

Figure 3 visualizes the rooine model with computational
roof and I/O bandwidth roof. Algorithm 2 in the gure has
higher computation to communication ratio, or better data
reuse, compared to Algorithm 1. From the gure, we can see
that by fully-utilizing all hardware computation resources,
Algorithm 2 can outperform Algorithm 1, in which compu-
tation resources are under-utilized because of the inecient
o-chip communication.

3. ACCELERATOR DESIGN EXPLORATION

In this section, we rst present an overview of our acceler-
ator structure and introduce several design challenges on an
FPGA platform. Then, in order to overcome them, we pro-
pose corresponding optimization techniques to explore the
design space.
3.1 Design Overview

As shown in Figure 4, a CNN accelerator design on FPGA
is composed of several major components, which are process-
ing elements (PEs), on-chip buer, external memory, and
on-/o-chip interconnect. A PE is the basic computation
unit for convolution. All data for processing are stored in
external memory. Due to on-chip resource limitation, data
are rst cached in on-chip buers before being fed to PEs.
Double buers are used to cover computation time with data
transfer time. The on-chip interconnect is dedicated for data
communication between PEs and on-chip buer banks.

Figure 3: Basis of the rooine model

Figure 4: Overview of accelerator design

There are several design challenges that obstacle an e-
cient CNN accelerator design on an FPGA platform. First,
loop tiling is mandatory to t a small portion of data on-
chip. An improper tiling may degrade the eciency of data

...Inter-connectComputation Resourcebuffer1buffer2External MemoryOff-chip BusOn-chip memoryPE-1PE-2PE-n163// onc h i p d a t a c o m p u t a t i o n
f o r ( t r r =row ;

t r r <min ( row+Tr , R ) ;

f o r ( t c c=c o l ;
f o r ( t o o=t o ;

t c c <min ( c o l+Tc , C ) ;
too<min ( t o+Tm,M) ;

f o r ( t i i = t i ;

t i i <min ( t i +Tn , N ) ;

f o r ( i =0;

i <K ;

f o r ( j =0;

j <K ;

i ++) {
j ++) {

t r r ++){
t c c ++){
t o o ++){
t i i ++){

L :

o u t p u t f m [ t o o ] [ t r r ] [ t c c ] +=

w e i g h t s [ t o o ] [ t i i ] [ i ] [ j ]
i n p u t f m [ t i i ] [ S t r r +i ] [ S t c c+j ] ;

} } } } } }

Code 2: On-chip data computation

// onc h i p d a t a c o m p u t a t i o n
f o r ( i =0;

i <K ;

i ++) {
j ++) {
t r r <min ( row+Tr , R ) ;

f o r ( j =0;

j <K ;
f o r ( t r r =row ;

f o r ( t c c=c o l ;
f o r ( t o o=t o ;

t c c <min ( c o l+Tc , C ) ;
too<min ( t o+Tm,M) ;

#pragma HLS UNROLL

f o r ( t i i = t i ;

t i i <min ( t i +Tn , N ) ;

#pragma HLS UNROLL

t r r ++){
t c c ++){
t o o ++){
t i i ++){

o u t p u t f m [ t o o ] [ t r r ] [ t c c ] +=

w e i g h t s [ t o o ] [ t i i ] [ i ] [ j ]
i n p u t f m [ t i i ] [ S t r r +i ] [ S t c c+j ] ;

} } } } } }

Figure 5: Pseudo code of a tiled convolutional layer

L :

reuse and parallelism of data processing. Second, the orga-
nization of PEs and buer banks and interconnects between
them should be carefully considered in order to process on-
chip data eciently. Third, the data processing throughput
of PEs should match the o-chip bandwidth provided by the
FPGA platform.

In this section, we start our optimization from Code 1 and
present our methodology for exploring the design space to
achieve an ecient design in successive steps. First, we ap-
ply loop tiling (Figure 5). Note that loop iterators i and j
are not tiled because of the relatively small size (typically
ranging from 3 to 11) of convolution window size K in CNN.
Other loop iterators (row, col, to and ti) are tiled into tile
loops and point loops (trr, tcc, too and tii in Figure 5). Sec-
ond, we discuss the computation engine optimization and
formulate the computational performance with the tilling
factors (Section 3.2). Third, We use data reuse technique
to reduce external memory trac and formulate the com-
putation to communication ratio with tiling factors (Sec-
tion 3.3). Forth, with the two variables dened above, we de-
ne the design space, in which we present the computation-
memory-access-matched-design under FPGA board speci-
cation (Section 3.4). Fifth, We discuss how to select a best
uniform accelerator for the entire multi-layer CNN applica-
tion (Section 3.5).
3.2 Computation Optimization

In this section, we use standard polyhedral-based data
dependence analysis [13] to derive a series of legal design
variants of equivalently CNN implementations through loop
scheduling and loop tile size enumeration. The objective
of computation optimization is to enable ecient loop un-
rolling/pipelining while fully utilizing of all computation re-
sources provided by the FPGA hardware platform. In this
section, we assume that all required data are on-chip. O-
chip memory bandwidth constraints will be discussed in Sec-
tion 3.3.

Loop Unrolling. Loop unrolling can be used to increase
the utilization of massive computation resources in FPGA
devices. Unrolling along dierent loop dimensions will gen-
erate dierent implementation varients. Whether and to

Code 3: Proposed accelerator structure

what extent two unrolled execution instances share data will
aect the complexity of generated hardware, and eventually
aect the number of unrolled copies and the hardware opera-
tion frequency. The data sharing relations between dierent
loop iterations of a loop dimension on a given array can be
classied into three categories:

 Irrelevant. If a loop iterator ik does not appear in any
access functions of an array A, the corresponding loop
dimension is irrelevant to array A.

 Independent.

If the union of data space accessed on
an array A is totally separable along a certain loop
dimension ik, or for any given two distinct param-
eters p1 and p2, the data accessed by DS(A, ik =

p1) = (cid:83) Image(cid:0)F A
DS(A, ik = p2) =(cid:83) Image(cid:0)F A

S , (DS  ik = p1)(cid:1) is disjoint with
S , (DS  ik = p2)(cid:1)1, the

loop dimension ik is independent of array A.

 Dependent. If the union of data space accessed on an
array A is not separable along a certain loop dimension
ik, the loop dimension ik is dependent of array A.

The hardware implementations generated by dierent data
sharing relations are shown in Figure 6. An independent
data sharing relation generates direct connections between
buers and computation engines. An irrelevant data shar-
ing relation generates broadcast connections. A dependent
data sharing relation generates interconnects with complex
topology.

The data sharing relations of Figure 5 is shown in Table
2. Loop dimensions too and tii are selected to be unrolled to
avoid complex connection topologies for all arrays. T oo and
tii are permuted to the innermost loop levels to simplify HLS
code generation. The generated hardware implementation
can be found in Figure 7.

1The polyhedral annotations used here can be found in [13]

164(a) irrelevant

(b) independent

(c) dependent

Figure 6: Hardware implementations of dierent
data sharing relations

Table 2: Data sharing relations of CNN code

input f m
dependent
dependent
irrelevant

independent
dependent
dependent

weights
irrelevant
irrelevant

independent
independent
independent
independent

output f m
independent
independent
independent

irrelevant
irrelevant
irrelevant

trr
tcc
too
tii
i
j

Loop Pipelining. Loop pipelining is a key optimization
technique in high-level synthesis to improve system through-
put by overlapping the execution of operations from dierent
loop iterations. The throughput achieved is limited both by
resource constraints and data dependencies in the applica-
tion. Loop-carried dependence will prevent loops to be fully
pipelined. Polyhedral-based optimization framework [16]
can be used to perform automatic loop transformation to
permute the parallel loop levels to the innermost levels to
avoid loop carried dependence. Code structure after opti-
mization for loop unrolling and loop pipelining is shown in
Code 3.

Figure 7: Computation engine

3.3 Memory Access Optimization

In Section 3.2, we discussed how to derive design vari-
ants with dierent computational roofs, assuming all data
accessed by the computation engine are already buered on-
chip. However, design variants with the higher computa-
tional roof does not necessarily achieve the higher perfor-
mance under memory bandwidth constraints.
In this sec-
tion, we will show how to reduce communication volume
with ecient data reuse.

Figure 9 illustrates the memory transfer operations of a
CNN layer.
Input/output feature maps and weights are
loaded before the computation engine starts and the gener-
ated output feature maps are written back to main memory.

Tile Size Selection. Fixing the loop structure, design vari-
ants with dierent loop tile size will also have signicantly
dierent performance. The space of all legal tile sizes for
Code 3 are illustrated by Equation (2).

0 < T m  T n  (# of P Es)

0 < T m  M
0 < T n  N
0 < T r  R
0 < T c  C

(2)



Given a specic tile size combination (cid:104)T m, T n, T r, T c(cid:105),
the computational performance (or computational roof in
the rooine model) can be calculated by Equation (3). From
the equation, we can observe that the computational roof is
a function of Tm and Tn.

computational roof
total number of operations
number of execution cycles

(cid:6) M
(cid:6) M

Tm

Tm

(cid:7) (cid:6) N
(cid:7) (cid:6) N

Tn

Tn

(cid:7)  R
(cid:7)  R  C  K  K

 C

Tr

Tc

2  R  C  M  N  K  K

2  R  C  M  N  K  K

 (Tr  Tc  K  K + P )

(3)

=

=



where P = pipeline depth  1.

Figure 9: Local memory promotion for CNN

Local Memory Promotion. If the innermost loop in the
communication part (loop dimension ti in Figure 9) is irrel-
evant to an array, there will be redundant memory opera-
tions between dierent loop iterations. Local memory pro-
motion [13] can be used to reduce the redundant operations.
In Figure 9, the innermost loop dimension ti is irrelevant to
array output f m. Thus, the accesses to array output f m
can be promoted to outer loops. Note that the promotion
process can be iteratively performed until the innermost loop
surrounding the accesses is nally relevant. With local mem-
ory promotion, the trip count of memory access operations
on array output f m reduces from 2 M
to

 N

 R

 C

Tm

Tn

Tr

Tc

 R

Tr

 C

Tc

.

M
Tm

Loop Transformations for Data Reuse. To maximize the
opportunities of data reuse through local memory promo-

165(a) Design space of all possible designs

(b) Design space of platform-supported designs

Figure 8: Design space exploration

tions, we use polyhedral-based optimization framework to
identify all legal loop transformations. Table 3 shows the
data sharing relations between loop iterations and arrays.
Local memory promotions are used in each legal loop sched-
ule whenever applicable to reduce the total communication
volume.

Table 3: Data sharing relations of communication
part

irrelevant dimension(s)

input f m
weights

output f m

to

row,col

ti

CTC Ratio. Computation to communication (CTC) ratio
is used to describe the computation operations per memory
access. Data reuse optimization will reduce the total num-
ber of memory accesses, thus increase the computation to
communication ratio. The computation to communication
ratio of the code shown in Figure 9 can be calculated by
Equation (4), where in, out, wght and Bin, Bout, Bwght
denote the trip counts and buer sizes of memory accesses
to input/output feature maps and weights respectively.

=

=

where

Computation to Communication Ratio

total number of operations

total amount of external data access

2  R  C  M  N  K  K

in  Bin + wght  Bwght + out  Bout

Bin = Tn(STr + K  S)(STc + K  S)
Bwght = TmTnK 2
Bout = TmTrTc
0 < Bin + Bwght + Bout  BRAMcapacity

M
Tm

in = wght =

 N
Tn
Without output f ms data reuse,
 N
Tn

out = 2  M
Tm

(4)

(5)

(6)

(7)

(8)

(9)

 R
Tr

 C
Tc

 R
Tr

 C
Tc

(10)

With output f ms data reuse,

out =

M
Tm

 R
Tr

 C
Tc

(11)

Given a specic loop schedule and a set of tile size tuple
(cid:104)T m, T n, T r, T c(cid:105), computation to communication ratio can
be calculated with above formula.
3.4 Design Space Exploration
As mentioned in Section 3.2 and Section 3.3, given a spe-
cic loop schedule and tile size tuple (cid:104)T m, T n, T r, T c(cid:105), the
computational roof and computation to communication ra-
tio of the design variant can be calculated. Enumerating all
possible loop orders and tile sizes will generate a series of
computational performance and computation to communi-
cation ratio pairs. Figure 8(a) depicts all legal solutions for
layer 5 of the example CNN application in the rooline model
coordinate system. The x axis denotes the computation to
communication ratio, or the ratio of oating point operation
per DRAM byte access. The y axis denotes the compu-
tational performance (GFLOPS). The slope of the line be-
tween any point and the origin point (0, 0) denotes the mini-
mal bandwidth requirement for this implementation. For ex-
ample, design P s minimal bandwidth requirement is equal
to the slope of the line P (cid:48).

In Figure 8(b), the line of bandwidth roof and computa-
tional roof are dened by the platform specication. Any
point at the left side of bandwidth rooine requires a higher
bandwidth than what the platform can provide. For exam-
ple, although implementation A achieves the highest possi-
ble computational performance, the memory bandwidth re-
quired cannot be satised by the target platform. The actual
performance achievable on the platform would be the ordi-
nate value of A(cid:48). Thus the platform-supported designs are
dened as a set including those located at the right side of
the bandwidth rooine and those just located on the band-
width rooine, which are projections of the left side designs.
We explore this platform-supported design space and a
set of implementations with the highest performance can
be collected. If this set only include one design, then this
design will be our nal result of design space exploration.
However, a more common situation is that we could nd
several counterparts within this set, e.g. point C, D and
some others in Figure 8(b). We pick the one with the highest
CI value because this design requires the least bandwidth.

0204060801001200102030405060Ratio of(FLOP)/(DRAM byte access)Computation to Communication RatioAttainable performance (GFLOPS)PLine: P0204060801001200102030405060Computational roofBandwidth roof upper bound (4.5 GB/s)CAttainable performance (GFLOPS)Ratio of(FLOP)/(DRAM byte access)DAAComputation to Communication Ratio166This selection criteria derives from the fact that we can use
fewer I/O ports, fewer LUTs and hardwired connections etc.
for data transfer engine in designs with lower bandwidth
requirement. Thus, point C is our nally chosen design in
this case for layer 5. Its bandwidth requirement is 2.2 GB/s.
3.5 Multi-Layer CNN Accelerator Design

Table 4: Layer specic optimal solution and cross-
layer optimization

Optimal Unroll Factor

(cid:104)T m, T n(cid:105)
(cid:104)48, 3(cid:105)
(cid:104)20, 24(cid:105)
(cid:104)96, 5(cid:105)
(cid:104)95, 5(cid:105)
(cid:104)32, 15(cid:105)

-

(cid:104)64, 7(cid:105)

Layer 1
Layer 2
Layer 3
Layer 4
Layer 5
Total

Cross-Layer
Optimization

Execution Cycles

366025
237185
160264
120198
80132
963804

1008246

In previous sections, we discussed how to nd optimal im-
plementation parameters for each convolutional layer. In a
CNN application, these parameters may vary between dif-
ferent layers. Table 4 shows the optimal unroll factors (T m
and T n) for all layers of the example CNN application (see
Figure 2).

Designing a hardware accelerator to support multiple con-
volutional layer with dierent unroll factors would be chal-
lenging. Complex hardware structures are required to re-
congure computation engines and interconnects.

An alternative approach is to design a hardware archi-
tecture with uniform unroll factors across dierent convo-
lutional layers. We enumerate all legal solutions to select
the optimal global design parameters. CNN accelerator with
unied unroll factors is simple to design and implement, but
may be sub-optimal for some layers. Table 4 shows that with
unied unroll factors ((cid:104)64, 7(cid:105)), the degradation is within 5%
compared to the total execution cycles of each optimized
convolutional layer. With this analysis, CNN accelerator
with unied unroll factors across convolutional layers are
selected in our experiments. The upper bound of the enu-
meration space size is 98 thousand legal designs, which can
nish in 10 minutes at a common laptop.

4.

IMPLEMENTATION DETAILS

This section describes the detailed implementation of our

solution.
4.1 System Overview

Figure 10 shows an overview of our implementation. The
whole system ts in one single FPGA chip and uses a DDR3
DRAM for external storage. MicroBlaze, a RISC soft pro-
cessor core developed for Xilinx FPGAs, is used to assist
with CNN accelerator startup, communication with host
CPU and time measurement etc. AXI4lite bus is for com-
mand transfer and AXI4 bus is for data transfer. The CNN
accelerator works as an IP on the AXI4 bus.
It receives
commands and conguration parameters from MicroBlaze
through AXI4lite bus and communicate with customized
data transfer engines through FIFO interfaces. This data

Figure 10: Implementation overview

Figure 11: Block diagram of proposed accelerator

transfer engine can access external memory through AXI4
bus. Interruption mechanism is enabled between MicroBlaze
and CNN accelerator to provide an accurate time measure-
ment.
4.2 Computation Engines

The computation engine part in Figure 11 shows a block
diagram of our implementation. They are designed accord-
ing to our analysis in Section 3. The two-level unrolled loops
(Tm, Tn in Figure 2) are implemented as concurrently exe-
cuting computation engines. A tree-shaped poly structure
like that in Figure 7 is used. For the best cross-layer design
((cid:104)Tm, Tn(cid:105) = (cid:104)64, 7(cid:105)) case, the computation engine is imple-
mented as a tree-shaped poly structure with 7 inputs from
input feature maps and 7 inputs from weights and one input
from bias, which is stored in the buers of output feature
maps. 64 poly structures are duplicated for unrolling loop
Tm. An overview can be found in the compute engine part
of Figure 11.
4.3 Memory Sub-System

On-chip buers are built upon a basic idea of double-
buering, in which double buers are operated in a ping-
pong manner to overlap data transfer time with computa-
tion. Therefore, they are organized in four sets: two for
input feature maps and weights and two for output feature

167Figure 12: Timing graph

maps. We rst introduce each buer sets organization and
followed by the ping-pong data transfer mechanism.

Every buer set contains several independent buer banks.
The number of buer banks in each input buer set is equal
to Tn (tile size of input f m). The number of buer banks in
each output buer set is equal to Tm (tile size of output f m).
Double buer sets are used to realize ping-pong opera-
tions. To simplify discussion, we use a concrete case in Fig-
ure 9 to illustrate the mechanism of ping-pong operation.
See the code in Figure 9. The o-load operation will oc-

(cid:7) times of load operation. But the

cur only once after(cid:6) N

amount of data in every output f m transfer are larger than
that of input f m in a ratio of  Tm
7 . To increase
the bandwidth utilization, we implement two independent
channels, one for load operation and the other for o-load
operation.

= 64

Tn

Tn

Figure 12 shows the timing of several compute and data
transfer phases. For the rst phase, computation engine is
processing with input buer set 0 while copying the next
phase data to input buer set 1. The next phase will do the
opposite operation. This is the ping-pong operation of input

feature maps and weights. When(cid:6) N
of(cid:6) N

(cid:7) phases of computa-
(cid:7) phases till the reused temporary data in the output

tion and data copying are done, the resulting output feature
maps are written down to DRAM. The o-load operation
would o-load results in the output buer set 0 in the period

Tn

buer set 1 generates the new results. This is the ping-pong
operation of output feature maps. Note that those two in-
dependent channel for load and store operation mechanism
work for any other data reuse situation in this framework.
4.4 External Data Transfer Engines

Tn

The purposes of using external data transfer engines are
in two folds: 1) It can provide data transfer between acceler-
ator and external memory; 2) It can isolate our accelerator
from various platform and tool specic bandwidth features.
Figure 13 shows an experiment with AXI4 bus bandwidth in
Vivado 2013.4. In these two gures, we set two parameters,
bitwidth of AXI bus to DRAM controller and DRAM con-
trollers external bandwidth, at their highest congurations
while changing the number of IP-AXI interfaces and the
bitwidth of each IP. In Figure 13(a), the increase in IP-AXI
interface bitwidth has no eect on bandwidth (400MB/s un-
der 100MHz frequency). In Figure 13(b), with more IP in-
terfaces added to AXI bus, its bandwidth increases almost
linearly and the highest bandwidth is about 4.5 GB/s. In
our CNN accelerator design, a minimal bandwidth of 1.55
GB/s is required. Therefore, 4 IP interfaces are sucient
for this design according to Figure 13. We use two AXI-IP
interfaces in data transfer engine 0 and two in data transfer
engine 1, as is shown in Figure 10.

(a) Single IPs bandwidth-
bitwidth relation

(b) bandwith-IP numbers
relation

Figure 13: IP-DRAM bandwidth(Vivado 2013.4)

5. EVALUATION

In this section, we rst introduce the environment setup of
our experiments. Then, comprehensive experimental results
are provided.
5.1 Experimental Setup

The accelerator design is implemented with Vivado HLS
(v2013.4). This tool enables implementing the accelerator
with C language and exporting the RTL as a Vivados IP
core. The C code of our CNN design is parallelized by adding
HLS-dened pragma and the parallel version is validated
with the timing analysis tool. Fast pre-synthesis simula-
tion is completed with this tools C simulation and C/RTL
co-simulation. Pre-synthesis resource report are used for
design space exploration and performance estimation. The
exported RTL is synthesized and implemented in Vivado
(v2013.4).

Our implementation is built on the VC707 board which
has a Xilinx FPGA chip Virtex7 485t. Its working frequency
is 100 MHz. Software implementation runs on an Intel Xeon
CPU E5-2430 (@2.20GHz) with 15MB cache.
5.2 Experimental Results

In this subsection, we rst report resource utilization.
Then, we compare software implementation (on CPU) to
our accelerator on FPGA. Finally, the comparison between
our implementation and existing FPGA approaches is pro-
vided.

The placement and routing is completed with Vivado tool
set. After that, the resource utilization of our implemen-
tation is reported out, as shown in Table 6. We can tell
that our CNN accelerator has almost fully utilized FPGAs
hardware resource.

Table 6: FPGA Resource utilization
FF

DSP BRAM LUT
205704
2240
186251
303600
2800
607200
61.3% 33.87%
80%

1024
2060
50%

Resource
Used
Available
Utilization

168ICCD2013
[12]
xed point
150 MHz

Precision
Frequency
FPGA chip Virtex6

FPGA ca-
pacity
LUT type
CNN Size

Performance

VLX240T
37,680 slices
768 DSP
6-input LUT
2.74 GMAC
8.5 GMACS

ASAP2009
[14]
16bits xed
115 MHz
Virtex5
LX330T
51,840 slices
192 DSP
6-input LUT
0.53 GMAC
3.37 GMACS

48bits xed
125 MHz
Spartan-3A
DSP3400
23,872 slices
126 DSP
4-input LUT
0.26 GMAC
2.6 GMACS

PACT2010
[2]
xed point
125 MHz

48bits xed
125 MHz
Virtex4 SX35 Virtex5
SX240T
37,440 slices
1056 DSP
6-input LUT
0.53 GMAC
3.5 GMACS

15,360 slices
192 DSP
4-input LUT
0.26 GMAC
2.6 GMACS

48bits xed
200 MHz
Virtex5
SX240T
37,440 slices
1056 DSP
6-input LUT
0.26 GMAC
8 GMACS

slices

32bits oat
100 MHz
Virtex7
VX485T
75,900
2800 DSP
6-input LUT
1.33 GFLOP
61.62
GFLOPS
61.62 GOPS
8.12E-04
GOPS/Slice

Table 5: Comparison to previous implementations

FPL2009 [6]

FPL2009 [6]

ISCA2010 [3] Our Impl.

Performance
Density

17 GOPS
4.5E-04
GOPs/Slice

6.74 GOPS
1.3E-04
GOPs/Slice

5.25 GOPS
2.2E-04
GOPs/Slice

5.25 GOPS
3.42E-04
GOPs/Slice

7.0 GOPS
1.9E-04
GOPs/Slice

16 GOPS
4.3E-04
GOPs/Slice

CPU 2.20GHz (ms)

1thd -O3

16thd -O3

FPGA

Table 7: Performance comparison to CPU
oat
32 bit
layer 1
layer 2
layer 3
layer 4
layer 5
Total

(ms) GFLOPS
7.67
5.35
3.79
2.88
1.93
21.61

98.18
94.66
77.38
65.58
40.70
376.50

19.36
27.00
24.30
18.64
14.18
103.48

27.50
83.79
78.81
77.94
77.61

-

Overall
GFLOPS
Speedup

3.54

1.00x

12.87

3.64x

61.62

17.42x

Table 8: Power consumption and energy

Intel Xeon 2.20GHz

1 thread -O3

16 threads -O3

Power (Watt)
Comparison
Energy (J)
Comparison

95.00
5.1x
35.77
89.4x

95.00
5.1x
9.83
24.6x

FPGA
18.61

1x
0.40
1x

The performance comparison between our accelerator and
software based counterpart is shown in Table 7. We select
our proposed cross-layer accelerator for comparison. The
software implementations are realized in 1 thread and 16
threads using gcc with -O3 optimization options, respec-
tively. Overall, our FPGA based implementation achieves
up to a 17.42x speedup over software implementation of 1
thread. It also achieves a 4.8x speedup over software imple-
mentation of 16 threads. Our accelerators overall perfor-
mance achieves 61.62 GFLOPS.

Figure 14 shows a picture of our on-board implementation.
A power meter is plugged in to measure its runtime power
performance, which is 18.6 Watt. CPUs thermal design
power is 95 Watt. Therefore, we can have a rough estima-
tion of software and FPGAs implementations power. Table
8 shows that the ratio of the consumed energy between soft-
ware implementation and FPGA implementation is 24.6x at
least. FPGA implementation uses much less energy than its
software counterparts.

In Table 5, various existing FPGA based CNN accelera-
tors are listed and compared to our implementation in this

Figure 14: Power measurement of on-board execu-
tion

Table 9: Resource occupation comparison

32-bit

Fixed point(adder)
Fixed point(mul.)

Floating point(adder)
Floating point(mul.)

DSP LUT FF
0
0

0
0

214
135

227
128

2
2
2
3

work. Since previous approaches use GMACS (giga multipli-
cation and accumulation per second) and we use GFLOPS
(giga oating point operations per second) as the perfor-
mance metric, we rst convert all result numbers to GOPS
(giga operations per second) to employ the same metric.
Note that each multiply-accumulate operation contains two
integer operations. As shown in the 9th row of Table 5, our
accelerator has a throughput of 61.62 GOPS, which outper-
forms other approaches by a speedup of at least 3.62x.

Since dierent work exploits dierent parallelism oppor-
tunities and use dierent FPGA platforms, it is hardly to
have a straightforward comparison between them. In order
to provide a fair comparison, we further present results of
performance density in Table 5. It is dened as average
GOPS per area unit (slice), which can represent the e-
ciency of a design independent of the FPGA platforms used
for implementation. As shown in the last row of Table 5,
our implementation achieves the highest performance den-
sity, which is 1.8x better than the second best. In addition,

169our method could achieve even better performance and per-
formance density if using xed point computation engines
because xed point processing elements uses much less re-
source (Table 9).

6. RELATED WORK

In this section, we discuss dierent design methods with
reference to other previous work on FPGA-based CNN ac-
celerator designs.

First, many CNN application accelerators are focused on
optimizing computation engines. Implementations [6], [14]
and [3] are three representatives. The earliest approach in
work [6] was to build their CNN application mainly by soft-
ware implementation while using one hardware systolic ar-
chitecture accelerator to do the ltering convolution job.
This design saves a lot of hardware resources and is used
in embedded system for automotive robots.
Implementa-
tions in work [14], [2] and [3] implement complete CNN ap-
plications on FPGA but exploits dierent parallelism op-
portunities. Work [14] and [2] mainly uses the parallelism
within feature maps and convolution kernel. Work [3] uses
inter-output and intra-output parallelism. Our paral-
lelization method is similar to theirs, but they do not use
on-chip buers for data reuse; alternatively they use very
high bandwidth and dynamical congurations to improve
performance. Our implementation take advantage of data
reuse, and balances limitations of bandwidth and FPGA
computation power.

Second, work [12], considering CNNs communication is-
sue, chooses to maximize date reuse and reduce bandwidth
requirement to the minimum. But their approach do not
consider maximizing computation performance. In addition,
they need to take time (in order of 10 seconds) to program
FPGA when shifting to next layers computation while our
solution only take no more than a microsecond to congure
a few registers.

7. CONCLUSIONS

In this work, we propose a rooine-model-based method
for convolutional neural networks FPGA acceleration.
In
this method we rst optimize CNNs computation and mem-
ory access. We then model all possible designs in rooine
model and nd the best design for each layer. We also nd
the best cross-layer design by enumeration. Finally, we re-
alize an implementation on Xilinx VC707 board which out-
performs all previous work.

8. ACKNOWLEDGMENT

This work was supported in part by NSF China 61202072,
RFDP 20110001110099, National High Technology Research
and Development Program of China 2012AA010902 and C-
FAR, one of six centers of STARnet, a Semiconductor Re-
search Corporation program sponsored by MARCO and DARPA.
We thank the UCLA/PKU Joint Research Institute for their
support of our research.

9. REFERENCES
[1] D. Aysegul, J. Jonghoon, G. Vinayak, K. Bharadwaj,

C. Alfredo, M. Berin, and C. Eugenio. Accelerating deep
neural networks on mobile processor with embedded
programmable logic. In NIPS 2013. IEEE, 2013.

[2] S. Cadambi, A. Majumdar, M. Becchi, S. Chakradhar, and

H. P. Graf. A programmable parallel accelerator for
learning and classication. In Proceedings of the 19th
international conference on Parallel architectures and
compilation techniques, pages 273284. ACM, 2010.

[3] S. Chakradhar, M. Sankaradas, V. Jakkula, and

S. Cadambi. A dynamically congurable coprocessor for
convolutional neural networks. In ACM SIGARCH
Computer Architecture News, volume 38, pages 247257.
ACM, 2010.

[4] T. Chen, Z. Du, N. Sun, J. Wang, C. Wu, Y. Chen, and
O. Temam. Diannao: A small-footprint high-throughput
accelerator for ubiquitous machine-learning. SIGPLAN
Not., 49(4):269284, Feb. 2014.

[5] J. Cong and B. Xiao. Minimizing computation in

convolutional neural networks. In Articial Neural
Networks and Machine LearningICANN 2014, pages
281290. Springer, 2014.

[6] C. Farabet, C. Poulet, J. Y. Han, and Y. LeCun. Cnp: An

fpga-based processor for convolutional networks. In Field
Programmable Logic and Applications, 2009. FPL 2009.
International Conference on, pages 3237. IEEE, 2009.

[7] Google. Improving photo search: A step across the

semantic gap. http://googleresearch.blogspot.com/
2013/06/improving-photo-search-step-across.html.

[8] S. Ji, W. Xu, M. Yang, and K. Yu. 3d convolutional neural

networks for human action recognition. IEEE Trans.
Pattern Anal. Mach. Intell., 35(1):221231, Jan. 2013.

[9] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet
classication with deep convolutional neural networks. In
F. Pereira, C. Burges, L. Bottou, and K. Weinberger,
editors, Advances in Neural Information Processing
Systems 25, pages 10971105. Curran Associates, Inc.,
2012.

[10] H. Larochelle, D. Erhan, A. Courville, J. Bergstra, and

Y. Bengio. An empirical evaluation of deep architectures on
problems with many factors of variation. In Proceedings of
the 24th International Conference on Machine Learning,
ICML 07, pages 473480, New York, NY, USA, 2007.
ACM.

[11] Y. LeCun, L. Bottou, Y. Bengio, and P. Haner.

Gradient-based learning applied to document recognition.
Proceedings of the IEEE, 86(11):22782324, 1998.

[12] M. Peemen, A. A. Setio, B. Mesman, and H. Corporaal.

Memory-centric accelerator design for convolutional neural
networks. In Computer Design (ICCD), 2013 IEEE 31st
International Conference on, pages 1319. IEEE, 2013.
[13] L.-N. Pouchet, P. Zhang, P. Sadayappan, and J. Cong.

Polyhedral-based data reuse optimization for congurable
computing. In Proceedings of the ACM/SIGDA
International Symposium on Field Programmable Gate
Arrays, FPGA 13, pages 2938, New York, NY, USA,
2013. ACM.

[14] M. Sankaradas, V. Jakkula, S. Cadambi, S. Chakradhar,

I. Durdanovic, E. Cosatto, and H. P. Graf. A massively
parallel coprocessor for convolutional neural networks. In
Application-specic Systems, Architectures and Processors,
2009. ASAP 2009. 20th IEEE International Conference
on, pages 5360. IEEE, 2009.

[15] S. Williams, A. Waterman, and D. Patterson. Rooine: An

insightful visual performance model for multicore
architectures. Commun. ACM, 52(4):6576, Apr. 2009.

[16] W. Zuo, Y. Liang, P. Li, K. Rupnow, D. Chen, and

J. Cong. Improving high level synthesis optimization
opportunity through polyhedral transformations. In
Proceedings of the ACM/SIGDA International Symposium
on Field Programmable Gate Arrays, FPGA 13, pages
918, New York, NY, USA, 2013. ACM.

170