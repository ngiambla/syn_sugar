Massively Parallel Methods for Deep Reinforcement Learning

5
1
0
2

 
l
u
J
 

6
1

 
 
]

G
L
.
s
c
[
 
 

2
v
6
9
2
4
0

.

7
0
5
1
:
v
i
X
r
a

Arun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory Fearon, Alessandro De Maria, Vedavyas
Panneershelvam, Mustafa Suleyman, Charles Beattie, Stig Petersen, Shane Legg, Volodymyr Mnih, Koray
Kavukcuoglu, David Silver
{ARUNSNAIR, PRAV, BLACKWELLS, CAGDASALCICEK, RORYF, ADEMARIA, DARTHVEDA, MUSTAFASUL, CBEATTIE,
SVP, LEGG, VMNIH, KORAYK, DAVIDSILVER @GOOGLE.COM }
Google DeepMind, London

Abstract

We present the ﬁrst massively distributed archi-
tecture for deep reinforcement learning. This
architecture uses four main components: paral-
lel actors that generate new behaviour; paral-
lel learners that are trained from stored experi-
ence; a distributed neural network to represent
the value function or behaviour policy; and a dis-
tributed store of experience. We used our archi-
tecture to implement the Deep Q-Network algo-
rithm (DQN) (Mnih et al., 2013). Our distributed
algorithm was applied to 49 games from Atari
2600 games from the Arcade Learning Environ-
ment, using identical hyperparameters. Our per-
formance surpassed non-distributed DQN in 41
of the 49 games and also reduced the wall-time
required to achieve these results by an order of
magnitude on most games.

1. Introduction
Deep learning methods have recently achieved state-of-
the-art results in vision and speech domains (Krizhevsky
et al., 2012; Simonyan & Zisserman, 2014; Szegedy et al.,
2014; Graves et al., 2013; Dahl et al., 2012), mainly due to
their ability to automatically learn high-level features from
a supervised signal. Recent advances in reinforcement
learning (RL) have successfully combined deep learning
with value function approximation, by using a deep con-
volutional neural network to represent the action-value (Q)
function (Mnih et al., 2013). Speciﬁcally, a new method
for training such deep Q-networks, known as DQN, has en-
abled RL to learn control policies in complex environments
with high dimensional images as inputs (Mnih et al., 2015).
This method outperformed a human professional in many

Presented at the Deep Learning Workshop, International Confer-
ence on Machine Learning, Lille, France, 2015.

games on the Atari 2600 platform, using the same net-
work architecture and hyper-parameters. However, DQN
has only previously been applied to single-machine archi-
tectures, in practice leading to long training times. For
example, it took 12-14 days on a GPU to train the DQN
algorithm on a single Atari game (Mnih et al., 2015). In
this work, our goal is to build a distributed architecture that
enables us to scale up deep reinforcement learning algo-
rithms such as DQN by exploiting massive computational
resources.
One of the main advantages of deep learning is that com-
putation can be easily parallelized. In order to exploit this
scalability, deep learning algorithms have made extensive
use of hardware advances such as GPUs. However, re-
cent approaches have focused on massively distributed ar-
chitectures that can learn from more data in parallel and
therefore outperform training on a single machine (Coates
et al., 2013; Dean et al., 2012). For example, the DistBelief
framework (Dean et al., 2012) distributes the neural net-
work parameters across many machines, and parallelizes
the training by using asynchronous stochastic gradient de-
scent (ASGD). DistBelief has been used to achieve state-
of-the-art results in several domains (Szegedy et al., 2014)
and has been shown to be much faster than single GPU
training (Dean et al., 2012).
Existing work on distributed deep learning has focused ex-
clusively on supervised and unsupervised learning. In this
paper we develop a new architecture for the reinforcement
learning paradigm. This architecture consists of four main
components: parallel actors that generate new behaviour;
parallel learners that are trained from stored experience; a
distributed neural network to represent the value function
or behaviour policy; and a distributed experience replay
memory.
A unique property of RL is that an agent inﬂuences the
training data distribution by interacting with its environ-
ment.
In order to generate more data, we deploy multi-
ple agents running in parallel that interact with multiple

Massively Parallel Methods for Deep Reinforcement Learning

instances of the same environment. Each such actor can
store its own record of past experience, effectively provid-
ing a distributed experience replay memory with vastly in-
creased capacity compared to a single machine implemen-
tation. Alternatively this experience can be explicitly ag-
gregated into a distributed database.
In addition to gen-
erating more data, distributed actors can explore the state
space more effectively, as each actor behaves according to
a slightly different policy.
A conceptually distinct set of distributed learners reads
samples of stored experience from the experience replay
memory, and updates the value function or policy accord-
ing to a given RL algorithm. Speciﬁcally, we focus in this
paper on a variant of the DQN algorithm, which applies
ASGD updates to the parameters of the Q-network. As in
DistBelief, the parameters of the Q-network may also be
distributed over many machines.
We applied our distributed framework for RL, known as
Gorila (General Reinforcement Learning Architecture), to
create a massively distributed version of the DQN algo-
rithm. We applied Gorila DQN to 49 games on the Atari
2600 platform. We outperformed single GPU DQN on
41 games and outperformed human professional on 25
games. Gorila DQN also trained much faster than the non-
distributed version in terms of wall-time, reaching the per-
formance of single GPU DQN roughly ten times faster for
most games.

2. Related Work
There have been several previous approaches to parallel or
distributed RL. A signiﬁcant part of this work has focused
on distributed multi-agent systems (Weiss, 1995; Lauer &
Riedmiller, 2000). In this approach, there are many agents
taking actions within a single shared environment, working
cooperatively to achieve a common objective. While com-
putation is distributed in the sense of decentralized control,
these algorithms focus on effective teamwork and emergent
group behaviors. Another paradigm which has been ex-
plored is concurrent reinforcement learning (Silver et al.,
2013), in which an agent can interact in parallel with an
inherently distributed environment, e.g. to optimize inter-
actions with multiple users on the internet. Our goal is
quite different to both these distributed and concurrent RL
paradigms: we simply seek to solve a single-agent problem
more efﬁciently by exploiting parallel computation.
The MapReduce framework has been applied to standard
MDP solution methods such as policy evaluation, policy
iteration and value iteration, by distributing the computa-
tion involved in large matrix multiplications (Li & Schu-
urmans, 2011). However, this work is narrowly focused
on batch methods for linear function approximation, and

is not immediately applicable to non-linear representations
using online reinforcement learning in environments with
unknown dynamics.
Perhaps the closest prior work to our own is a paralleliza-
tion of the canonical Sarsa algorithm over multiple ma-
chines. Each machine has its own instance of the agent and
environment (Grounds & Kudenko, 2008), running a sim-
ple reinforcement learning algorithm (linear Sarsa, in this
case). The changes to the parameters of the linear function
approximator are periodically communicated using a peer-
to-peer mechanism, focusing especially on those parame-
ters that have changed most. In contrast, our architecture
allows for client-server communication and a separation
between acting, learning and parameter updates; further-
more we exploit much richer function approximators using
a distributed framework for deep learning.

3. Background
3.1. DistBelief

DistBelief (Dean et al., 2012) is a distributed system for
training large neural networks on massive amounts of data
efﬁciently by using two types of parallelism. Model paral-
lelism, where different machines are responsible for storing
and training different parts of the model, is used to allow
efﬁcient training of models much larger than what is feasi-
ble on a single machine or GPU. Data parallelism, where
multiple copies or replicas of each model are trained on
different parts of the data in parallel, allows for more efﬁ-
cient training on massive datasets than a single process. We
brieﬂy discuss the two main components of the DistBelief
architecture – the central parameter server and the model
replicas.
The central parameter server holds the master copy of the
model. The job of the parameter server is to apply the in-
coming gradients from the replicas to the model and, when
requested, to send its latest copy of the model to the repli-
cas. The parameter server can be sharded across many ma-
chines and different shards apply gradients independently
of other shards.
Each replica maintains a copy of the model being trained.
This copy could be sharded across multiple machines if,
for example, the model is too big to ﬁt on a single ma-
chine. The job of the replicas is to calculate the gradients
given a mini-batch, send them to the parameter server, and
to periodically query the parameter server for an updated
version of the model. The replicas send gradients and re-
quest updated parameters independently of each other and
hence may not be synced to the same parameters at any
given time.

Massively Parallel Methods for Deep Reinforcement Learning

3.2. Reinforcement Learning

3.3. Deep Q-Networks

Recently, a new RL algorithm has been developed which is
in practice much more stable when combined with deep Q-
networks (Mnih et al., 2013; 2015). Like Q-learning, it iter-
atively solves the Bellman equation by adjusting the param-
eters of the Q-network towards the Bellman target. How-
ever, DQN, as shown in Figure 1 differs from Q-learning
in two ways. First, DQN uses experience replay (Lin,
1993). At each time-step t during an agent’s interaction
with the environment it stores the experience tuple et =
(st, at, rt, st+1) into a replay memory Dt = {e1, ..., et}.
Second, DQN maintains
two separate Q-networks
Q(s, a; θ) and Q(s, a; θ−) with current parameters θ and
old parameters θ− respectively. The current parameters θ
may be updated many times per time-step, and are copied
into the old parameters θ− after N iterations. At every
update iteration i the current parameters θ are updated
so as to minimise the mean-squared Bellman error with
respect to old parameters θ−, by optimizing the following
loss function (DQN Loss),

(cid:19)2(cid:35)

(cid:34)(cid:18)

Li(θi) = E

r + γ max

(cid:48)
a(cid:48) Q(s

(cid:48)

, a

; θ

i ) − Q(s, a; θi)
−

(1)

For each update i, a tuple of experience (s, a, r, s(cid:48)) ∼
U (D) (or a minibatch of such samples) is sampled uni-
formly from the replay memory D. For each sample
(or minibatch), the current parameters θ are updated by a
stochastic gradient descent algorithm. Speciﬁcally, θ is ad-
justed in the direction of the sample gradient gi of the loss
with respect to θ,

(cid:19)
i ) − Q(s, a; θi)
−

∇θi Q(s, a; θ)
(2)

(cid:18)

gi =

r + γ max

(cid:48)
a(cid:48) Q(s

(cid:48)

, a

; θ

Finally, actions are selected at each time-step t by an
-greedy behavior with respect to the current Q-network
Q(s, a; θ).

4. Distributed Architecture
We now introduce Gorila (General Reinforcement Learn-
ing Architecture), a framework for massively distributed
reinforcement learning. The Gorila architecture, shown in
Figure 2 contains the following components:
Actors. Any reinforcement
learning agent must ulti-
mately select actions at to apply in its environment. We
refer to this process as acting.
The Gorila architec-
ture contains Nact different actor processes, applied to
Nact corresponding instantiations of the same environ-
ment. Each actor i generates its own trajectories of ex-
perience si
T within the environment,
and as a result each actor may visit different parts of the
state space. The quantity of experience that is generated
by the actors after T time-steps is approximately T Nact.

1, ..., si

1, ai

T , ai

T , ri

1, ri

Figure 1. The DQN algorithm is composed of three main compo-
nents, the Q-network (Q(s, a; θ)) that deﬁnes the behavior pol-
icy, the target Q-network (Q(s, a; θ−)) that is used to generate
target Q values for the DQN loss term and the replay memory
that the agent uses to sample random transitions for training the
Q-network.

T(cid:80)

In the reinforcement learning (RL) paradigm, the agent
interacts sequentially with an environment, with the goal
of maximising cumulative rewards. At each step t the
agent observes state st, selects an action at, and receives
a reward rt. The agent’s policy π(a|s) maps states to
actions and deﬁnes its behavior. The goal of an RL
agent is to maximize its expected total reward, where
the rewards are discounted by a factor γ ∈ [0, 1] per
the return at time t is Rt =
time-step. Speciﬁcally,
γt(cid:48)−trt(cid:48) where T is the step when the episode termi-
t(cid:48)=t
nates. The action-value function Qπ(s, a) is the expected
return after observing state st and taking an action un-
der a policy π, Qπ(s, a) = E [Rt|st = s, at = a, π], and
the optimal action-value function is the maximum possi-
ble value that can be achieved by any policy, Q∗(s, a) =
Qπ(s, a). The action-value function obeys a
argmax
fundamental recursion known as the Bellman equation,

π

Q∗(s, a) = E(cid:104)

(cid:105)
a(cid:48) Q∗(s(cid:48), a(cid:48))

.

r + γ max

One of the core ideas behind reinforcement learning is to
represent the action-value function using a function ap-
proximator such as a neural network, Q(s, a) = Q(s, a; θ).
The parameters θ of the so-called Q-network are optimized
so as to approximately solve the Bellman equation. For
example, the Q-learning algorithm iteratively updates the
action-value function Q(s, a; θ) towards a sample of the
a(cid:48) Q(s(cid:48), a(cid:48); θ). However, it is
Bellman target, r + γ max
well-known that the Q-learning algorithm is highly unsta-
ble when combined with non-linear function approximators
such as deep neural networks (Tsitsiklis & Roy, 1997).

Q NetworkDQN LossTargetQ Network(s,a)s’argmaxa Q(s,a; θ)sQ(s,a; θ)Gradientwrt lossrmaxa’ Q(s’,a’; θ–)Store(s,a,r,s’)Copy everyN updatesReplayMemoryEnvironmentMassively Parallel Methods for Deep Reinforcement Learning

Figure 2. The Gorila agent parallelises the training procedure by separating out learners, actors and parameter server. In a single exper-
iment, several learner processes exist and they continuously send the gradients to parameter server and receive updated parameters. At
the same time, independent actors can also in parallel accumulate experience and update their Q-networks from the parameter server.

t, ai

t, ri

t, si

1, ..., ei

t = {ei

Each actor contains a replica of the Q-network, which is
used to determine behavior, for example using an -greedy
policy. The parameters of the Q-network are synchronized
periodically from the parameter server.
Experience replay memory. The experience tuples ei
t =
t+1) generated by the actors are stored in a re-
(si
play memory D. We consider two forms of experience
replay memory. First, a local replay memory stores each
t} locally on that ac-
actor’s experience Di
tor’s machine. If a single machine has sufﬁcient memory
to store M experience tuples, then the overall memory ca-
pacity becomes M Nact. Second, a global replay memory
aggregates the experience into a distributed database.
In
this approach the overall memory capacity is independent
of Nact and may be scaled as desired, at the cost of addi-
tional communication overhead.
Learners. Gorila contains Nlearn learner processes. Each
learner contains a replica of the Q-network and its job is
to compute desired changes to the parameters of the Q-
network. For each learner update k, a minibatch of experi-
ence tuples e = (s, a, r, s(cid:48)) is sampled from either a local
or global experience replay memory D (see above). The
learner applies an off-policy RL algorithm such as DQN
(Mnih et al., 2013) to this minibatch of experience, in or-
der to generate a gradient vector gi.1 The gradients gi are
communicated to the parameter server; and the parameters

1The experience in the replay memory is generated by old be-
havior policies which are most likely different to the current be-
havior of the agent; therefore all updates must be performed off-
policy (Sutton & Barto, 1998).

of the Q-network are updated periodically from the param-
eter server.
Parameter server. Like DistBelief, the Gorila architecture
uses a central parameter server to maintain a distributed
representation of the Q-network Q(s, a; θ+). The param-
eter vector θ+ is split disjointly across Nparam different
machines. Each machine is responsible for applying gra-
dient updates to a subset of the parameters. The parame-
ter server receives gradients from the learners, and applies
these gradients to modify the parameter vector θ+, using
an asynchronous stochastic gradient descent algorithm.
The Gorila architecture provides considerable ﬂexibility in
the number of ways an RL agent may be parallelized. It is
possible to have parallel acting to generate large quantities
of data into a global replay database, and then process that
data with a single serial learner. In contrast, it is possible
to have a single actor generating data into a local replay
memory, and then have multiple learners process this data
in parallel to learn as effectively as possible from this expe-
rience. However, to avoid any individual component from
becoming a bottleneck, the Gorila architecture in general
allows for arbitrary numbers of actors, learners, and param-
eter servers to both generate data, learn from that data, and
update the model in a scalable and fully distributed fashion.
The simplest overall instantiation of Gorila, which we con-
sider in our subsequent experiments, is the bundled mode
in which there is a one-to-one correspondence between ac-
tors, replay memory, and learners (Nact = Nlearn). Each
bundle has an actor generating experience, a local replay

Massively Parallel Methods for Deep Reinforcement Learning

Algorithm 1 Distributed DQN Algorithm

Initialise replay memory D to size P .
Initialise the training network for the action-value func-
tion Q(s, a; θ) with weights θ and target network
Q(s, a; θ−) with weights θ− = θ.
for episode = 1 to M do

Initialise the start state to s1.
Update θ from parameters θ+ of the parameter server.
for t = 1 to T do

a

Q(s, a; θ).

With probability  take a random action at or else
at = argmax
Execute the action in the environment and ob-
serve the reward rt and the next state st+1. Store
(st, at, rt, st+1) in D.
Update θ from parameters θ+ of the parameter
server.
Sample random mini-batch from D. And for each
tuple (si, ai, ri, si+1) set target yt as
if si+1 is terminal then

yt = ri

else

yt = ri + γmax

a(cid:48) Q(si+1, a(cid:48); θ−)

end if
Calculate the loss Lt = (yt − Q(si, ai; θ)2).
Compute gradients with respect to the network pa-
rameters θ using equation 2.
Send gradients to the parameter server.
Every global N steps sync θ− with parameters θ+
from the parameter server.

end for

end for

memory to store that experience, and a learner that updates
parameters based on samples of experience from the local
replay memory. The only communication between bundles
is via parameters: the learners communicate their gradients
to the parameter server; and the Q-networks in the actors
and learners are periodically synchronized to the parameter
server.

4.1. Gorila DQN

We now consider a speciﬁc instantiation of the Gorila ar-
chitecture implementing the DQN algorithm. As described
in the previous section, the DQN algorithm utilizes two
copies of the Q-network: a current Q-network with param-
eters θ and a target Q-network with parameters θ−. The
DQN algorithm is extended to the distributed implementa-
tion in Gorila as follows. The parameter server maintains
the current parameters θ+ and the actors and learners con-
tain replicas of the current Q-network Q(s, a; θ) that are
synchronized from the parameter server before every act-
ing step.

The learner additionally maintains the target Q-network
Q(s, a; θ−). The learner’s target network is updated from
the parameter server θ+ after every N gradient updates in
the central parameter server.
Note that N is a global parameter that counts the total num-
ber of updates to the central parameter server rather than
counting the updates from the local learner.
The learners generate gradients using the DQN gradient
given in Equation 2. However, the gradients are not ap-
plied directly, but instead communicated to the parameter
server. The parameter server then applies the updates that
are accumulated from many learners.

4.2. Stability

While the DQN training algorithm was designed to ensure
stability of training neural networks with reinforcement
learning, training using a large cluster of machines running
multiple other tasks poses additional challenges. The Go-
rila DQN implementation uses additional safeguards to en-
sure stability in the presence of disappearing nodes, slow-
downs in network trafﬁc, and slowdowns of individual ma-
chines. One such safeguard is a parameter that determines
the maximum time delay between the local parameters θ
(the gradients gi are computed using θ) and the parameters
θ+ in the parameter server.
All gradients older than the threshold are discarded by the
parameter server. Additionally, each actor/learner keeps
a running average and standard deviation of the absolute
DQN loss for the data it sees and discards gradients with
absolute loss higher than the mean plus several standard de-
viations. Finally, we used the AdaGrad update rule (Duchi
et al., 2011).

5. Experiments
5.1. Experimental Set Up

We evaluated Gorila by conducting experiments on 49
Atari 2600 games using the Arcade Learning Environ-
ment (Bellemare et al., 2012). Atari games provide a chal-
lenging and diverse set of reinforcement learning problems
where an agent must learn to play the games directly from
210 × 160 RGB video input with only the changes in the
score provided as rewards. We closely followed the ex-
perimental setup of DQN (Mnih et al., 2015) using the
same preprocessing and network architecture. We prepro-
cessed the 210 × 160 RGB images by downsampling them
to 84 × 84 and extracting the luminance channel.
The Q-network Q(s, a; θ) had 3 convolutional layers fol-
lowed by a fully-connected hidden layer. The 84 × 84 × 4
input to the network is obtained by concatenating the im-
ages from four previous preprocessed frames. The ﬁrst

Massively Parallel Methods for Deep Reinforcement Learning

convolutional layer had 32 ﬁlters of size 4 × 8 × 8 and
stride 4. The second convolutional layer had 64 ﬁlters of
size 32 × 4 × 4 with stride 2, while the third had 64 ﬁlters
with size 64 × 3 × 3 and stride 1. The next layer had 512
fully-connected output units, which is followed by a lin-
ear fully-connected output layer with a single output unit
for each valid action. Each hidden layer was followed by a
rectiﬁer nonlinearity.
We have used the same frame skipping step implemented
in (Mnih et al., 2015) by repeating every action at over the
next 4 frames.
In all experiments, Gorila DQN used: Nparam = 31 and
Nlearn = Nact = 100. We use the bundled mode. Replay
memory size D = 1 million frames and used -greedy as
the behaviour policy with  annealed from 1 to 0.1 over the
ﬁrst one million global updates. Each learner syncs the pa-
rameters θ− of its target network after every 60K parameter
updates performed in the parameter server.

5.2. Evaluation

We used two types of evaluations. The ﬁrst follows the
protocol established by DQN. Each trained agent was eval-
uated on 30 episodes of the game it was trained on. A ran-
dom number of frames were skipped by repeatedly taking
the null or do nothing action before giving control to the
agent in order to ensure variation in the initial conditions.
The agents were allowed to play until the end of the game
or up to 18000 frames (5 minutes), whichever came ﬁrst,
and the scores were averaged over all 30 episodes. We re-
fer to this evaluation procedure as null op starts.
Testing how well an agent generalizes is especially impor-
tant in the Atari domain because the emulator is completely
deterministic.
Our second evaluation method, which we call human starts,
aims to measure how well the agent generalizes to states it
may not have trained on. To that end, we have introduced
100 random starting points that were sampled from a hu-
man professional’s gameplay for each game. To evaluate
an agent, we ran it from each of the 100 starting points until
the end of the game or until a total of 108000 frames (equiv-
alent to 30 minutes) were played counting the frames the
human played to reach the starting point. The total score
accumulated only by the agent (not considering any points
won by the human player) were averaged to obtain the eval-
uation score.
In order to make it easier to compare results on 49 games
with a greatly varying range of scores we present the re-
sults on a scale where 0 is the score obtained by a random
agent and 100 is the score obtained by a professional hu-
man game player. The random agent selected actions uni-
formly at random at 10Hz and it was evaluated using the

same starting states as the agents for both kinds of evalua-
tions (null op starts and human starts).
We selected hyperparameter values by performing an infor-
mal search on the games of Breakout, Pong and Seaquest
which were then ﬁxed for all the games. We have trained
Gorila DQN 5 times on each game using the same ﬁxed hy-
perparameter settings and random network initializations.
Following DQN, we periodically evaluated each model
during training and kept the best performing network pa-
rameters for the ﬁnal evaluation. We average these ﬁnal
evaluations over the 5 runs, and compare the mean evalua-
tions with DQN and human expert scores.

6. Results

Figure 5. The time required by Gorila DQN to surpass single
DQN performance (red curve) and to reach its peak performance
(blue curve).

We ﬁrst compared Gorila DQN agents trained for up to 6
days to single GPU DQN agents trained for 12-14 days.
Figure 3 shows the normalized scores under the human
starts evaluation. Using human starts Gorila DQN out-
performed single GPU DQN on 41 out of 49 games given
roughly one half of the training time of single GPU DQN.
On 22 of the games Gorila DQN obtained double the score
of single GPU DQN, and on 11 games Gorila DQN’s score
was 5 times higher. Similarly, using the original null op
starts evaluation Gorila DQN outperformed the single GPU
DQN on 31 out of 49 games. These results show that par-
allel training signiﬁcantly improved performance in less
training time. Also, better results on human starts com-
pared to null op starts suggest that Gorila DQN is es-
pecially good at generalizing to potentially unseen states
compared to single GPU DQN. Figure 4 further illustrates
these improvements in generalization by showing Gorila
DQN scores with human starts normalized with respect to
GPU DQN scores with human starts (blue bars) and Gorila
DQN scores from null op starts normalized by GPU DQN

012345601020304050HIGHESTBEATINGTIME (Days)GAMESMassively Parallel Methods for Deep Reinforcement Learning

Figure 3. Performance of the Gorila agent on 49 Atari games with human starts evaluation compared with DQN (Mnih et al., 2015)
performance with scores normalized to expert human performance. Font color indicates which method has the higher score. *Not
showing DQN scores for Asterix, Asteroids, Double Dunk, Private Eye, Wizard Of Wor and Gravitar because the DQN human starts
scores are less than the random agent baselines. Also not showing Video Pinball because the human expert scores are less than the
random agent scores.

scores from null op starts (gray bars). In fact, Gorila DQN
performs at a level similar or superior to a human profes-
sional (75% of the human score or above) in 25 games de-
spite starting from states sampled from human play. One
possible reason for the improved generalization is the sig-
niﬁcant increase in the number of states Gorila DQN sees
by using 100 parallel actors.

We next look at how the performance of Gorila DQN im-
proved during training. Figure 5 shows how quickly Gorila
DQN reached the performance of single GPU DQN and
how quickly Gorila DQN reached its own best score un-
der the human starts evaluation. Gorila DQN surpassed the
best single GPU DQN scores on 19 games in 6 hours, 23
games in 12 hours, 30 in 24 hours and 38 games in 36 hours
(red curve). This is a roughly an order of magnitude reduc-

0%200%400%600%800%1,000%5,000%Asteroids*Montezuma_RevengePrivate_Eye*Ms_PacmanFrostbiteGravitar*AlienAmidarBowlingEnduroBeam_RiderSeaquestHeroChopper_CommandRiverRaidFreewayAsterix*VentureKangarooCentipedeBattle_ZoneQBertBank_HeistZaxxonSpace_InvadersIce_HockeyTutankhamUp_n_DownKung_Fu_MasterFishing_DerbyPongJamesBondTennisName_This_GameStar_GunnerGopherTime_PilotAssaultCrazy_ClimberWizard_of_Wor*Double_Dunk*Demon_AttackKrullRoad_RunnerBoxingRobotankBreakoutAtlantisHuman Scorebelow human-levelat human-level or aboveGORILADQNMassively Parallel Methods for Deep Reinforcement Learning

Figure 4. Performance of the Gorila agent on 49 Atari games with human starts and null op evaluations normalized with respect to DQN
human start and null op scores respectively. This ﬁgure shows the generalization improvements of Gorila compared to DQN. *Using
a score of 0 for the human starts random agent score for Asterix, Asteroids, Double Dunk, Private Eye, Wizard Of Wor and Gravitar
because the human starts DQN scores are less than the random agent scores. Not showing Double Dunk because both the DQN scores
and the random agent scores are negative. **Not showing null op scores for Montezuma Revenge because both the human start scores
and random agent scores are 0.

tion in training time required to reach the single process
DQN score. On some games Gorila DQN achieved its best
score in under two days but for most of the games the per-
formance keeps improving with longer training time (blue
curve).

7. Conclusion
In this paper we have introduced the ﬁrst massively dis-
tributed architecture for deep reinforcement learning. The
Gorila architecture acts and learns in parallel, using a dis-
tributed replay memory and distributed neural network. We
applied Gorila to an asynchronous variant of the state-of-

0%200%400%600%800%1,000%5,000%EnduroAssaultFreewayBeam RiderStar GunnerKangarooHeroSpace InvadersPongBreakoutRobotankChopper CommandTennisFishing DerbyDemon AttackBattle ZoneJamesBondAsteroids*Crazy ClimberIce HockeyRiverRaidAmidarAlienQBertGopherKung Fu MasterMs PacmanKrullName This GameTime PilotMontezuma Revenge**CentipedeBank HeistGravitar*BoxingUp n DownBowlingSeaquestFrostbiteRoad RunnerTutankhamVideo Pinball*Private Eye*AtlantisVentureZaxxonAsterix*Wizard of Wor*DQN ScoreHUMAN STARTSNULL OPMassively Parallel Methods for Deep Reinforcement Learning

the-art DQN algorithm. A single machine had previously
achieved state-of-the-art results in the challenging suite of
Atari 2600 games, but it was not previously known whether
the good performance of DQN would continue to scale
with additional computation. By leveraging massive par-
allelism, Gorila DQN signiﬁcantly outperformed single-
GPU DQN on 41 out of 49 games; achieving by far the
best results in this domain to date. Gorila takes a further
step towards fulﬁlling the promise of deep learning in RL:
a scalable architecture that performs better and better with
increased computation and memory.

References
Bellemare, Marc G, Naddaf, Yavar, Veness, Joel, and
Bowling, Michael. The arcade learning environment: An
evaluation platform for general agents. arXiv preprint
arXiv:1207.4708, 2012.

Coates, Adam, Huval, Brody, Wang, Tao, Wu, David,
Catanzaro, Bryan, and Andrew, Ng. Deep learning with
cots hpc systems. In Proceedings of The 30th Interna-
tional Conference on Machine Learning, pp. 1337–1345,
2013.

Dahl, George E, Yu, Dong, Deng, Li, and Acero, Alex.
Context-dependent pre-trained deep neural networks for
large-vocabulary speech recognition. Audio, Speech, and
Language Processing, IEEE Transactions on, 20(1):30–
42, 2012.

Dean, Jeffrey, Corrado, Greg, Monga, Rajat, Chen, Kai,
Devin, Matthieu, Mao, Mark, Senior, Andrew, Tucker,
Paul, Yang, Ke, Le, Quoc V, et al. Large scale distributed
deep networks. In Advances in Neural Information Pro-
cessing Systems, pp. 1223–1231, 2012.

Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive
subgradient methods for online learning and stochastic
optimization. The Journal of Machine Learning Re-
search, 12:2121–2159, 2011.

Graves, Alex, Mohamed, A-R, and Hinton, Geoffrey.
Speech recognition with deep recurrent neural networks.
In Acoustics, Speech and Signal Processing (ICASSP),
2013 IEEE International Conference on, pp. 6645–6649.
IEEE, 2013.

Grounds, Matthew and Kudenko, Daniel. Parallel rein-
forcement learning with linear function approximation.
In Proceedings of the 5th, 6th and 7th European Confer-
ence on Adaptive and Learning Agents and Multi-agent
Systems: Adaptation and Multi-agent Learning, pp. 60–
74. Springer-Verlag, 2008.

Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoff. Im-
agenet classiﬁcation with deep convolutional neural net-

In Advances in Neural Information Processing

works.
Systems 25, pp. 1106–1114, 2012.

Lauer, Martin and Riedmiller, Martin. An algorithm for
distributed reinforcement learning in cooperative multi-
agent systems. In In Proceedings of the Seventeenth In-
ternational Conference on Machine Learning, pp. 535–
542. Morgan Kaufmann, 2000.

Li, Yuxi and Schuurmans, Dale. Mapreduce for parallel re-
inforcement learning. In Recent Advances in Reinforce-
ment Learning - 9th European Workshop, EWRL 2011,
Athens, Greece, September 9-11, 2011, Revised Selected
Papers, pp. 309–320, 2011.

Lin, Long-Ji. Reinforcement learning for robots using neu-
ral networks. Technical report, DTIC Document, 1993.

Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David,
Graves, Alex, Antonoglou, Ioannis, Wierstra, Daan, and
Riedmiller, Martin. Playing atari with deep reinforce-
ment learning. In NIPS Deep Learning Workshop. 2013.

Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David,
Rusu, Andrei A., Veness, Joel, Bellemare, Marc G.,
Graves, Alex, Riedmiller, Martin, Fidjeland, Andreas K.,
Ostrovski, Georg, Petersen, Stig, Beattie, Charles, Sadik,
Amir, Antonoglou, Ioannis, King, Helen, Kumaran,
Dharshan, Wierstra, Daan, Legg, Shane, and Hassabis,
Demis. Human-level control through deep reinforcement
learning. Nature, 518(7540):529–533, 02 2015. URL
http://dx.doi.org/10.1038/nature14236.

Silver, David, Newnham, Leonard, Barker, David, Weller,
Suzanne, and McFall, Jason. Concurrent reinforcement
learning from customer interactions. In Proceedings of
the 30th International Conference on Machine Learning,
pp. 924–932, 2013.

Simonyan, Karen and Zisserman, Andrew. Very deep con-
volutional networks for large-scale image recognition.
arXiv preprint arXiv:1409.1556, 2014.

Sutton, R. and Barto, A. Reinforcement Learning: an In-

troduction. MIT Press, 1998.

Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet,
Pierre, Reed, Scott, Anguelov, Dragomir, Erhan, Du-
mitru, Vanhoucke, Vincent, and Rabinovich, Andrew.
arXiv preprint
Going deeper with convolutions.
arXiv:1409.4842, 2014.

Tsitsiklis, J. and Roy, B. Van. An analysis of temporal-
difference learning with function approximation. IEEE
Transactions on Automatic Control, 42(5):674–690,
1997.

Weiss, Gerhard. Distributed reinforcement learning. 15:

135–142, 1995.

Appendix

July 17, 2015

8. Appendix
8.1. Data

We present all the data that has been used in the paper.

• Table 1 shows the various normalized scores for null

op evaluation.

• Table 2 shows the various normalized scores for hu-

man start evaluation.

• Table 3 shows the various raw scores for human start

evaluation.

• Table 4 shows the various raw scores for null op eval-

uation.

10

Massively Parallel Methods for Deep Reinforcement Learning

Games

Alien
Amidar
Assault
Asterix
Asteroids
Atlantis
Bank Heist
Battle Zone
Beam Rider
Bowling
Boxing
Breakout
Centipede
Chopper Command
Crazy Climber
Demon Attack
Double Dunk
Enduro
Fishing Derby
Freeway
Frostbite
Gopher
Gravitar
Hero
Ice Hockey
JamesBond
Kangaroo
Krull
Kung Fu Master
Montezuma Revenge
Ms Pacman
Name This Game
Pong
Private Eye
QBert
RiverRaid
Road Runner
Robotank
Seaquest
Space Invaders
Star Gunner
Tennis
Time Pilot
Tutankham
Up n Down
Venture
Video Pinball
Wizard of Wor
Zaxxon

Table 1. NULL OP NORMALIZED

DQN
(human normalized)
42.74
43.93
246.16
69.95
7.31
451.84
57.69
67.55
119.79
14.65
1707.14
1327.24
62.98
64.77
419.49
294.19
16.12
97.48
93.51
102.36
6.16
400.42
5.35
76.50
79.33
145.00
224.20
277.01
102.37
0.00
13.02
278.28
132
2.53
78.48
57.30
232.91
509.27
25.94
121.48
598.08
148.99
100.92
112.22
92.68
32.00
2539.36
67.48
54.08

Gorila
(human normalized)
35.99
70.89
96.39
75.04
2.64
539.11
82.58
64.63
54.31
23.47
2256.66
1330.56
64.23
37.00
305.06
416.74
257.34
37.11
115.11
39.49
12.64
243.35
35.27
56.14
87.49
152.50
83.71
788.85
121.38
0.09
19.01
218.05
130
1.04
80.14
57.54
651.00
352.92
65.13
115.36
192.79
232.70
300.86
149.53
140.70
104.87
13576.75
314.04
77.63

Gorila
(DQN normalized)
84.20
161.36
39.15
107.26
36.09
119.31
143.15
95.68
45.34
160.18
132.18
100.25
101.97
57.12
72.72
141.65
1595.55
38.07
123.09
38.58
205.23
60.77
659.37
73.38
110.27
105.16
37.33
284.76
118.57
0.00
146.03
78.35
98.48
41.05
102.10
100.41
279.50
69.29
251.08
94.96
32.23
156.18
298.11
133.24
151.81
327.71
534.65
465.32
143.53

Massively Parallel Methods for Deep Reinforcement Learning

Games

Alien
Amidar
Assault
Asterix
Asteroids
Atlantis
Bank Heist
Battle Zone
Beam Rider
Bowling
Boxing
Breakout
Centipede
Chopper Command
Crazy Climber
Demon Attack
Double Dunk
Enduro
Fishing Derby
Freeway
Frostbite
Gopher
Gravitar
Hero
Ice Hockey
JamesBond
Kangaroo
Krull
Kung Fu Master
Montezuma Revenge
Ms Pacman
Name This Game
Pong
Private Eye
QBert
RiverRaid
Road Runner
Robotank
Seaquest
Space Invaders
Star Gunner
Tennis
Time Pilot
Tutankham
Up n Down
Venture
Video Pinball
Wizard of Wor
Zaxxon

Table 2. HUMAN STARTS NORMALIZED

DQN
(human normalized)
7.07
7.95
685.15
-0.54
-0.50
477.76
24.82
47.50
57.23
5.39
245.94
1149.42
22.00
28.98
178.54
390.38
-350.00
67.81
90.99
100.78
2.19
120.41
-1.01
46.87
57.84
94.02
98.37
283.33
56.49
0.60
3.72
73.13
102.08
-0.57
36.55
25.20
135.72
863.07
6.41
98.81
378.03
129.93
99.57
15.68
28.33
3.52
-4.65
-14.87
4.46

Gorila
(human normalized)
10.97
11.60
222.71
42.87
0.15
4695.72
60.64
55.57
24.25
16.85
682.03
1184.15
52.06
30.74
240.52
453.60
290.62
18.59
99.44
39.23
8.70
200.05
10.20
30.43
78.23
122.53
50.43
544.42
99.18
1.41
7.01
148.38
103.63
3.04
57.71
34.23
642.10
913.69
24.69
78.03
161.04
140.84
210.13
84.19
87.50
49.50
1904.86
256.58
71.34

Gorila
(DQN normalized)
155.06
145.85
32.50
2670.44
133.93
982.84
244.32
116.98
42.38
312.62
277.31
103.02
236.59
106.06
134.71
116.19
0.00
27.42
109.28
38.92
395.82
166.13
248.67
64.92
135.25
130.31
51.27
192.14
175.57
236.00
188.30
202.88
101.51
871.41
157.89
135.80
473.07
105.86
385.13
78.97
42.60
108.39
211.01
536.80
308.76
1403.88
554.14
4240.24
1596.74

Massively Parallel Methods for Deep Reinforcement Learning

Table 3. RAW DATA - HUMAN STARTS

Games
Alien
Amidar
Assault
Asterix
Asteroids
Atlantis
Bank Heist
Battle Zone
Beam Rider
Bowling
Boxing
Breakout
Centipede
Chopper Command
Crazy Climber
Demon Attack
Double Dunk
Enduro
Fishing Derby
Freeway
Frostbite
Gopher
Gravitar
Hero
Ice Hockey
JamesBond
Kangaroo
Krull
Kung Fu Master
Montezuma Revenge
Ms Pacman
Name This Game
Pong
Private Eye
QBert
RiverRaid
Road Runner
Robotank
Seaquest
Space Invaders
Star Gunner
Tennis
Time Pilot
Tutankham
Up n Down
Venture
Video Pinball
Wizard of Wor
Zaxxon

Random
128.30
11.80
166.90
164.50
877.10
13463.00
21.70
3560.00
254.60
35.20
-1.50
1.60
1925.50
644.00
9337.00
208.30
-16.00
-81.80
-77.10
0.20
66.40
250.00
245.50
1580.30
-9.70
33.50
100.00
1151.90
304.00
25.00
197.80
1747.80
-18.00
662.80
271.80
588.30
200.00
2.40
215.50
182.60
697.00
-21.40
3273.00
12.70
707.20
18.00
20452.00
804.00
475.00

Human
6371.30
1540.40
628.90
7536.00
36517.30
26575.00
644.50
33030.00
14961.00
146.50
9.60
27.90
10321.90
8930.00
32667.00
3442.80
-14.40
740.20
5.10
25.60
4202.80
2311.00
3116.00
25839.40
0.50
368.50
2739.00
2109.10
20786.80
4182.00
15375.00
6796.00
15.50
64169.10
12085.00
14382.20
6878.00
8.90
40425.80
1464.90
9528.00
-6.70
5650.00
138.30
9896.10
1039.00
15641.10
4556.00
8443.00

DQN Gorila Avg
813.54
570.20
189.15
133.40
1195.85
3332.30
124.50
3324.70
933.63
697.10
629166.50
76108.00
399.42
176.30
17560.00
19938.00
3822.07
8672.40
53.95
41.20
74.20
25.80
303.90
313.03
6296.87
3773.10
3191.75
3046.00
65451.00
50992.00
12835.20
14880.13
-11.35
-21.60
71.04
475.60
4.64
-2.30
25.80
10.16
426.60
157.40
4373.04
2731.80
538.37
216.50
12952.50
8963.36
-1.72
-3.80
444.00
348.50
1431.00
2696.00
6363.09
3864.00
11875.00
20620.00
84.00
50.00
1263.05
763.50
9238.50
5439.90
16.20
16.71
2598.55
298.20
7089.83
4589.80
5310.27
4065.30
9264.00
43079.80
61.78
58.50
10145.85
2793.90
1183.29
1449.70
34081.00
14919.25
-0.69
-2.30
8267.80
5640.00
118.45
32.40
3311.30
8747.67
523.40
54.00
112093.37
20228.10
10431.00
246.00
831.00
6159.40

Massively Parallel Methods for Deep Reinforcement Learning

Table 4. RAW DATA - NULL OP

Games
Alien
Amidar
Assault
Asterix
Asteroids
Atlantis
Bank Heist
Battle Zone
Beam Rider
Bowling
Boxing
Breakout
Centipede
Chopper Command
Crazy Climber
Demon Attack
Double Dunk
Enduro
Fishing Derby
Freeway
Frostbite
Gopher
Gravitar
Hero
Ice Hockey
JamesBond
Kangaroo
Krull
Kung Fu Master
Montezuma Revenge
Ms Pacman
Name This Game
Pong
Private Eye
QBert
RiverRaid
Road Runner
Robotank
Seaquest
Space Invaders
Star Gunner
Tennis
Time Pilot
Tutankham
Up n Down
Venture
Video Pinball
Wizard of Wor
Zaxxon

Random
227.80
5.80
222.40
210.00
719.10
12850.00
14.20
2360.00
363.90
23.10
0.10
1.70
2090.90
811.00
10780.50
152.10
-18.60
0.00
-91.70
0.00
65.20
257.60
173.00
1027.00
-11.20
29.00
52.00
1598.00
258.50
0.00
307.30
2292.30
-20.70
24.90
163.90
1338.50
11.50
2.20
68.40
148.00
664.00
-23.80
3568.00
11.40
533.40
0.00
16256.90
563.50
32.50

Human
6875.40
1675.80
1496.40
8503.30
13156.70
29028.10
734.40
37800.00
5774.70
154.80
4.30
31.80
11963.20
9881.80
35410.50
3401.30
-15.50
309.60
5.50
29.60
4334.70
2321.00
2672.00
25762.50
0.90
406.70
3035.00
2394.60
22736.20
4366.70
15693.40
4076.20
9.30
69571.30
13455.00
13513.30
7845.00
11.90
20181.80
1652.30
10250.00
-8.90
5925.00
167.60
9082.00
1187.50
17297.60
4756.50
9173.30

DQN Gorila Avg
2620.53
1189.70
1450.41
6433.33
1047.66
100069.16
609.00
25266.66
3302.91
54.01
94.88
402.20
8432.30
4167.50
85919.16
13693.12
-10.62
114.90
20.19
11.69
605.16
5279.00
1054.58
14913.87
-0.61
605.00
2549.16
7882.00
27543.33
4.16
3233.50
6182.16
18.30
748.60
10815.55
8344.83
51007.99
36.43
13169.06
1883.41
19144.99
10.87
10659.33
244.97
12561.58
1245.33
157550.21
13731.33
7129.33

3069.30
739.50
3358.60
6011.70
1629.30
85950.00
429.70
26300.00
6845.90
42.40
71.80
401.20
8309.40
6686.70
114103.30
9711.20
-18.10
301.80
-0.80
30.30
328.30
8520.00
306.70
19950.30
-1.60
576.70
6740.00
3804.70
23270.00
0.00
2311.00
7256.70
18.90
1787.60
10595.80
8315.70
18256.70
51.60
5286.00
1975.50
57996.70
-1.60
5946.70
186.70
8456.30
380.00
42684.10
3393.30
4976.70

